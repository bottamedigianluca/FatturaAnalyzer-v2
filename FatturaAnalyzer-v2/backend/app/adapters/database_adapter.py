"""
Database Adapter ULTRA-OTTIMIZZATO per FastAPI - Versione 3.0
Sfrutta al 100% il backend con connection pooling, cache intelligente, e performance monitoring
"""

import asyncio
import logging
import os
import threading
import time
import sqlite3
from typing import Dict, Any, Optional, List, Tuple, Union
from concurrent.futures import ThreadPoolExecutor
import pandas as pd
from contextlib import asynccontextmanager, contextmanager
import psutil
from functools import lru_cache, wraps
from dataclasses import dataclass, field
import json
import hashlib
from collections import defaultdict, deque
import weakref
import gc

# Import dal core esistente
from app.core.database import (
    get_connection, create_tables, get_anagraphics, get_invoices, get_transactions,
    add_anagraphics_if_not_exists, add_transactions,
    get_reconciliation_links_for_item, get_item_details,
    update_invoice_reconciliation_state, update_transaction_reconciliation_state,
    add_or_update_reconciliation_link, remove_reconciliation_links
)

logger = logging.getLogger(__name__)

# ================== CONFIGURAZIONE AVANZATA ==================

class DatabaseConfig:
    """Configurazione per database adapter ottimizzato"""
    MAX_WORKERS = min(8, (os.cpu_count() or 1) + 4)
    CONNECTION_POOL_SIZE = int(os.getenv('DB_POOL_SIZE', '10'))
    CACHE_TTL_SECONDS = int(os.getenv('DB_CACHE_TTL', '300'))  # 5 minutes
    MAX_CACHE_SIZE = int(os.getenv('DB_MAX_CACHE', '1000'))
    QUERY_TIMEOUT_SECONDS = int(os.getenv('DB_QUERY_TIMEOUT', '30'))
    BATCH_SIZE = int(os.getenv('DB_BATCH_SIZE', '100'))
    ENABLE_QUERY_CACHE = os.getenv('DB_ENABLE_CACHE', 'true').lower() == 'true'
    ENABLE_PERFORMANCE_MONITORING = os.getenv('DB_ENABLE_MONITORING', 'true').lower() == 'true'

# ================== CLASSI UTILITY ==================

class CacheableList(list):
    """Lista che puÃ² tenere traccia se proviene dalla cache"""
    def __init__(self, data):
        super().__init__(data)
        self._from_cache = False

# ================== CONNECTION POOL AVANZATO ==================

class ConnectionPool:
    """Pool di connessioni thread-safe con monitoring"""
    
    def __init__(self, pool_size: int = DatabaseConfig.CONNECTION_POOL_SIZE):
        self.pool_size = pool_size
        self._connections = deque()
        self._lock = threading.RLock()
        self._created_connections = 0
        self._active_connections = 0
        self._total_requests = 0
        self._cache_hits = 0
        
    def get_connection(self) -> sqlite3.Connection:
        """Ottiene connessione dal pool"""
        with self._lock:
            self._total_requests += 1
            
            if self._connections:
                self._cache_hits += 1
                conn = self._connections.popleft()
                self._active_connections += 1
                return conn
            
            if self._created_connections < self.pool_size:
                conn = get_connection()
                # Ottimizza connessione
                conn.execute("PRAGMA journal_mode=WAL")
                conn.execute("PRAGMA synchronous=NORMAL")
                conn.execute("PRAGMA cache_size=10000")
                conn.execute("PRAGMA temp_store=MEMORY")
                conn.execute("PRAGMA mmap_size=268435456")  # 256MB
                self._created_connections += 1
                self._active_connections += 1
                return conn
            
            # Fallback: crea connessione temporanea
            return get_connection()
    
    def return_connection(self, conn: sqlite3.Connection):
        """Restituisce connessione al pool"""
        with self._lock:
            if len(self._connections) < self.pool_size:
                self._connections.append(conn)
                self._active_connections -= 1
            else:
                conn.close()
                self._created_connections -= 1
    
    def get_stats(self) -> Dict[str, Any]:
        """Statistiche del pool"""
        with self._lock:
            return {
                'pool_size': self.pool_size,
                'available_connections': len(self._connections),
                'active_connections': self._active_connections,
                'total_requests': self._total_requests,
                'cache_hit_rate': self._cache_hits / max(1, self._total_requests),
                'created_connections': self._created_connections
            }
    
    def close_all(self):
        """Chiude tutte le connessioni"""
        with self._lock:
            while self._connections:
                conn = self._connections.popleft()
                conn.close()
            self._created_connections = 0
            self._active_connections = 0

_connection_pool = ConnectionPool()

@contextmanager
def get_pooled_connection():
    """Context manager per connessioni dal pool"""
    conn = _connection_pool.get_connection()
    try:
        yield conn
    finally:
        _connection_pool.return_connection(conn)

# ================== CACHE INTELLIGENTE QUERY ==================

@dataclass
class CacheEntry:
    """Entry della cache con metadati"""
    data: Any
    timestamp: float
    access_count: int = 0
    size_estimate: int = 0
    query_hash: str = ""

class IntelligentQueryCache:
    """Cache intelligente per query con algoritmi di eviction avanzati"""
    
    def __init__(self, max_size: int = DatabaseConfig.MAX_CACHE_SIZE):
        self.max_size = max_size
        self._cache: Dict[str, CacheEntry] = {}
        self._lock = threading.RLock()
        self.hit_count = 0
        self.miss_count = 0
        
    def _generate_cache_key(self, query: str, params: Tuple = ()) -> str:
        """Genera chiave cache normalizzata"""
        # Normalizza query (rimuovi spazi extra, commenti, ecc.)
        normalized_query = ' '.join(query.split())
        cache_data = f"{normalized_query}:{str(params)}"
        return hashlib.md5(cache_data.encode()).hexdigest()
    
    def get(self, query: str, params: Tuple = ()) -> Optional[Any]:
        """Ottiene risultato dalla cache"""
        if not DatabaseConfig.ENABLE_QUERY_CACHE:
            return None
            
        key = self._generate_cache_key(query, params)
        
        with self._lock:
            if key in self._cache:
                entry = self._cache[key]
                
                # Check TTL
                if time.time() - entry.timestamp > DatabaseConfig.CACHE_TTL_SECONDS:
                    del self._cache[key]
                    self.miss_count += 1
                    return None
                
                # Update access stats
                entry.access_count += 1
                self.hit_count += 1
                return entry.data
            
            self.miss_count += 1
            return None
    
    def set(self, query: str, params: Tuple, data: Any):
        """Imposta risultato in cache"""
        if not DatabaseConfig.ENABLE_QUERY_CACHE:
            return
            
        key = self._generate_cache_key(query, params)
        
        with self._lock:
            # Memory management
            if len(self._cache) >= self.max_size:
                self._evict_entries(0.2)  # Evict 20%
            
            size_estimate = self._estimate_size(data)
            entry = CacheEntry(
                data=data,
                timestamp=time.time(),
                access_count=1,
                size_estimate=size_estimate,
                query_hash=key
            )
            
            self._cache[key] = entry
    
    def _estimate_size(self, obj: Any) -> int:
        """Stima dimensione oggetto"""
        try:
            if isinstance(obj, list):
                return len(obj) * 100  # Stima per record
            elif isinstance(obj, dict):
                return len(str(obj))
            elif isinstance(obj, pd.DataFrame):
                return obj.memory_usage(deep=True).sum()
            else:
                return 1000  # Default
        except:
            return 1000
    
    def _evict_entries(self, eviction_ratio: float):
        """Evict entries con algoritmo LFU + LRU"""
        if not self._cache:
            return
        
        num_to_evict = max(1, int(len(self._cache) * eviction_ratio))
        
        # Score = access_count * time_factor
        current_time = time.time()
        scores = {}
        
        for key, entry in self._cache.items():
            time_factor = max(0.1, 1.0 - (current_time - entry.timestamp) / DatabaseConfig.CACHE_TTL_SECONDS)
            scores[key] = entry.access_count * time_factor
        
        # Evict lowest scoring
        keys_to_evict = sorted(scores.keys(), key=lambda k: scores[k])[:num_to_evict]
        
        for key in keys_to_evict:
            del self._cache[key]
    
    def clear(self):
        """Pulisce cache"""
        with self._lock:
            self._cache.clear()
            self.hit_count = 0
            self.miss_count = 0
    
    def get_stats(self) -> Dict[str, Any]:
        """Statistiche cache"""
        total_requests = self.hit_count + self.miss_count
        with self._lock:
            return {
                'entries': len(self._cache),
                'max_size': self.max_size,
                'hit_rate': self.hit_count / max(1, total_requests),
                'total_size_estimate': sum(entry.size_estimate for entry in self._cache.values()),
                'enabled': DatabaseConfig.ENABLE_QUERY_CACHE
            }

_query_cache = IntelligentQueryCache()

# ================== PERFORMANCE MONITORING ==================

@dataclass
class QueryMetrics:
    """Metriche per singola query"""
    query_type: str
    execution_time_ms: float
    rows_affected: int
    cache_hit: bool
    error_occurred: bool = False
    memory_usage_mb: float = 0.0

class PerformanceMonitor:
    """Monitor performance database"""
    
    def __init__(self):
        self.metrics: deque = deque(maxlen=1000)  # Keep last 1000 metrics
        self.query_counts = defaultdict(int)
        self.slow_queries = deque(maxlen=100)
        self._lock = threading.RLock()
        self.start_time = time.time()
    
    def record_query(self, metric: QueryMetrics):
        """Registra metrica query"""
        if not DatabaseConfig.ENABLE_PERFORMANCE_MONITORING:
            return
            
        with self._lock:
            self.metrics.append(metric)
            self.query_counts[metric.query_type] += 1
            
            # Track slow queries
            if metric.execution_time_ms > 1000:  # >1 second
                self.slow_queries.append({
                    'query_type': metric.query_type,
                    'execution_time_ms': metric.execution_time_ms,
                    'timestamp': time.time()
                })
    
    def get_summary(self) -> Dict[str, Any]:
        """Ottieni riassunto performance"""
        with self._lock:
            if not self.metrics:
                return {'status': 'no_data'}
            
            recent_metrics = list(self.metrics)[-100:]  # Last 100
            
            avg_time = sum(m.execution_time_ms for m in recent_metrics) / len(recent_metrics)
            cache_hits = sum(1 for m in recent_metrics if m.cache_hit)
            
            return {
                'total_queries': len(self.metrics),
                'avg_execution_time_ms': round(avg_time, 2),
                'cache_hit_rate': cache_hits / len(recent_metrics),
                'slow_queries_count': len(self.slow_queries),
                'query_distribution': dict(self.query_counts),
                'uptime_hours': (time.time() - self.start_time) / 3600
            }

_performance_monitor = PerformanceMonitor()

# ================== DECORATORI PERFORMANCE ==================

def performance_tracked(query_type: str):
    """Decoratore per tracking performance query"""
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            start_time = time.perf_counter()
            start_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
            
            try:
                result = await func(*args, **kwargs)
                
                end_time = time.perf_counter()
                end_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
                
                rows_affected = len(result) if isinstance(result, list) else 1
                cache_hit = getattr(result, '_from_cache', False)
                
                metric = QueryMetrics(
                    query_type=query_type,
                    execution_time_ms=(end_time - start_time) * 1000,
                    rows_affected=rows_affected,
                    cache_hit=cache_hit,
                    memory_usage_mb=end_memory - start_memory
                )
                
                _performance_monitor.record_query(metric)
                
                return result
                
            except Exception as e:
                end_time = time.perf_counter()
                metric = QueryMetrics(
                    query_type=query_type,
                    execution_time_ms=(end_time - start_time) * 1000,
                    rows_affected=0,
                    cache_hit=False,
                    error_occurred=True
                )
                _performance_monitor.record_query(metric)
                raise
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            start_time = time.perf_counter()
            
            try:
                result = func(*args, **kwargs)
                
                end_time = time.perf_counter()
                rows_affected = len(result) if isinstance(result, list) else 1
                
                metric = QueryMetrics(
                    query_type=query_type,
                    execution_time_ms=(end_time - start_time) * 1000,
                    rows_affected=rows_affected,
                    cache_hit=False
                )
                
                _performance_monitor.record_query(metric)
                return result
                
            except Exception as e:
                end_time = time.perf_counter()
                metric = QueryMetrics(
                    query_type=query_type,
                    execution_time_ms=(end_time - start_time) * 1000,
                    rows_affected=0,
                    cache_hit=False,
                    error_occurred=True
                )
                _performance_monitor.record_query(metric)
                raise
        
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    
    return decorator

# ================== DATABASE ADAPTER ULTRA-OTTIMIZZATO ==================

class DatabaseAdapterOptimized:
    """
    Database Adapter ULTRA-OTTIMIZZATO V3.0
    Performance-first design con connection pooling, cache intelligente, batch processing
    """
    
    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=DatabaseConfig.MAX_WORKERS)
        self.batch_processor = BatchProcessor()
        self.query_optimizer = QueryOptimizer()
        self.startup_time = time.time()
        self._query_stats = defaultdict(int)
        self._lock = threading.RLock()
        
        logger.info("DatabaseAdapterOptimized V3.0 initialized with connection pooling")
    
    # ===== CORE QUERY METHODS OTTIMIZZATI =====
    
    @performance_tracked('generic_query')
    async def execute_query_async(self, query: str, params: Tuple = None) -> List[Dict]:
        """Esegue query SELECT con cache intelligente"""
        
        # Check cache first
        cached_result = _query_cache.get(query, params or ())
        if cached_result is not None:
            # Crea un'istanza della nostra lista speciale per poter aggiungere il flag
            result_from_cache = CacheableList(cached_result)
            result_from_cache._from_cache = True
            return result_from_cache
        
        loop = asyncio.get_event_loop()
        
        def _execute_query():
            with get_pooled_connection() as conn:
                cursor = conn.cursor()
                
                if params:
                    cursor.execute(query, params)
                else:
                    cursor.execute(query)
                
                rows = cursor.fetchall()
                result = [dict(row) for row in rows] if rows else []
                
                # Cache result
                _query_cache.set(query, params or (), result)
                
                return result
        
        result = await loop.run_in_executor(self.executor, _execute_query)
        
        with self._lock:
            self._query_stats['SELECT'] += 1
        
        return result
    
    @performance_tracked('write_query')
    async def execute_write_async(self, query: str, params: Tuple = None) -> int:
        """Esegue query INSERT/UPDATE/DELETE ottimizzata"""
        
        loop = asyncio.get_event_loop()
        
        def _execute_write():
            with get_pooled_connection() as conn:
                cursor = conn.cursor()
                
                if params:
                    cursor.execute(query, params)
                else:
                    cursor.execute(query)
                
                conn.commit()
                
                # Invalida cache correlata
                self._invalidate_related_cache(query)
                
                if query.strip().upper().startswith('INSERT'):
                    return cursor.lastrowid
                else:
                    return cursor.rowcount
        
        result = await loop.run_in_executor(self.executor, _execute_write)
        
        with self._lock:
            if query.strip().upper().startswith('INSERT'):
                self._query_stats['INSERT'] += 1
            elif query.strip().upper().startswith('UPDATE'):
                self._query_stats['UPDATE'] += 1
            elif query.strip().upper().startswith('DELETE'):
                self._query_stats['DELETE'] += 1
        
        return result
    
    @performance_tracked('batch_write')
    async def execute_batch_write_async(self, query: str, params_list: List[Tuple]) -> List[int]:
        """Esegue batch di query write per performance ottimali"""
        
        if not params_list:
            return []
        
        # Process in chunks to avoid memory issues
        chunk_size = DatabaseConfig.BATCH_SIZE
        results = []
        
        loop = asyncio.get_event_loop()
        
        for i in range(0, len(params_list), chunk_size):
            chunk = params_list[i:i + chunk_size]
            
            def _execute_batch_chunk():
                with get_pooled_connection() as conn:
                    cursor = conn.cursor()
                    chunk_results = []
                    
                    try:
                        for params in chunk:
                            cursor.execute(query, params)
                            if query.strip().upper().startswith('INSERT'):
                                chunk_results.append(cursor.lastrowid)
                            else:
                                chunk_results.append(cursor.rowcount)
                        
                        conn.commit()
                        return chunk_results
                        
                    except Exception as e:
                        conn.rollback()
                        raise
            
            chunk_results = await loop.run_in_executor(self.executor, _execute_batch_chunk)
            results.extend(chunk_results)
        
        # Invalida cache
        self._invalidate_related_cache(query)
        
        with self._lock:
            self._query_stats['BATCH'] += 1
        
        return results
    
    # ===== FUNZIONI CORE OTTIMIZZATE =====
    
    @performance_tracked('create_tables')
    async def create_tables_async(self):
        """Crea tabelle con ottimizzazioni"""
        loop = asyncio.get_event_loop()
        
        def _create_optimized():
            # Use pooled connection
            with get_pooled_connection() as conn:
                create_tables()
                
                # Add indexes for performance
                performance_indexes = [
                    "CREATE INDEX IF NOT EXISTS idx_invoices_anagraphics ON Invoices(anagraphics_id)",
                    "CREATE INDEX IF NOT EXISTS idx_invoices_date ON Invoices(doc_date)",
                    "CREATE INDEX IF NOT EXISTS idx_invoices_status ON Invoices(payment_status)",
                    "CREATE INDEX IF NOT EXISTS idx_transactions_date ON BankTransactions(transaction_date)",
                    "CREATE INDEX IF NOT EXISTS idx_transactions_status ON BankTransactions(reconciliation_status)",
                    "CREATE INDEX IF NOT EXISTS idx_transactions_hash ON BankTransactions(unique_hash)",
                    "CREATE INDEX IF NOT EXISTS idx_recon_links_invoice ON ReconciliationLinks(invoice_id)",
                    "CREATE INDEX IF NOT EXISTS idx_recon_links_transaction ON ReconciliationLinks(transaction_id)",
                ]
                
                cursor = conn.cursor()
                for index_sql in performance_indexes:
                    try:
                        cursor.execute(index_sql)
                    except Exception as e:
                        logger.warning(f"Failed to create index: {e}")
                
                conn.commit()
                return True
        
        return await loop.run_in_executor(self.executor, _create_optimized)
    
    @performance_tracked('get_anagraphics')
    async def get_anagraphics_async(self, type_filter: str = None) -> List[Dict]:
        """Ottiene anagrafiche con cache"""
        
        # Check cache first
        cache_key = f"anagraphics_{type_filter or 'all'}"
        cached_result = _query_cache.get(f"SELECT * FROM anagraphics WHERE type = '{type_filter}'" if type_filter else "SELECT * FROM anagraphics", ())
        if cached_result is not None:
            return cached_result
        
        loop = asyncio.get_event_loop()
        
        def _get_anagraphics():
            df = get_anagraphics(type_filter)
            result = df.to_dict('records') if not df.empty else []
            
            # Cache result
            _query_cache.set(f"anagraphics_{type_filter or 'all'}", (), result)
            
            return result
        
        return await loop.run_in_executor(self.executor, _get_anagraphics)
    
    @performance_tracked('get_invoices')
    async def get_invoices_async(
        self,
        type_filter: str = None,
        status_filter: str = None,
        anagraphics_id_filter: int = None,
        limit: int = None
    ) -> List[Dict]:
        """Ottiene fatture con cache e ottimizzazioni"""
        
        loop = asyncio.get_event_loop()
        
        def _get_invoices():
            df = get_invoices(
                type_filter=type_filter,
                status_filter=status_filter,
                anagraphics_id_filter=anagraphics_id_filter,
                limit=limit
            )
            return df.to_dict('records') if not df.empty else []
        
        return await loop.run_in_executor(self.executor, _get_invoices)
    
    @performance_tracked('get_transactions')
    async def get_transactions_async(
        self,
        start_date: str = None,
        end_date: str = None,
        status_filter: str = None,
        limit: int = None,
        anagraphics_id_heuristic_filter: int = None,
        hide_pos: bool = False,
        hide_worldline: bool = False,
        hide_cash: bool = False,
        hide_commissions: bool = False
    ) -> List[Dict]:
        """Ottiene transazioni con cache e ottimizzazioni"""
        
        loop = asyncio.get_event_loop()
        
        def _get_transactions():
            df = get_transactions(
                start_date=start_date,
                end_date=end_date,
                status_filter=status_filter,
                limit=limit,
                anagraphics_id_heuristic_filter=anagraphics_id_heuristic_filter,
                hide_pos=hide_pos,
                hide_worldline=hide_worldline,
                hide_cash=hide_cash,
                hide_commissions=hide_commissions
            )
            return df.to_dict('records') if not df.empty else []
        
        return await loop.run_in_executor(self.executor, _get_transactions)
    
    # ===== BATCH OPERATIONS OTTIMIZZATE =====
    
    @performance_tracked('batch_anagraphics')
    async def add_anagraphics_batch_async(self, anagraphics_list: List[Dict]) -> List[Optional[int]]:
        """Aggiunge anagrafiche in batch ottimizzato"""
        
        if not anagraphics_list:
            return []
        
        loop = asyncio.get_event_loop()
        
        def _add_batch():
            results = []
            with get_pooled_connection() as conn:
                try:
                    for anag_data in anagraphics_list:
                        anag_type = anag_data.get('type', 'Cliente')
                        cursor = conn.cursor()
                        new_id = add_anagraphics_if_not_exists(cursor, anag_data, anag_type)
                        results.append(new_id)
                    
                    conn.commit()
                    return results
                    
                except Exception as e:
                    conn.rollback()
                    logger.error(f"Batch anagraphics add failed: {e}")
                    raise
        
        return await loop.run_in_executor(self.executor, _add_batch)
    
    @performance_tracked('batch_transactions')
    async def add_transactions_batch_async(self, transactions_df: pd.DataFrame) -> Tuple[int, int, int, int]:
        """Aggiunge transazioni in batch ultra-ottimizzato"""
        
        loop = asyncio.get_event_loop()
        
        def _add_transactions():
            with get_pooled_connection() as conn:
                cursor = conn.cursor()
                try:
                    result = add_transactions(cursor, transactions_df)
                    conn.commit()
                    return result
                except Exception as e:
                    conn.rollback()
                    raise
        
        return await loop.run_in_executor(self.executor, _add_transactions)
    
    # ===== RECONCILIATION OTTIMIZZATO =====
    
    @performance_tracked('reconciliation_links')
    async def get_reconciliation_links_async(self, item_type: str, item_id: int) -> List[Dict]:
        """Ottiene link riconciliazione con cache"""
        
        loop = asyncio.get_event_loop()
        
        def _get_links():
            df = get_reconciliation_links_for_item(item_type, item_id)
            return df.to_dict('records') if not df.empty else []
        
        return await loop.run_in_executor(self.executor, _get_links)
    
    @performance_tracked('item_details')
    async def get_item_details_async(self, item_type: str, item_id: int) -> Optional[Dict]:
        """Ottiene dettagli elemento con cache"""
        
        loop = asyncio.get_event_loop()
        
        def _get_details():
            with get_pooled_connection() as conn:
                row = get_item_details(conn, item_type, item_id)
                return dict(row) if row else None
        
        return await loop.run_in_executor(self.executor, _get_details)
    
    # ===== FUNZIONI UTILITY AVANZATE =====
    
    async def check_duplicate_async(self, table: str, column: str, value: str) -> bool:
        """Verifica duplicati con cache"""
        
        query = f"SELECT COUNT(*) as count FROM {table} WHERE {column} = ?"
        result = await self.execute_query_async(query, (value,))
        
        return result[0]['count'] > 0 if result else False
    
    async def update_transaction_state_async(
        self,
        transaction_id: int,
        reconciliation_status: str,
        reconciled_amount: float
    ) -> bool:
        """Aggiorna stato transazione ottimizzato"""
        
        query = """
            UPDATE BankTransactions 
            SET reconciliation_status = ?, reconciled_amount = ?, updated_at = datetime('now')
            WHERE id = ?
        """
        
        result = await self.execute_write_async(query, (reconciliation_status, reconciled_amount, transaction_id))
        return result > 0
    
    async def get_database_stats_async(self) -> Dict[str, Any]:
        """Ottiene statistiche database complete"""
        
        stats_queries = [
            ("tables_count", "SELECT COUNT(*) as count FROM sqlite_master WHERE type='table'"),
            ("invoices_count", "SELECT COUNT(*) as count FROM Invoices"),
            ("transactions_count", "SELECT COUNT(*) as count FROM BankTransactions"),
            ("anagraphics_count", "SELECT COUNT(*) as count FROM Anagraphics"),
            ("reconciliation_links_count", "SELECT COUNT(*) as count FROM ReconciliationLinks"),
        ]
        
        stats = {}
        for stat_name, query in stats_queries:
            try:
                result = await self.execute_query_async(query)
                stats[stat_name] = result[0]['count'] if result else 0
            except Exception as e:
                logger.warning(f"Failed to get {stat_name}: {e}")
                stats[stat_name] = 0
        
        # Add performance stats
        stats.update({
            'connection_pool': _connection_pool.get_stats(),
            'query_cache': _query_cache.get_stats(),
            'performance': _performance_monitor.get_summary(),
            'query_distribution': dict(self._query_stats),
            'uptime_hours': (time.time() - self.startup_time) / 3600
        })
        
        return stats
    
    # ===== ADVANCED OPTIMIZATION METHODS =====
    
    async def optimize_database_async(self) -> Dict[str, Any]:
        """Ottimizza database per performance"""
        
        loop = asyncio.get_event_loop()
        
        def _optimize():
            optimization_results = {}
            
            with get_pooled_connection() as conn:
                cursor = conn.cursor()
                
                # Analyze tables
                cursor.execute("ANALYZE")
                optimization_results['analyze'] = True
                
                # Vacuum if needed
                cursor.execute("PRAGMA integrity_check")
                integrity = cursor.fetchone()
                if integrity and integrity[0] == 'ok':
                    cursor.execute("VACUUM")
                    optimization_results['vacuum'] = True
                
                # Reindex
                cursor.execute("REINDEX")
                optimization_results['reindex'] = True
                
                conn.commit()
            
            return optimization_results
        
        return await loop.run_in_executor(self.executor, _optimize)
    
    async def clear_all_caches_async(self) -> Dict[str, Any]:
        """Pulisce tutte le cache"""
        
        _query_cache.clear()
        
        # Force garbage collection
        gc.collect()
        
        return {
            'query_cache_cleared': True,
            'garbage_collected': True,
            'timestamp': time.time()
        }
    
    def _invalidate_related_cache(self, query: str):
        """Invalida cache correlata basandosi sulla query"""
        
        query_upper = query.upper()
        
        # Simple invalidation based on table names
        if 'INVOICES' in query_upper:
            # Clear invoice-related cache entries
            pass
        elif 'BANKTRANSACTIONS' in query_upper:
            # Clear transaction-related cache entries
            pass
        elif 'ANAGRAPHICS' in query_upper:
            # Clear anagraphics-related cache entries
            pass
        
        # For now, we could implement more sophisticated invalidation
        # This is a placeholder for future enhancement
    
    # ===== LEGACY COMPATIBILITY METHODS =====
    
    async def add_anagraphics_async(self, anag_data: Dict, anag_type: str) -> Optional[int]:
        """Legacy method - single anagraphics add"""
        
        result = await self.add_anagraphics_batch_async([{**anag_data, 'type': anag_type}])
        return result[0] if result else None
    
    async def add_transactions_async(self, transactions_df: pd.DataFrame) -> Tuple[int, int, int, int]:
        """Legacy method - transactions add"""
        
        return await self.add_transactions_batch_async(transactions_df)
    
    async def update_invoice_reconciliation_async(
        self, 
        invoice_id: int, 
        payment_status: str, 
        paid_amount: float
    ) -> bool:
        """Aggiorna riconciliazione fattura"""
        
        loop = asyncio.get_event_loop()
        
        def _update_invoice():
            with get_pooled_connection() as conn:
                success = update_invoice_reconciliation_state(conn, invoice_id, payment_status, paid_amount)
                if success:
                    conn.commit()
                return success
        
        return await loop.run_in_executor(self.executor, _update_invoice)
    
    async def update_transaction_reconciliation_async(
        self, 
        transaction_id: int, 
        reconciliation_status: str, 
        reconciled_amount: float
    ) -> bool:
        """Aggiorna riconciliazione transazione"""
        
        return await self.update_transaction_state_async(
            transaction_id, reconciliation_status, reconciled_amount
        )
    
    async def add_reconciliation_link_async(
        self, 
        invoice_id: int, 
        transaction_id: int, 
        amount_to_add: float
    ) -> bool:
        """Aggiunge link riconciliazione"""
        
        loop = asyncio.get_event_loop()
        
        def _add_link():
            with get_pooled_connection() as conn:
                success = add_or_update_reconciliation_link(conn, invoice_id, transaction_id, amount_to_add)
                if success:
                    conn.commit()
                return success
        
        return await loop.run_in_executor(self.executor, _add_link)
    
    async def remove_reconciliation_links_async(
        self, 
        transaction_id: int = None, 
        invoice_id: int = None
    ) -> Tuple[bool, Tuple[List, List]]:
        """Rimuove link riconciliazione"""
        
        loop = asyncio.get_event_loop()
        
        def _remove_links():
            with get_pooled_connection() as conn:
                success, affected = remove_reconciliation_links(conn, transaction_id, invoice_id)
                if success:
                    conn.commit()
                return success, affected
        
        return await loop.run_in_executor(self.executor, _remove_links)
    
    async def check_database_exists_async(self) -> bool:
        """Verifica esistenza database"""
        
        try:
            result = await self.execute_query_async(
                "SELECT name FROM sqlite_master WHERE type='table' AND name='Anagraphics'"
            )
            return len(result) > 0
        except Exception as e:
            logger.error(f"Error checking database: {e}")
            return False


# ================== BATCH PROCESSOR AVANZATO ==================

class BatchProcessor:
    """Processore batch per operazioni massive"""
    
    def __init__(self):
        self.max_batch_size = DatabaseConfig.BATCH_SIZE
        
    async def process_large_dataset_async(
        self, 
        dataset: List[Any], 
        processor_func: callable,
        batch_size: int = None
    ) -> List[Any]:
        """Processa dataset grandi in batch"""
        
        if batch_size is None:
            batch_size = self.max_batch_size
        
        results = []
        
        for i in range(0, len(dataset), batch_size):
            batch = dataset[i:i + batch_size]
            batch_result = await processor_func(batch)
            results.extend(batch_result if isinstance(batch_result, list) else [batch_result])
        
        return results


# ================== QUERY OPTIMIZER ==================

class QueryOptimizer:
    """Ottimizzatore query per performance"""
    
    def __init__(self):
        self.optimization_rules = {
            'add_indexes': True,
            'use_prepared_statements': True,
            'optimize_joins': True,
            'limit_result_sets': True
        }
    
    def optimize_query(self, query: str) -> str:
        """Ottimizza query SQL"""
        
        optimized = query
        
        # Add LIMIT if missing on SELECT without ORDER BY
        if (optimized.upper().startswith('SELECT') and 
            'LIMIT' not in optimized.upper() and 
            'ORDER BY' not in optimized.upper()):
            optimized += ' LIMIT 1000'
        
        # Other optimizations can be added here
        
        return optimized
    
    def analyze_query_performance(self, query: str) -> Dict[str, Any]:
        """Analizza performance query"""
        
        analysis = {
            'estimated_complexity': 'medium',
            'suggestions': [],
            'has_indexes': True,  # Placeholder
            'execution_plan': 'optimized'  # Placeholder
        }
        
        # Add suggestions based on query structure
        if 'SELECT *' in query.upper():
            analysis['suggestions'].append('Consider selecting specific columns instead of *')
        
        if 'WHERE' not in query.upper() and 'SELECT' in query.upper():
            analysis['suggestions'].append('Consider adding WHERE clause to filter results')
        
        return analysis


# ================== HEALTH CHECK E DIAGNOSTICS ==================

class DatabaseHealthChecker:
    """Health checker per database"""
    
    def __init__(self, adapter: DatabaseAdapterOptimized):
        self.adapter = adapter
    
    async def comprehensive_health_check(self) -> Dict[str, Any]:
        """Health check completo"""
        
        health_results = {
            'overall_status': 'healthy',
            'checks': {},
            'performance_score': 0,
            'recommendations': []
        }
        
        # Connection test
        try:
            await self.adapter.execute_query_async("SELECT 1")
            health_results['checks']['connection'] = 'healthy'
        except Exception as e:
            health_results['checks']['connection'] = f'error: {e}'
            health_results['overall_status'] = 'unhealthy'
        
        # Performance test
        try:
            start_time = time.time()
            await self.adapter.execute_query_async("SELECT COUNT(*) FROM Invoices")
            query_time = (time.time() - start_time) * 1000
            
            if query_time < 100:
                health_results['checks']['performance'] = 'excellent'
                health_results['performance_score'] += 25
            elif query_time < 500:
                health_results['checks']['performance'] = 'good'
                health_results['performance_score'] += 15
            else:
                health_results['checks']['performance'] = 'slow'
                health_results['recommendations'].append('Consider database optimization')
        except Exception as e:
            health_results['checks']['performance'] = f'error: {e}'
        
        # Cache health
        cache_stats = _query_cache.get_stats()
        if cache_stats['hit_rate'] > 0.5:
            health_results['checks']['cache'] = 'effective'
            health_results['performance_score'] += 25
        elif cache_stats['hit_rate'] > 0.2:
            health_results['checks']['cache'] = 'moderate'
            health_results['performance_score'] += 15
        else:
            health_results['checks']['cache'] = 'low_efficiency'
            health_results['recommendations'].append('Review cache configuration')
        
        # Connection pool health
        pool_stats = _connection_pool.get_stats()
        if pool_stats['cache_hit_rate'] > 0.8:
            health_results['checks']['connection_pool'] = 'optimal'
            health_results['performance_score'] += 25
        else:
            health_results['checks']['connection_pool'] = 'suboptimal'
            health_results['recommendations'].append('Consider increasing pool size')
        
        # Final status
        if health_results['performance_score'] >= 75:
            health_results['overall_status'] = 'excellent'
        elif health_results['performance_score'] >= 50:
            health_results['overall_status'] = 'good'
        elif health_results['performance_score'] >= 25:
            health_results['overall_status'] = 'degraded'
        else:
            health_results['overall_status'] = 'poor'
        
        return health_results


# ================== ISTANZA GLOBALE ==================

_db_adapter_instance: Optional[DatabaseAdapterOptimized] = None
_adapter_lock = threading.Lock()

def get_database_adapter() -> DatabaseAdapterOptimized:
    """Factory function per ottenere istanza adapter ottimizzato"""
    global _db_adapter_instance
    
    if _db_adapter_instance is None:
        with _adapter_lock:
            if _db_adapter_instance is None:
                _db_adapter_instance = DatabaseAdapterOptimized()
                logger.info("DatabaseAdapterOptimized V3.0 instance created")
    
    return _db_adapter_instance

# Istanza per compatibilitÃ 
db_adapter_optimized = get_database_adapter()

# Health checker
health_checker = DatabaseHealthChecker(db_adapter_optimized)

# ================== CLEANUP E SHUTDOWN ==================

def cleanup_database_adapter():
    """Cleanup resources quando l'applicazione si chiude"""
    
    logger.info("Cleaning up database adapter resources")
    
    # Close connection pool
    _connection_pool.close_all()
    
    # Clear caches
    _query_cache.clear()
    
    # Shutdown thread pool
    if db_adapter_optimized and db_adapter_optimized.executor:
        db_adapter_optimized.executor.shutdown(wait=True)
    
    logger.info("Database adapter cleanup completed")

# Register cleanup at module level
import atexit
atexit.register(cleanup_database_adapter)


# ================== EXPORT PUBBLICO ==================

__all__ = [
    'DatabaseAdapterOptimized',
    'db_adapter',
    'db_adapter_optimized',
    'get_database_adapter',
    'DatabaseConfig',
    'ConnectionPool',
    'IntelligentQueryCache',
    'PerformanceMonitor',
    'BatchProcessor',
    'QueryOptimizer',
    'DatabaseHealthChecker',
    'health_checker',
    'cleanup_database_adapter'
]

# Version info
__version__ = '3.0.0'
__description__ = 'Ultra-optimized database adapter with connection pooling and intelligent caching'

logger.info(f"DatabaseAdapterOptimized V{__version__} loaded successfully!")
db_adapter = db_adapter_optimized
