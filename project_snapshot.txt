SNAPSHOT DEL PROGETTO
Cartella analizzata: /Users/gianlucabottamedi/Documents/fatture analyzer v2 react
(Relativa alla cartella di esecuzione dello script: /Users/gianlucabottamedi/Documents/fatture analyzer v2 react)
=======================================================

STRUTTURA DI CARTELLE E FILE (all'interno di 'fatture analyzer v2 react'):
-------------------------------------------------------
fatture analyzer v2 react/
    ├── FatturaAnalyzer-v2
    │   ├── .env.example
    │   ├── .gitignore
    │   ├── README.md
    │   ├── backend
    │   │   ├── Dockerfile
    │   │   ├── app
    │   │   │   ├── __init__.py
    │   │   │   ├── adapters
    │   │   │   │   ├── __init__.py
    │   │   │   │   ├── analytics_adapter.py
    │   │   │   │   ├── cloud_sync_adapter.py
    │   │   │   │   ├── database_adapter.py
    │   │   │   │   ├── importer_adapter.py
    │   │   │   │   └── reconciliation_adapter.py
    │   │   │   ├── api
    │   │   │   │   ├── __init__.py
    │   │   │   │   ├── anagraphics.py
    │   │   │   │   ├── analytics.py
    │   │   │   │   ├── import_export.py
    │   │   │   │   ├── invoices.py
    │   │   │   │   ├── reconciliation.py
    │   │   │   │   ├── sync.py
    │   │   │   │   └── transactions.py
    │   │   │   ├── config.py
    │   │   │   ├── core
    │   │   │   │   ├── __init__.py
    │   │   │   │   ├── analysis.py
    │   │   │   │   ├── cloud_sync.py
    │   │   │   │   ├── database.py
    │   │   │   │   ├── importer.py
    │   │   │   │   ├── parser_csv.py
    │   │   │   │   ├── parser_p7m.py
    │   │   │   │   ├── parser_xml.py
    │   │   │   │   ├── reconciliation.py
    │   │   │   │   ├── smart_client_reconciliation.py
    │   │   │   │   └── utils.py
    │   │   │   ├── main.py
    │   │   │   ├── middleware
    │   │   │   │   ├── __init__.py
    │   │   │   │   ├── auth.py
    │   │   │   │   ├── cors.py
    │   │   │   │   └── error_handler.py
    │   │   │   └── models
    │   │   │       ├── __init__.py
    │   │   │       ├── anagraphics.py
    │   │   │       ├── invoice.py
    │   │   │       ├── reconciliation.py
    │   │   │       └── transaction.py
    │   │   ├── requirements.txt
    │   │   └── run.py
    │   ├── config
    │   │   ├── database_schema.sql
    │   │   ├── development.ini
    │   │   └── production.ini
    │   ├── docs
    │   │   ├── API.md
    │   │   ├── DEPLOYMENT.md
    │   │   └── SETUP.md
    │   ├── frontend
    │   │   ├── package.json
    │   │   ├── postcss.config.js
    │   │   ├── public
    │   │   │   ├── favicon.ico
    │   │   │   └── icons
    │   │   ├── src
    │   │   │   ├── App.tsx
    │   │   │   ├── components
    │   │   │   │   ├── anagraphics
    │   │   │   │   │   ├── AnagraphicsForm.tsx
    │   │   │   │   │   └── AnagraphicsList.tsx
    │   │   │   │   ├── analytics
    │   │   │   │   │   ├── ChartsLibrary.tsx
    │   │   │   │   │   ├── ExportTools.tsx
    │   │   │   │   │   └── ReportsView.tsx
    │   │   │   │   ├── dashboard
    │   │   │   │   │   ├── CashFlowChart.tsx
    │   │   │   │   │   ├── Charts.tsx
    │   │   │   │   │   ├── DashboardView.tsx
    │   │   │   │   │   ├── KPICards.tsx
    │   │   │   │   │   ├── OverdueInvoices.tsx
    │   │   │   │   │   ├── RecentActivity.tsx
    │   │   │   │   │   ├── RevenueChart.tsx
    │   │   │   │   │   └── TopClientsTable.tsx
    │   │   │   │   ├── invoices
    │   │   │   │   │   ├── InvoiceDetail.tsx
    │   │   │   │   │   ├── InvoiceForm.tsx
    │   │   │   │   │   └── InvoiceList.tsx
    │   │   │   │   ├── layout
    │   │   │   │   │   ├── Header.tsx
    │   │   │   │   │   ├── Layout.tsx
    │   │   │   │   │   └── Sidebar.tsx
    │   │   │   │   ├── reconciliation
    │   │   │   │   │   ├── DragDropReconciliation.tsx
    │   │   │   │   │   ├── MatchSuggestions.tsx
    │   │   │   │   │   ├── ReconciliationActions.tsx
    │   │   │   │   │   └── ReconciliationView.tsx
    │   │   │   │   ├── transactions
    │   │   │   │   │   ├── TransactionImport.tsx
    │   │   │   │   │   └── TransactionList.tsx
    │   │   │   │   └── ui
    │   │   │   │       ├── badge.tsx
    │   │   │   │       ├── button.tsx
    │   │   │   │       ├── card.tsx
    │   │   │   │       ├── checkbox.tsx
    │   │   │   │       ├── dialog.tsx
    │   │   │   │       ├── dropdown-menu.tsx
    │   │   │   │       ├── index.ts
    │   │   │   │       ├── input.tsx
    │   │   │   │       ├── label.tsx
    │   │   │   │       ├── select.tsx
    │   │   │   │       ├── skeleton.tsx
    │   │   │   │       ├── sonner.tsx
    │   │   │   │       ├── table.tsx
    │   │   │   │       └── tooltip.tsx
    │   │   │   ├── global.css
    │   │   │   ├── hooks
    │   │   │   │   ├── useAnalytics.ts
    │   │   │   │   ├── useApi.ts
    │   │   │   │   ├── useInvoices.ts
    │   │   │   │   ├── useReconciliation.ts
    │   │   │   │   └── useTransactions.ts
    │   │   │   ├── lib
    │   │   │   │   ├── formatters.ts
    │   │   │   │   ├── utils.ts
    │   │   │   │   └── validations.ts
    │   │   │   ├── main.tsx
    │   │   │   ├── pages
    │   │   │   │   ├── AnagraphicsDetailPage.tsx
    │   │   │   │   ├── AnagraphicsPage.tsx
    │   │   │   │   ├── AnalyticsPage.tsx
    │   │   │   │   ├── DashboardPage.tsx
    │   │   │   │   ├── ImportExportPage.tsx
    │   │   │   │   ├── InvoiceDetailPage.tsx
    │   │   │   │   ├── InvoicesPage.tsx
    │   │   │   │   ├── ReconciliationPage.tsx
    │   │   │   │   ├── SettingsPage.tsx
    │   │   │   │   ├── TransactionDetailPage.tsx
    │   │   │   │   └── TransactionsPage.tsx
    │   │   │   ├── providers
    │   │   │   │   ├── AuthProvider.tsx
    │   │   │   │   ├── QueryClientProvider.tsx
    │   │   │   │   ├── ThemeProvider.tsx
    │   │   │   │   └── index.ts
    │   │   │   ├── services
    │   │   │   │   └── api.ts
    │   │   │   ├── store
    │   │   │   │   └── index.ts
    │   │   │   ├── styles
    │   │   │   │   ├── components.css
    │   │   │   │   └── globals.css
    │   │   │   ├── types
    │   │   │   │   └── index.ts
    │   │   │   └── utils
    │   │   │       ├── constants.ts
    │   │   │       ├── formatters.ts
    │   │   │       └── validators.ts
    │   │   ├── tailwind.config.js
    │   │   ├── tsconfig.json
    │   │   ├── tsconfig.node.json
    │   │   └── vite.config.ts
    │   ├── package.json
    │   ├── scripts
    │   │   ├── build.sh
    │   │   ├── dev.sh
    │   │   └── release.sh
    │   └── src-tauri
    │       ├── Cargo.toml
    │       ├── build.rs
    │       ├── icons
    │       │   ├── 128x128.png
    │       │   ├── 32x32.png
    │       │   ├── Square107x107Logo.png
    │       │   ├── Square150x150Logo.png
    │       │   ├── Square24x24Logo.png
    │       │   ├── Square284x284Logo.png
    │       │   ├── Square30x30Logo.png
    │       │   ├── Square310x310Logo.png
    │       │   ├── Square44x44Logo.png
    │       │   ├── Square70x70Logo.png
    │       │   ├── StoreLogo.png
    │       │   ├── icon.icns
    │       │   └── icon.ico
    │       ├── src
    │       │   ├── commands.rs
    │       │   ├── lib.rs
    │       │   └── main.rs
    │       └── tauri.conf.json
    └── create.py


CONTENUTO DEI FILE DI TESTO (da 'fatture analyzer v2 react'):
-------------------------------------------------

--- Contenuto di: FatturaAnalyzer-v2/.env.example ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/.env.example ---

--- Contenuto di: FatturaAnalyzer-v2/.gitignore ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/.gitignore ---

--- Contenuto di: FatturaAnalyzer-v2/README.md ---
# FatturaAnalyzer-v2

Descrizione del progetto FatturaAnalyzer v2.

Path: README.md

--- Fine contenuto di: FatturaAnalyzer-v2/README.md ---

--- Contenuto di: FatturaAnalyzer-v2/backend/Dockerfile ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/backend/Dockerfile ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/__init__.py ---
# Path: backend/app/__init__.py

# Questo file rende la directory un package Python

--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/__init__.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/adapters/__init__.py ---
# Path: backend/app/adapters/__init__.py

# Questo file rende la directory un package Python

--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/adapters/__init__.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/adapters/analytics_adapter.py ---
"""
Analytics Adapter per FastAPI
Fornisce interfaccia async per il core/analysis.py esistente senza modificarlo
"""

import asyncio
import logging
from typing import Dict, Any, Optional, List
from concurrent.futures import ThreadPoolExecutor
import pandas as pd

# Import del core esistente (INVARIATO)
from app.core.analysis import (
    calculate_main_kpis,
    get_monthly_cash_flow_analysis,
    get_monthly_revenue_analysis,
    get_top_clients_by_revenue,
    get_top_overdue_invoices,
    get_product_analysis,
    get_aging_summary,
    get_anagraphic_financial_summary,
    calculate_and_update_client_scores
)

logger = logging.getLogger(__name__)

# Thread pool per operazioni sincrone del core
_thread_pool = ThreadPoolExecutor(max_workers=3)  # Più workers per analisi

class AnalyticsAdapter:
    """
    Adapter che fornisce interfaccia async per le funzioni di analisi del core esistente
    """
    
    @staticmethod
    async def calculate_main_kpis_async() -> Dict[str, Any]:
        """Versione async di calculate_main_kpis"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, calculate_main_kpis)
    
    @staticmethod
    async def get_monthly_cash_flow_analysis_async(months: int = 12) -> pd.DataFrame:
        """Versione async di get_monthly_cash_flow_analysis"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool, 
            get_monthly_cash_flow_analysis, 
            months
        )
    
    @staticmethod
    async def get_monthly_revenue_analysis_async(
        months: int = 12, 
        invoice_type: Optional[str] = None
    ) -> pd.DataFrame:
        """Versione async di get_monthly_revenue_analysis"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            get_monthly_revenue_analysis,
            months,
            invoice_type
        )
    
    @staticmethod
    async def get_top_clients_by_revenue_async(
        limit: int = 20,
        period_months: int = 12,
        min_revenue: Optional[float] = None
    ) -> pd.DataFrame:
        """Versione async di get_top_clients_by_revenue"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            get_top_clients_by_revenue,
            limit,
            period_months,
            min_revenue
        )
    
    @staticmethod
    async def get_top_overdue_invoices_async(
        limit: int = 20,
        priority_sort: bool = True
    ) -> pd.DataFrame:
        """Versione async di get_top_overdue_invoices"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            get_top_overdue_invoices,
            limit,
            priority_sort
        )
    
    @staticmethod
    async def get_product_analysis_async(
        limit: int = 50,
        period_months: int = 12,
        min_quantity: Optional[float] = None,
        invoice_type: Optional[str] = None
    ) -> pd.DataFrame:
        """Versione async di get_product_analysis"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            get_product_analysis,
            limit,
            period_months,
            min_quantity,
            invoice_type
        )
    
    @staticmethod
    async def get_aging_summary_async(invoice_type: str = "Attiva") -> Dict[str, Dict]:
        """Versione async di get_aging_summary"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            get_aging_summary,
            invoice_type
        )
    
    @staticmethod
    async def get_anagraphic_financial_summary_async(anagraphics_id: int) -> Dict[str, Any]:
        """Versione async di get_anagraphic_financial_summary"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            get_anagraphic_financial_summary,
            anagraphics_id
        )
    
    @staticmethod
    async def calculate_and_update_client_scores_async() -> bool:
        """Versione async di calculate_and_update_client_scores"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            calculate_and_update_client_scores
        )
    
    @staticmethod
    async def get_dashboard_data_async() -> Dict[str, Any]:
        """Ottiene tutti i dati dashboard in una chiamata"""
        def _get_dashboard_data():
            try:
                # Calcola KPI principali
                kpis = calculate_main_kpis()
                
                # Cash flow ultimi 6 mesi
                cash_flow_df = get_monthly_cash_flow_analysis(6)
                cash_flow_summary = cash_flow_df.to_dict('records') if not cash_flow_df.empty else []
                
                # Top clienti
                top_clients_df = get_top_clients_by_revenue(10)
                top_clients = top_clients_df.to_dict('records') if not top_clients_df.empty else []
                
                # Fatture scadute
                overdue_invoices_df = get_top_overdue_invoices(5)
                overdue_invoices = overdue_invoices_df.to_dict('records') if not overdue_invoices_df.empty else []
                
                return {
                    'kpis': kpis,
                    'cash_flow_summary': cash_flow_summary,
                    'top_clients': top_clients,
                    'overdue_invoices': overdue_invoices
                }
                
            except Exception as e:
                logger.error(f"Error getting dashboard data: {e}")
                # Ritorna dati vuoti in caso di errore
                return {
                    'kpis': {},
                    'cash_flow_summary': [],
                    'top_clients': [],
                    'overdue_invoices': []
                }
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _get_dashboard_data)
    
    @staticmethod
    async def get_revenue_trends_async(
        period: str = "monthly",
        months_back: int = 12
    ) -> List[Dict[str, Any]]:
        """Calcola trend fatturato usando database adapter"""
        from app.adapters.database_adapter import db_adapter
        
        # Formatta date based on period
        if period == "daily":
            date_trunc = "date(doc_date)"
        elif period == "weekly":
            date_trunc = "strftime('%Y-W%W', doc_date)"
        elif period == "quarterly":
            date_trunc = "strftime('%Y', doc_date) || '-Q' || ((CAST(strftime('%m', doc_date) AS INTEGER) - 1) / 3 + 1)"
        else:  # monthly
            date_trunc = "strftime('%Y-%m', doc_date)"
        
        trends_query = f"""
            SELECT 
                {date_trunc} as period,
                type,
                COUNT(*) as invoice_count,
                SUM(total_amount) as total_revenue,
                AVG(total_amount) as avg_invoice_amount,
                SUM(paid_amount) as total_paid,
                SUM(total_amount - paid_amount) as total_outstanding
            FROM Invoices
            WHERE doc_date >= date('now', '-{months_back} months')
            GROUP BY {date_trunc}, type
            ORDER BY period, type
        """
        
        return await db_adapter.execute_query_async(trends_query)
    
    @staticmethod
    async def get_payment_performance_async() -> Dict[str, Any]:
        """Calcola metriche performance pagamenti"""
        from app.adapters.database_adapter import db_adapter
        
        # Performance pagamenti
        payment_performance_query = """
            WITH payment_times AS (
                SELECT 
                    i.id,
                    i.doc_date,
                    i.due_date,
                    i.total_amount,
                    i.payment_status,
                    CASE 
                        WHEN i.payment_status = 'Pagata Tot.' THEN
                            (SELECT MAX(reconciliation_date) FROM ReconciliationLinks rl WHERE rl.invoice_id = i.id)
                        ELSE NULL
                    END as estimated_payment_date
                FROM Invoices i
                WHERE i.type = 'Attiva'
                  AND i.payment_status IN ('Pagata Tot.', 'Aperta', 'Scaduta', 'Pagata Parz.')
            )
            SELECT 
                payment_status,
                COUNT(*) as count,
                AVG(
                    CASE 
                        WHEN estimated_payment_date IS NOT NULL THEN
                            julianday(estimated_payment_date) - julianday(doc_date)
                        WHEN due_date IS NOT NULL THEN
                            julianday('now') - julianday(due_date)
                        ELSE NULL
                    END
                ) as avg_days,
                SUM(total_amount) as total_amount
            FROM payment_times
            GROUP BY payment_status
        """
        
        performance = await db_adapter.execute_query_async(payment_performance_query)
        
        # Efficienza incasso
        collection_query = """
            SELECT 
                strftime('%Y-%m', doc_date) as month,
                COUNT(*) as invoices_issued,
                COUNT(CASE WHEN payment_status = 'Pagata Tot.' THEN 1 END) as invoices_paid,
                SUM(total_amount) as total_issued,
                SUM(paid_amount) as total_collected,
                (CAST(COUNT(CASE WHEN payment_status = 'Pagata Tot.' THEN 1 END) AS REAL) / COUNT(*)) * 100 as collection_rate_pct
            FROM Invoices
            WHERE type = 'Attiva'
              AND doc_date >= date('now', '-12 months')
            GROUP BY strftime('%Y-%m', doc_date)
            ORDER BY month
        """
        
        collection = await db_adapter.execute_query_async(collection_query)
        
        return {
            'payment_performance': performance,
            'collection_efficiency': collection
        }
    
    @staticmethod
    async def get_client_segmentation_async(
        segmentation_type: str = "revenue",
        period_months: int = 12
    ) -> List[Dict[str, Any]]:
        """Calcola segmentazione clienti"""
        from app.adapters.database_adapter import db_adapter
        
        if segmentation_type == "revenue":
            # Segmentazione RFM per fatturato
            segmentation_query = f"""
                WITH client_metrics AS (
                    SELECT 
                        a.id,
                        a.denomination,
                        COUNT(i.id) as invoice_count,
                        SUM(i.total_amount) as total_revenue,
                        AVG(i.total_amount) as avg_order_value,
                        MAX(i.doc_date) as last_order_date,
                        MIN(i.doc_date) as first_order_date
                    FROM Anagraphics a
                    JOIN Invoices i ON a.id = i.anagraphics_id
                    WHERE a.type = 'Cliente'
                      AND i.type = 'Attiva'
                      AND i.doc_date >= date('now', '-{period_months} months')
                    GROUP BY a.id, a.denomination
                ),
                revenue_quartiles AS (
                    SELECT 
                        *,
                        NTILE(4) OVER (ORDER BY total_revenue) as revenue_quartile
                    FROM client_metrics
                )
                SELECT 
                    revenue_quartile,
                    CASE 
                        WHEN revenue_quartile = 4 THEN 'High Value'
                        WHEN revenue_quartile = 3 THEN 'Medium-High Value'
                        WHEN revenue_quartile = 2 THEN 'Medium Value'
                        ELSE 'Low Value'
                    END as segment_name,
                    COUNT(*) as client_count,
                    SUM(total_revenue) as segment_revenue,
                    AVG(total_revenue) as avg_revenue_per_client,
                    AVG(invoice_count) as avg_orders_per_client,
                    AVG(avg_order_value) as avg_order_value
                FROM revenue_quartiles
                GROUP BY revenue_quartile
                ORDER BY revenue_quartile DESC
            """
            
        elif segmentation_type == "frequency":
            segmentation_query = f"""
                WITH client_frequency AS (
                    SELECT 
                        a.id,
                        a.denomination,
                        COUNT(i.id) as order_frequency,
                        SUM(i.total_amount) as total_revenue
                    FROM Anagraphics a
                    JOIN Invoices i ON a.id = i.anagraphics_id
                    WHERE a.type = 'Cliente'
                      AND i.type = 'Attiva'
                      AND i.doc_date >= date('now', '-{period_months} months')
                    GROUP BY a.id, a.denomination
                )
                SELECT 
                    CASE 
                        WHEN order_frequency >= 10 THEN 'Very Frequent (10+)'
                        WHEN order_frequency >= 5 THEN 'Frequent (5-9)'
                        WHEN order_frequency >= 2 THEN 'Regular (2-4)'
                        ELSE 'Occasional (1)'
                    END as segment_name,
                    COUNT(*) as client_count,
                    SUM(total_revenue) as segment_revenue,
                    AVG(total_revenue) as avg_revenue_per_client,
                    AVG(order_frequency) as avg_order_frequency
                FROM client_frequency
                GROUP BY 
                    CASE 
                        WHEN order_frequency >= 10 THEN 'Very Frequent (10+)'
                        WHEN order_frequency >= 5 THEN 'Frequent (5-9)'
                        WHEN order_frequency >= 2 THEN 'Regular (2-4)'
                        ELSE 'Occasional (1)'
                    END
                ORDER BY avg_order_frequency DESC
            """
            
        elif segmentation_type == "recency":
            segmentation_query = """
                WITH client_recency AS (
                    SELECT 
                        a.id,
                        a.denomination,
                        MAX(i.doc_date) as last_order_date,
                        julianday('now') - julianday(MAX(i.doc_date)) as days_since_last_order,
                        COUNT(i.id) as total_orders,
                        SUM(i.total_amount) as total_revenue
                    FROM Anagraphics a
                    JOIN Invoices i ON a.id = i.anagraphics_id
                    WHERE a.type = 'Cliente'
                      AND i.type = 'Attiva'
                    GROUP BY a.id, a.denomination
                )
                SELECT 
                    CASE 
                        WHEN days_since_last_order <= 30 THEN 'Active (≤30 days)'
                        WHEN days_since_last_order <= 90 THEN 'Recent (31-90 days)'
                        WHEN days_since_last_order <= 180 THEN 'Lapsed (91-180 days)'
                        WHEN days_since_last_order <= 365 THEN 'At Risk (181-365 days)'
                        ELSE 'Lost (>365 days)'
                    END as segment_name,
                    COUNT(*) as client_count,
                    SUM(total_revenue) as segment_revenue,
                    AVG(total_revenue) as avg_revenue_per_client,
                    AVG(days_since_last_order) as avg_days_since_last_order
                FROM client_recency
                GROUP BY 
                    CASE 
                        WHEN days_since_last_order <= 30 THEN 'Active (≤30 days)'
                        WHEN days_since_last_order <= 90 THEN 'Recent (31-90 days)'
                        WHEN days_since_last_order <= 180 THEN 'Lapsed (91-180 days)'
                        WHEN days_since_last_order <= 365 THEN 'At Risk (181-365 days)'
                        ELSE 'Lost (>365 days)'
                    END
                ORDER BY avg_days_since_last_order
            """
        else:
            raise ValueError(f"Unsupported segmentation type: {segmentation_type}")
        
        return await db_adapter.execute_query_async(segmentation_query)
    
    @staticmethod
    async def get_cash_flow_forecast_async(
        months_ahead: int = 6,
        include_scheduled: bool = True
    ) -> Dict[str, Any]:
        """Genera forecast cash flow"""
        from app.adapters.database_adapter import db_adapter
        from datetime import datetime, timedelta
        
        # Ottieni pattern storici mensili
        historical_query = """
            SELECT 
                strftime('%m', transaction_date) as month_num,
                AVG(monthly_inflow) as avg_inflow,
                AVG(monthly_outflow) as avg_outflow,
                AVG(monthly_net) as avg_net
            FROM (
                SELECT 
                    strftime('%Y-%m', transaction_date) as year_month,
                    transaction_date,
                    SUM(CASE WHEN amount > 0 THEN amount ELSE 0 END) as monthly_inflow,
                    SUM(CASE WHEN amount < 0 THEN ABS(amount) ELSE 0 END) as monthly_outflow,
                    SUM(amount) as monthly_net
                FROM BankTransactions
                WHERE transaction_date >= date('now', '-24 months')
                GROUP BY strftime('%Y-%m', transaction_date)
            ) monthly_data
            GROUP BY strftime('%m', transaction_date)
            ORDER BY month_num
        """
        
        historical = await db_adapter.execute_query_async(historical_query)
        
        scheduled_receivables = []
        scheduled_payables = []
        
        if include_scheduled:
            # Crediti programmati (fatture aperte con scadenza)
            scheduled_receivables_query = f"""
                SELECT 
                    strftime('%Y-%m', due_date) as due_month,
                    SUM(total_amount - paid_amount) as expected_inflow,
                    COUNT(*) as invoice_count
                FROM Invoices
                WHERE type = 'Attiva'
                  AND payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.')
                  AND due_date IS NOT NULL
                  AND due_date <= date('now', '+{months_ahead} months')
                  AND (total_amount - paid_amount) > 0
                GROUP BY strftime('%Y-%m', due_date)
                ORDER BY due_month
            """
            
            scheduled_payables_query = f"""
                SELECT 
                    strftime('%Y-%m', due_date) as due_month,
                    SUM(total_amount - paid_amount) as expected_outflow,
                    COUNT(*) as invoice_count
                FROM Invoices
                WHERE type = 'Passiva'
                  AND payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.')
                  AND due_date IS NOT NULL
                  AND due_date <= date('now', '+{months_ahead} months')
                  AND (total_amount - paid_amount) > 0
                GROUP BY strftime('%Y-%m', due_date)
                ORDER BY due_month
            """
            
            scheduled_receivables = await db_adapter.execute_query_async(scheduled_receivables_query)
            scheduled_payables = await db_adapter.execute_query_async(scheduled_payables_query)
        
        # Genera forecast
        forecast = []
        current_date = datetime.now()
        
        for i in range(months_ahead):
            forecast_date = current_date + timedelta(days=30 * i)
            month_num = forecast_date.strftime('%m')
            year_month = forecast_date.strftime('%Y-%m')
            
            # Trova pattern storico per questo mese
            historical_pattern = next(
                (h for h in historical if h['month_num'] == month_num), 
                {'avg_inflow': 0, 'avg_outflow': 0, 'avg_net': 0}
            )
            
            # Trova importi programmati per questo mese
            scheduled_in = next(
                (s['expected_inflow'] for s in scheduled_receivables if s['due_month'] == year_month),
                0
            )
            scheduled_out = next(
                (s['expected_outflow'] for s in scheduled_payables if s['due_month'] == year_month),
                0
            )
            
            # Combina storico e programmato
            forecast_inflow = (historical_pattern['avg_inflow'] or 0) + scheduled_in
            forecast_outflow = (historical_pattern['avg_outflow'] or 0) + scheduled_out
            forecast_net = forecast_inflow - forecast_outflow
            
            forecast.append({
                'month': year_month,
                'forecasted_inflow': forecast_inflow,
                'forecasted_outflow': forecast_outflow,
                'forecasted_net': forecast_net,
                'scheduled_receivables': scheduled_in,
                'scheduled_payables': scheduled_out,
                'historical_avg_inflow': historical_pattern['avg_inflow'] or 0,
                'historical_avg_outflow': historical_pattern['avg_outflow'] or 0
            })
        
        return {
            'forecast': forecast,
            'historical_patterns': historical,
            'methodology': 'Historical averages + scheduled payments'
        }
    
    @staticmethod
    async def get_year_over_year_comparison_async(
        metric: str = "revenue",
        current_year: Optional[int] = None
    ) -> Dict[str, Any]:
        """Confronto anno su anno"""
        from app.adapters.database_adapter import db_adapter
        from datetime import datetime
        
        if current_year is None:
            current_year = datetime.now().year
        
        previous_year = current_year - 1
        
        if metric == "revenue":
            comparison_query = """
                SELECT 
                    strftime('%m', doc_date) as month,
                    CAST(strftime('%Y', doc_date) AS INTEGER) as year,
                    type,
                    SUM(total_amount) as total_amount,
                    COUNT(*) as invoice_count,
                    AVG(total_amount) as avg_invoice_amount
                FROM Invoices
                WHERE CAST(strftime('%Y', doc_date) AS INTEGER) IN (?, ?)
                GROUP BY strftime('%m', doc_date), CAST(strftime('%Y', doc_date) AS INTEGER), type
                ORDER BY month, year, type
            """
            
            comparison = await db_adapter.execute_query_async(comparison_query, (previous_year, current_year))
            
        elif metric == "profit":
            # Calcolo profit semplificato (ricavi - costi)
            comparison_query = """
                SELECT 
                    strftime('%m', doc_date) as month,
                    CAST(strftime('%Y', doc_date) AS INTEGER) as year,
                    SUM(CASE WHEN type = 'Attiva' THEN total_amount ELSE 0 END) as revenue,
                    SUM(CASE WHEN type = 'Passiva' THEN total_amount ELSE 0 END) as costs,
                    (SUM(CASE WHEN type = 'Attiva' THEN total_amount ELSE 0 END) - 
                     SUM(CASE WHEN type = 'Passiva' THEN total_amount ELSE 0 END)) as profit
                FROM Invoices
                WHERE CAST(strftime('%Y', doc_date) AS INTEGER) IN (?, ?)
                GROUP BY strftime('%m', doc_date), CAST(strftime('%Y', doc_date) AS INTEGER)
                ORDER BY month, year
            """
            
            comparison = await db_adapter.execute_query_async(comparison_query, (previous_year, current_year))
            
        elif metric == "volume":
            comparison_query = """
                SELECT 
                    strftime('%m', doc_date) as month,
                    CAST(strftime('%Y', doc_date) AS INTEGER) as year,
                    type,
                    COUNT(*) as invoice_count,
                    COUNT(DISTINCT anagraphics_id) as unique_clients
                FROM Invoices
                WHERE CAST(strftime('%Y', doc_date) AS INTEGER) IN (?, ?)
                GROUP BY strftime('%m', doc_date), CAST(strftime('%Y', doc_date) AS INTEGER), type
                ORDER BY month, year, type
            """
            
            comparison = await db_adapter.execute_query_async(comparison_query, (previous_year, current_year))
        else:
            raise ValueError(f"Unsupported metric: {metric}")
        
        # Calcola variazioni percentuali
        comparison_with_changes = []
        current_year_data = [c for c in comparison if c['year'] == current_year]
        previous_year_data = [c for c in comparison if c['year'] == previous_year]
        
        for current in current_year_data:
            # Trova dato anno precedente corrispondente
            previous = next(
                (p for p in previous_year_data 
                 if p['month'] == current['month'] and 
                    (p.get('type') == current.get('type') if 'type' in current else True)),
                None
            )
            
            if previous:
                # Calcola variazioni per campi numerici
                changes = {}
                for key in current:
                    if key not in ['month', 'year', 'type'] and isinstance(current[key], (int, float)):
                        prev_val = previous.get(key, 0) or 0
                        curr_val = current[key] or 0
                        if prev_val != 0:
                            changes[f'{key}_change_pct'] = ((curr_val - prev_val) / prev_val) * 100
                        else:
                            changes[f'{key}_change_pct'] = 0 if curr_val == 0 else 100
                        changes[f'{key}_change_abs'] = curr_val - prev_val
                
                comparison_with_changes.append({
                    **current,
                    'previous_year_data': previous,
                    'changes': changes
                })
            else:
                comparison_with_changes.append({
                    **current,
                    'previous_year_data': None,
                    'changes': {'note': 'No previous year data available'}
                })
        
        return {
            'metric': metric,
            'current_year': current_year,
            'previous_year': previous_year,
            'comparison': comparison_with_changes,
            'raw_data': comparison
        }

# Istanza globale dell'adapter
analytics_adapter = AnalyticsAdapter()
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/adapters/analytics_adapter.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/adapters/cloud_sync_adapter.py ---
"""
Cloud Sync Adapter per FastAPI
Fornisce interfaccia async per il core/cloud_sync.py esistente senza modificarlo
"""

import asyncio
import logging
import os # Aggiunto import os mancante per get_sync_config_async
from typing import Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor

# Import del core esistente (INVARIATO)
# Assicurati che questo import funzioni correttamente in base alla tua struttura.
# Se core/cloud_sync.py è in app/core/cloud_sync.py:
from app.core.cloud_sync import get_sync_manager
# Altrimenti, aggiusta il percorso di importazione.

logger = logging.getLogger(__name__)

# Thread pool per operazioni sincrone del core
_thread_pool = ThreadPoolExecutor(max_workers=1)  # Single worker per sync

class CloudSyncAdapter:
    """
    Adapter che fornisce interfaccia async per il cloud sync core esistente
    """

    @staticmethod
    async def get_sync_status_async() -> Dict[str, Any]:
        """Versione async di get_sync_status"""
        def _get_status():
            sync_manager = get_sync_manager()
            return sync_manager.get_sync_status()

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _get_status) # Corretto: passa _get_status

    @staticmethod
    async def get_sync_config_async() -> Dict[str, Any]:
        """Ottiene configurazione sync in modo async"""
        def _get_config():
            sync_manager = get_sync_manager()
            return {
                "sync_enabled": sync_manager.sync_enabled,
                "auto_sync_interval": sync_manager.auto_sync_interval,
                "remote_file_name": sync_manager.remote_db_name,
                "credentials_file_exists": os.path.exists(sync_manager.credentials_file),
                "token_file_exists": os.path.exists(sync_manager.token_file),
                "service_available": sync_manager.service is not None,
                "remote_file_id": sync_manager.remote_file_id
            }

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _get_config)

    @staticmethod
    async def reset_authorization_async() -> bool:
        """Reset autorizzazione in modo async"""
        def _reset_auth():
            # import os # os è già importato a livello di modulo
            sync_manager = get_sync_manager()

            # Stop auto-sync first
            sync_manager.stop_auto_sync()

            # Delete token file if it exists
            if os.path.exists(sync_manager.token_file):
                os.remove(sync_manager.token_file)
                logger.info("Token file deleted")

            # Reset service
            sync_manager.service = None
            return True

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _reset_auth)

    @staticmethod
    async def sync_database_async(force_direction: Optional[str] = None) -> Dict[str, Any]:
        """Versione async di sync_database"""
        def _sync_database():
            sync_manager = get_sync_manager()
            return sync_manager.sync_database(force_direction=force_direction)

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _sync_database)

    @staticmethod
    async def enable_sync_async() -> bool:
        """Abilita la sincronizzazione in modo async"""
        def _enable_sync():
            sync_manager = get_sync_manager()
            sync_manager.sync_enabled = True
            sync_manager._save_config() # Assumendo che _save_config esista in sync_manager
            sync_manager.start_auto_sync()
            return True

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _enable_sync)

    @staticmethod
    async def disable_sync_async() -> bool:
        """Disabilita la sincronizzazione in modo async"""
        def _disable_sync():
            sync_manager = get_sync_manager()
            sync_manager.stop_auto_sync()
            sync_manager.sync_enabled = False
            sync_manager._save_config() # Assumendo che _save_config esista in sync_manager
            return True

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _disable_sync)

    @staticmethod
    async def start_auto_sync_async() -> bool:
        """Avvia auto-sync in modo async"""
        def _start_auto_sync():
            sync_manager = get_sync_manager()
            sync_manager.start_auto_sync()
            return True

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _start_auto_sync)

    @staticmethod
    async def stop_auto_sync_async() -> bool:
        """Ferma auto-sync in modo async"""
        def _stop_auto_sync():
            sync_manager = get_sync_manager()
            sync_manager.stop_auto_sync()
            return True

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _stop_auto_sync)

    @staticmethod
    async def update_auto_sync_interval_async(interval_seconds: int) -> bool:
        """Aggiorna intervallo auto-sync in modo async"""
        def _update_interval():
            sync_manager = get_sync_manager()
            sync_manager.auto_sync_interval = interval_seconds
            sync_manager._save_config() # Assumendo che _save_config esista in sync_manager

            # Restart auto-sync if running
            if sync_manager.sync_thread and sync_manager.sync_thread.is_alive(): # Assumendo che sync_thread esista
                sync_manager.stop_auto_sync()
                sync_manager.start_auto_sync()
            return True

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _update_interval)

    @staticmethod
    async def get_remote_file_info_async() -> Dict[str, Any]:
        """Ottiene info file remoto in modo async"""
        def _get_remote_info():
            sync_manager = get_sync_manager()

            if not sync_manager.service or not sync_manager.remote_file_id: # Assumendo che queste proprietà esistano
                return {"exists": False}

            try:
                file_metadata = sync_manager.service.files().get(
                    fileId=sync_manager.remote_file_id,
                    fields='id,name,size,modifiedTime,createdTime,md5Checksum'
                ).execute()

                remote_modified_time = sync_manager._get_remote_modified_time() # Assumendo che esista
                local_modified_time = sync_manager._get_local_modified_time()   # Assumendo che esista

                return {
                    "exists": True,
                    "file_id": file_metadata['id'],
                    "name": file_metadata['name'],
                    "size": int(file_metadata.get('size', 0)),
                    "size_mb": round(int(file_metadata.get('size', 0)) / (1024*1024), 2),
                    "created_time": file_metadata.get('createdTime'),
                    "modified_time": file_metadata.get('modifiedTime'),
                    "md5_checksum": file_metadata.get('md5Checksum'),
                    "local_modified_time": local_modified_time.isoformat() if local_modified_time else None,
                    "remote_modified_time": remote_modified_time.isoformat() if remote_modified_time else None,
                    "sync_needed": (local_modified_time and remote_modified_time and
                                  local_modified_time != remote_modified_time)
                }
            except Exception as e:
                logger.error(f"Error getting remote file info: {e}") # Log dell'errore
                return {"exists": False, "error": str(e)}

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _get_remote_info)

    @staticmethod
    async def delete_remote_file_async() -> bool:
        """Elimina file remoto in modo async"""
        def _delete_remote():
            sync_manager = get_sync_manager()

            if not sync_manager.service or not sync_manager.remote_file_id:
                return True  # Nothing to delete

            try:
                sync_manager.service.files().delete(fileId=sync_manager.remote_file_id).execute()
                sync_manager.remote_file_id = None
                sync_manager._save_config() # Assumendo che _save_config esista
                return True
            except Exception as e:
                logger.error(f"Error deleting remote file: {e}")
                return False

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _delete_remote)

    @staticmethod
    async def test_connection_async() -> Dict[str, Any]:
        """Testa connessione Google Drive in modo async"""
        def _test_connection(): # Questa funzione era mancante nella parte duplicata
            sync_manager = get_sync_manager()

            if not sync_manager.service:
                return {
                    "success": False,
                    "message": "Google Drive service not available"
                }

            try:
                # Test basic API access
                about = sync_manager.service.about().get(fields='user,storageQuota').execute()

                # Test file listing permission
                files_list = sync_manager.service.files().list(
                    pageSize=1,
                    fields='files(id,name)'
                ).execute()

                user_info = about.get('user', {})
                storage_info = about.get('storageQuota', {})

                return {
                    "success": True,
                    "message": "Google Drive connection successful",
                    "user": {
                        "email": user_info.get('emailAddress'),
                        "display_name": user_info.get('displayName')
                    },
                    "storage": {
                        "limit": storage_info.get('limit'),
                        "usage": storage_info.get('usage'),
                        "usage_in_drive": storage_info.get('usageInDrive')
                    },
                    "permissions": {
                        "can_list_files": len(files_list.get('files', [])) >= 0,
                        "can_access_about": True
                    }
                }

            except Exception as e:
                logger.error(f"Google Drive API test failed: {e}") # Log dell'errore
                return {
                    "success": False,
                    "message": f"Google Drive API test failed: {str(e)}"
                }

        loop = asyncio.get_event_loop()
        # La prima definizione di get_sync_status_async chiamava _test_connection per errore,
        # ma la funzione _test_connection è effettivamente usata dal metodo test_connection_async.
        # Quindi qui passiamo la funzione corretta:
        return await loop.run_in_executor(_thread_pool, _test_connection)

# Istanza globale dell'adapter (corretta)
sync_adapter = CloudSyncAdapter()

# Per esportare l'istanza se il file è usato come modulo
__all__ = ["sync_adapter", "CloudSyncAdapter"]
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/adapters/cloud_sync_adapter.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/adapters/database_adapter.py ---
"""
Database Adapter per FastAPI
Fornisce interfaccia async per il core/database.py esistente senza modificarlo
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor
import pandas as pd

# Import del core esistente (INVARIATO)
from app.core.database import (
    get_connection, create_tables, get_anagraphics, get_invoices, 
    get_transactions, add_anagraphics_if_not_exists, add_transactions,
    check_entity_duplicate, get_item_details, update_invoice_reconciliation_state,
    update_transaction_reconciliation_state, add_or_update_reconciliation_link,
    remove_reconciliation_links
)

logger = logging.getLogger(__name__)

# Thread pool per operazioni sincrone del core
_thread_pool = ThreadPoolExecutor(max_workers=4)

class DatabaseAdapter:
    """
    Adapter che fornisce interfaccia async per il database core esistente
    """
    
    @staticmethod
    async def get_anagraphics_async(type_filter: Optional[str] = None) -> pd.DataFrame:
        """Versione async di get_anagraphics"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, get_anagraphics, type_filter)
    
    @staticmethod
    async def get_invoices_async(
        type_filter: Optional[str] = None,
        status_filter: Optional[str] = None,
        anagraphics_id_filter: Optional[int] = None,
        limit: Optional[int] = None
    ) -> pd.DataFrame:
        """Versione async di get_invoices"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool, 
            get_invoices, 
            type_filter, 
            status_filter, 
            anagraphics_id_filter, 
            limit
        )
    
    @staticmethod
    async def get_transactions_async(
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        status_filter: Optional[str] = None,
        limit: Optional[int] = None,
        anagraphics_id_heuristic_filter: Optional[int] = None,
        hide_pos: bool = False,
        hide_worldline: bool = False,
        hide_cash: bool = False,
        hide_commissions: bool = False
    ) -> pd.DataFrame:
        """Versione async di get_transactions"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            get_transactions,
            start_date,
            end_date,
            status_filter,
            limit,
            anagraphics_id_heuristic_filter,
            hide_pos,
            hide_worldline,
            hide_cash,
            hide_commissions
        )
    
    @staticmethod
    async def add_anagraphics_async(anag_data: Dict[str, Any], anag_type: str) -> Optional[int]:
        """Versione async di add_anagraphics_if_not_exists"""
        def _add_anag():
            conn = get_connection()
            try:
                cursor = conn.cursor()
                return add_anagraphics_if_not_exists(cursor, anag_data, anag_type)
            finally:
                conn.close()
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _add_anag)
    
    @staticmethod
    async def add_transactions_async(transactions_df: pd.DataFrame) -> tuple:
        """Versione async di add_transactions"""
        def _add_transactions():
            conn = get_connection()
            try:
                cursor = conn.cursor()
                result = add_transactions(cursor, transactions_df)
                conn.commit()
                return result
            except Exception as e:
                conn.rollback()
                raise
            finally:
                conn.close()
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _add_transactions)
    
    @staticmethod
    async def execute_query_async(query: str, params: tuple = None) -> List[Dict]:
        """Esegue una query personalizzata in modo async"""
        def _execute_query():
            conn = get_connection()
            try:
                cursor = conn.cursor()
                if params:
                    cursor.execute(query, params)
                else:
                    cursor.execute(query)
                
                # Converte sqlite3.Row in dict
                columns = [description[0] for description in cursor.description]
                rows = cursor.fetchall()
                return [dict(zip(columns, row)) for row in rows]
            finally:
                conn.close()
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _execute_query)
    
    @staticmethod
    async def execute_write_async(query: str, params: tuple = None) -> int:
        """Esegue una query di scrittura in modo async"""
        def _execute_write():
            conn = get_connection()
            try:
                cursor = conn.cursor()
                if params:
                    cursor.execute(query, params)
                else:
                    cursor.execute(query)
                
                conn.commit()
                return cursor.lastrowid if cursor.lastrowid else cursor.rowcount
            except Exception as e:
                conn.rollback()
                raise
            finally:
                conn.close()
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _execute_write)
    
    @staticmethod
    async def execute_many_async(query: str, params_list: List[tuple]) -> int:
        """Esegue query multiple in modo async"""
        def _execute_many():
            conn = get_connection()
            try:
                cursor = conn.cursor()
                cursor.executemany(query, params_list)
                conn.commit()
                return cursor.rowcount
            except Exception as e:
                conn.rollback()
                raise
            finally:
                conn.close()
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _execute_many)
    
    @staticmethod
    async def check_duplicate_async(table: str, column: str, value: Any) -> bool:
        """Versione async di check_entity_duplicate"""
        def _check_duplicate():
            conn = get_connection()
            try:
                cursor = conn.cursor()
                return check_entity_duplicate(cursor, table, column, value)
            finally:
                conn.close()
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _check_duplicate)
    
    @staticmethod
    async def get_item_details_async(item_type: str, item_id: int) -> Optional[Dict]:
        """Versione async di get_item_details"""
        def _get_item_details():
            conn = get_connection()
            try:
                result = get_item_details(conn, item_type, item_id)
                return dict(result) if result else None
            finally:
                conn.close()
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _get_item_details)
    
    @staticmethod
    async def update_invoice_state_async(invoice_id: int, payment_status: str, paid_amount: float) -> bool:
        """Versione async di update_invoice_reconciliation_state"""
        def _update_invoice():
            conn = get_connection()
            try:
                result = update_invoice_reconciliation_state(conn, invoice_id, payment_status, paid_amount)
                conn.commit()
                return result
            except Exception as e:
                conn.rollback()
                raise
            finally:
                conn.close()
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _update_invoice)
    
    @staticmethod
    async def update_transaction_state_async(transaction_id: int, reconciliation_status: str, reconciled_amount: float) -> bool:
        """Versione async di update_transaction_reconciliation_state"""
        def _update_transaction():
            conn = get_connection()
            try:
                result = update_transaction_reconciliation_state(conn, transaction_id, reconciliation_status, reconciled_amount)
                conn.commit()
                return result
            except Exception as e:
                conn.rollback()
                raise
            finally:
                conn.close()
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _update_transaction)
    
    @staticmethod
    async def add_reconciliation_link_async(invoice_id: int, transaction_id: int, amount: float) -> bool:
        """Versione async di add_or_update_reconciliation_link"""
        def _add_link():
            conn = get_connection()
            try:
                result = add_or_update_reconciliation_link(conn, invoice_id, transaction_id, amount)
                conn.commit()
                return result
            except Exception as e:
                conn.rollback()
                raise
            finally:
                conn.close()
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _add_link)
    
    @staticmethod
    async def remove_reconciliation_links_async(transaction_id: int = None, invoice_id: int = None) -> tuple:
        """Versione async di remove_reconciliation_links"""
        def _remove_links():
            conn = get_connection()
            try:
                result = remove_reconciliation_links(conn, transaction_id, invoice_id)
                conn.commit()
                return result
            except Exception as e:
                conn.rollback()
                raise
            finally:
                conn.close()
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _remove_links)
    
    @staticmethod
    async def create_tables_async():
        """Versione async di create_tables"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, create_tables)

# Istanza globale dell'adapter
db_adapter = DatabaseAdapter()
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/adapters/database_adapter.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/adapters/importer_adapter.py ---
"""
Importer Adapter per FastAPI
Fornisce interfaccia async per il core/importer.py esistente senza modificarlo
"""

import asyncio
import logging
from typing import Dict, Any, Callable, Optional
from concurrent.futures import ThreadPoolExecutor
import pandas as pd
from io import StringIO

# Import del core esistente (INVARIATO)
from app.core.importer import import_from_source
from app.core.parser_csv import parse_bank_csv
from app.core.parser_xml import parse_fattura_xml
from app.core.parser_p7m import extract_xml_from_p7m

logger = logging.getLogger(__name__)

# Thread pool per operazioni sincrone del core
_thread_pool = ThreadPoolExecutor(max_workers=2)  # Meno workers per I/O intensivo

class ImporterAdapter:
    """
    Adapter che fornisce interfaccia async per gli importer del core esistente
    """
    
    @staticmethod
    async def import_from_source_async(
        source_path: str,
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """Versione async di import_from_source"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            import_from_source,
            source_path,
            progress_callback
        )
    
    @staticmethod
    async def parse_bank_csv_async(csv_content: str) -> Optional[pd.DataFrame]:
        """Versione async di parse_bank_csv"""
        def _parse_csv():
            csv_file = StringIO(csv_content)
            return parse_bank_csv(csv_file)
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _parse_csv)
    
    @staticmethod
    async def parse_fattura_xml_async(
        xml_filepath: str,
        my_company_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Versione async di parse_fattura_xml"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            parse_fattura_xml,
            xml_filepath,
            my_company_data
        )
    
    @staticmethod
    async def extract_xml_from_p7m_async(p7m_filepath: str) -> Optional[str]:
        """Versione async di extract_xml_from_p7m"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            extract_xml_from_p7m,
            p7m_filepath
        )

# Istanza globale dell'adapter
importer_adapter = ImporterAdapter()
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/adapters/importer_adapter.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/adapters/reconciliation_adapter.py ---
"""
Reconciliation Adapter per FastAPI
Fornisce interfaccia async per il core/reconciliation.py esistente senza modificarlo
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor

# Import del core esistente (INVARIATO)
from app.core.reconciliation import (
    suggest_reconciliation_matches_enhanced,
    suggest_cumulative_matches,
    apply_manual_match_optimized,
    attempt_auto_reconciliation_optimized,
    find_automatic_matches_optimized,
    ignore_transaction,
    find_anagraphics_id_from_description,
    update_items_statuses_batch
)

logger = logging.getLogger(__name__)

# Thread pool per operazioni sincrone del core
_thread_pool = ThreadPoolExecutor(max_workers=4)

class ReconciliationAdapter:
    """
    Adapter che fornisce interfaccia async per il reconciliation core esistente
    """
    
    @staticmethod
    async def suggest_1_to_1_matches_async(
        invoice_id: Optional[int] = None,
        transaction_id: Optional[int] = None,
        anagraphics_id_filter: Optional[int] = None
    ) -> List[Dict]:
        """Versione async di suggest_reconciliation_matches_enhanced"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            suggest_reconciliation_matches_enhanced,
            invoice_id,
            transaction_id,
            anagraphics_id_filter
        )
    
    @staticmethod
    async def suggest_n_to_m_matches_async(
        transaction_id: int,
        anagraphics_id_filter: Optional[int] = None,
        max_combination_size: int = 5,
        max_search_time_ms: int = 30000,
        exclude_invoice_ids: Optional[List[int]] = None,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None
    ) -> List[Dict]:
        """Versione async di suggest_cumulative_matches"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            suggest_cumulative_matches,
            transaction_id,
            anagraphics_id_filter,
            max_combination_size,
            max_search_time_ms,
            exclude_invoice_ids,
            start_date,
            end_date
        )
    
    @staticmethod
    async def apply_manual_match_async(
        invoice_id: int,
        transaction_id: int,
        amount_to_match: float
    ) -> tuple:
        """Versione async di apply_manual_match_optimized"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            apply_manual_match_optimized,
            invoice_id,
            transaction_id,
            amount_to_match
        )
    
    @staticmethod
    async def apply_auto_reconciliation_async(
        transaction_ids: List[int],
        invoice_ids: List[int]
    ) -> tuple:
        """Versione async di attempt_auto_reconciliation_optimized"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            attempt_auto_reconciliation_optimized,
            transaction_ids,
            invoice_ids
        )
    
    @staticmethod
    async def find_automatic_matches_async(confidence_level: str = 'Exact') -> List[Dict]:
        """Versione async di find_automatic_matches_optimized"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            find_automatic_matches_optimized,
            confidence_level
        )
    
    @staticmethod
    async def ignore_transaction_async(transaction_id: int) -> tuple:
        """Versione async di ignore_transaction"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            ignore_transaction,
            transaction_id
        )
    
    @staticmethod
    async def find_anagraphics_from_description_async(description: str) -> Optional[int]:
        """Versione async di find_anagraphics_id_from_description"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _thread_pool,
            find_anagraphics_id_from_description,
            description
        )
    
    @staticmethod
    async def update_items_statuses_async(
        invoice_ids: Optional[List[int]] = None,
        transaction_ids: Optional[List[int]] = None
    ) -> bool:
        """Versione async di update_items_statuses_batch"""
        def _update_statuses():
            from app.core.database import get_connection
            conn = get_connection()
            try:
                return update_items_statuses_batch(conn, invoice_ids, transaction_ids)
            except Exception as e:
                conn.rollback()
                raise
            finally:
                conn.close()
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(_thread_pool, _update_statuses)

# Istanza globale dell'adapter
reconciliation_adapter = ReconciliationAdapter()
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/adapters/reconciliation_adapter.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/api/__init__.py ---
# Path: backend/app/api/__init__.py

# Questo file rende la directory un package Python

--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/api/__init__.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/api/anagraphics.py ---
"""
Anagraphics API endpoints - Integrato con core PySide6 esistente
"""

import logging
from typing import List, Optional
from fastapi import APIRouter, HTTPException, Depends, Query, Path
from fastapi.responses import JSONResponse

# Usa adapter invece di accesso diretto al database
from app.adapters.database_adapter import db_adapter
from app.models import (
    Anagraphics, AnagraphicsCreate, AnagraphicsUpdate, AnagraphicsFilter,
    PaginationParams, AnagraphicsListResponse, APIResponse, ErrorResponse
)

logger = logging.getLogger(__name__)
router = APIRouter()


@router.get("/", response_model=AnagraphicsListResponse)
async def get_anagraphics_list(
    type_filter: Optional[str] = Query(None, description="Filter by type: Cliente or Fornitore"),
    search: Optional[str] = Query(None, description="Search in denomination, piva, cf"),
    city: Optional[str] = Query(None, description="Filter by city"),
    province: Optional[str] = Query(None, description="Filter by province"),
    page: int = Query(1, ge=1, description="Page number"),
    size: int = Query(50, ge=1, le=1000, description="Page size")
):
    """Get paginated list of anagraphics with optional filters"""
    try:
        # Usa il core esistente tramite adapter
        df_anagraphics = await db_adapter.get_anagraphics_async(type_filter=type_filter)
        
        if df_anagraphics.empty:
            return AnagraphicsListResponse(
                items=[],
                total=0,
                page=page,
                size=size,
                pages=0
            )
        
        # Applica filtri aggiuntivi
        if search:
            search_mask = (
                df_anagraphics['denomination'].str.contains(search, case=False, na=False) |
                df_anagraphics['piva'].str.contains(search, case=False, na=False) |
                df_anagraphics['cf'].str.contains(search, case=False, na=False)
            )
            df_anagraphics = df_anagraphics[search_mask]
        
        if city:
            df_anagraphics = df_anagraphics[
                df_anagraphics['city'].str.contains(city, case=False, na=False)
            ]
        
        if province:
            df_anagraphics = df_anagraphics[df_anagraphics['province'] == province]
        
        total = len(df_anagraphics)
        
        # Paginazione
        start_idx = (page - 1) * size
        end_idx = start_idx + size
        df_paginated = df_anagraphics.iloc[start_idx:end_idx]
        
        # Converti in formato API
        items = df_paginated.to_dict('records')
        pages = (total + size - 1) // size
        
        return AnagraphicsListResponse(
            items=items,
            total=total,
            page=page,
            size=size,
            pages=pages
        )
        
    except Exception as e:
        logger.error(f"Error getting anagraphics list: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error retrieving anagraphics: {str(e)}")


@router.get("/{anagraphics_id}", response_model=Anagraphics)
async def get_anagraphics_by_id(
    anagraphics_id: int = Path(..., description="Anagraphics ID")
):
    """Get anagraphics by ID"""
    try:
        # Usa query customizzata tramite adapter
        result = await db_adapter.execute_query_async(
            """
            SELECT id, type, denomination, piva, cf, address, cap, city, province, country,
                   iban, email, phone, pec, codice_destinatario, score, created_at, updated_at
            FROM Anagraphics WHERE id = ?
            """,
            (anagraphics_id,)
        )
        
        if not result:
            raise HTTPException(status_code=404, detail="Anagraphics not found")
        
        return result[0]
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting anagraphics {anagraphics_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving anagraphics")


@router.post("/", response_model=Anagraphics)
async def create_anagraphics(anagraphics_data: AnagraphicsCreate):
    """Create new anagraphics"""
    try:
        # Validazione
        if not anagraphics_data.piva and not anagraphics_data.cf:
            raise HTTPException(
                status_code=400, 
                detail="Either VAT number (piva) or Tax Code (cf) must be provided"
            )
        
        # Check duplicati usando adapter
        if anagraphics_data.piva:
            is_duplicate = await db_adapter.check_duplicate_async(
                'Anagraphics', 'piva', anagraphics_data.piva
            )
            if is_duplicate:
                raise HTTPException(
                    status_code=409, 
                    detail=f"Anagraphics with VAT {anagraphics_data.piva} already exists"
                )
        
        # Usa il core per aggiungere anagrafica
        anag_dict = anagraphics_data.model_dump()
        new_id = await db_adapter.add_anagraphics_async(anag_dict, anagraphics_data.type)
        
        if not new_id:
            raise HTTPException(status_code=500, detail="Failed to create anagraphics")
        
        # Restituisci l'anagrafica creata
        return await get_anagraphics_by_id(new_id)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating anagraphics: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error creating anagraphics")


@router.put("/{anagraphics_id}", response_model=Anagraphics)
async def update_anagraphics(
    anagraphics_id: int = Path(..., description="Anagraphics ID"),
    anagraphics_data: AnagraphicsUpdate = ...
):
    """Update anagraphics"""
    try:
        # Check se esiste
        existing = await db_adapter.execute_query_async(
            "SELECT id FROM Anagraphics WHERE id = ?", (anagraphics_id,)
        )
        if not existing:
            raise HTTPException(status_code=404, detail="Anagraphics not found")
        
        # Build update query dinamica
        update_fields = []
        params = []
        
        for field, value in anagraphics_data.model_dump(exclude_unset=True).items():
            if value is not None:
                update_fields.append(f"{field} = ?")
                params.append(value)
        
        if not update_fields:
            raise HTTPException(status_code=400, detail="No fields to update")
        
        # Aggiungi updated_at
        update_fields.append("updated_at = datetime('now')")
        params.append(anagraphics_id)
        
        update_query = f"UPDATE Anagraphics SET {', '.join(update_fields)} WHERE id = ?"
        
        rows_affected = await db_adapter.execute_write_async(update_query, tuple(params))
        
        if rows_affected == 0:
            raise HTTPException(status_code=404, detail="Anagraphics not found")
        
        # Restituisci l'anagrafica aggiornata
        return await get_anagraphics_by_id(anagraphics_id)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating anagraphics {anagraphics_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error updating anagraphics")


@router.delete("/{anagraphics_id}", response_model=APIResponse)
async def delete_anagraphics(
    anagraphics_id: int = Path(..., description="Anagraphics ID")
):
    """Delete anagraphics"""
    try:
        # Check se ha fatture associate
        invoices = await db_adapter.execute_query_async(
            "SELECT COUNT(*) as count FROM Invoices WHERE anagraphics_id = ?", 
            (anagraphics_id,)
        )
        
        if invoices and invoices[0]['count'] > 0:
            raise HTTPException(
                status_code=409, 
                detail="Cannot delete anagraphics with associated invoices"
            )
        
        # Elimina
        rows_affected = await db_adapter.execute_write_async(
            "DELETE FROM Anagraphics WHERE id = ?", (anagraphics_id,)
        )
        
        if rows_affected == 0:
            raise HTTPException(status_code=404, detail="Anagraphics not found")
        
        return APIResponse(
            success=True,
            message="Anagraphics deleted successfully"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting anagraphics {anagraphics_id}: {e}", exc_info=True)
        raise
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/api/anagraphics.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/api/analytics.py ---
"""
Analytics API endpoints
"""

import logging
from typing import List, Optional
from datetime import date, datetime, timedelta
from fastapi import APIRouter, HTTPException, Query, Path
from fastapi.responses import JSONResponse

from app.core.database import execute_query_async
from app.models import (
    KPIData, CashFlowData, MonthlyRevenueData, TopClientData,
    ProductAnalysisData, AgingSummary, DashboardData, APIResponse
)

logger = logging.getLogger(__name__)
router = APIRouter()


@router.get("/dashboard", response_model=DashboardData)
async def get_dashboard_data():
    """Get comprehensive dashboard data with KPIs and summaries"""
    try:
        from app.core.analysis import (
            calculate_main_kpis,
            get_monthly_cash_flow_analysis,
            get_top_clients_by_revenue,
            get_top_overdue_invoices
        )
        
        # Calculate main KPIs
        kpis = calculate_main_kpis()
        
        # Get recent invoices
        recent_invoices_query = """
            SELECT i.id, i.type, i.doc_number, i.doc_date, i.total_amount,
                   i.payment_status, i.due_date, a.denomination as counterparty_name,
                   (i.total_amount - i.paid_amount) as open_amount
            FROM Invoices i
            JOIN Anagraphics a ON i.anagraphics_id = a.id
            ORDER BY i.created_at DESC
            LIMIT 10
        """
        
        recent_invoices = await execute_query_async(recent_invoices_query)
        
        # Get recent transactions
        recent_transactions_query = """
            SELECT id, transaction_date, amount, description, reconciliation_status,
                   (amount - reconciled_amount) as remaining_amount
            FROM BankTransactions
            WHERE reconciliation_status != 'Ignorato'
            ORDER BY transaction_date DESC, id DESC
            LIMIT 10
        """
        
        recent_transactions = await execute_query_async(recent_transactions_query)
        
        # Get cash flow data (last 6 months)
        cash_flow_df = get_monthly_cash_flow_analysis(6)
        cash_flow_summary = cash_flow_df.to_dict('records') if not cash_flow_df.empty else []
        
        # Get top clients
        top_clients_df = get_top_clients_by_revenue(10)
        top_clients = top_clients_df.to_dict('records') if not top_clients_df.empty else []
        
        # Get overdue invoices
        overdue_invoices_df = get_top_overdue_invoices(5)
        overdue_invoices = overdue_invoices_df.to_dict('records') if not overdue_invoices_df.empty else []
        
        return DashboardData(
            kpis=kpis,
            recent_invoices=recent_invoices,
            recent_transactions=recent_transactions,
            cash_flow_summary=cash_flow_summary,
            top_clients=top_clients,
            overdue_invoices=overdue_invoices
        )
        
    except Exception as e:
        logger.error(f"Error getting dashboard data: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving dashboard data")


@router.get("/kpis", response_model=KPIData)
async def get_kpis():
    """Get main KPIs"""
    try:
        from app.core.analysis import calculate_main_kpis
        
        kpis = calculate_main_kpis()
        return kpis
        
    except Exception as e:
        logger.error(f"Error getting KPIs: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error calculating KPIs")


@router.get("/cash-flow/monthly")
async def get_monthly_cash_flow(
    months: int = Query(12, ge=1, le=60, description="Number of months to analyze")
):
    """Get monthly cash flow analysis"""
    try:
        from app.core.analysis import get_monthly_cash_flow_analysis
        
        cash_flow_df = get_monthly_cash_flow_analysis(months)
        
        if cash_flow_df.empty:
            return APIResponse(
                success=True,
                message="No cash flow data available",
                data=[]
            )
        
        return APIResponse(
            success=True,
            message=f"Cash flow data for {months} months",
            data=cash_flow_df.to_dict('records')
        )
        
    except Exception as e:
        logger.error(f"Error getting monthly cash flow: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving cash flow data")


@router.get("/revenue/monthly")
async def get_monthly_revenue(
    months: int = Query(12, ge=1, le=60, description="Number of months to analyze"),
    invoice_type: Optional[str] = Query(None, description="Filter by invoice type: Attiva or Passiva")
):
    """Get monthly revenue analysis"""
    try:
        from app.core.analysis import get_monthly_revenue_analysis
        
        revenue_df = get_monthly_revenue_analysis(months, invoice_type)
        
        if revenue_df.empty:
            return APIResponse(
                success=True,
                message="No revenue data available",
                data=[]
            )
        
        return APIResponse(
            success=True,
            message=f"Revenue data for {months} months",
            data=revenue_df.to_dict('records')
        )
        
    except Exception as e:
        logger.error(f"Error getting monthly revenue: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving revenue data")


@router.get("/clients/top")
async def get_top_clients(
    limit: int = Query(20, ge=1, le=100, description="Number of top clients"),
    period_months: int = Query(12, ge=1, le=60, description="Analysis period in months"),
    min_revenue: Optional[float] = Query(None, description="Minimum revenue filter")
):
    """Get top clients by revenue"""
    try:
        from app.core.analysis import get_top_clients_by_revenue
        
        clients_df = get_top_clients_by_revenue(limit, period_months, min_revenue)
        
        if clients_df.empty:
            return APIResponse(
                success=True,
                message="No client data available",
                data=[]
            )
        
        return APIResponse(
            success=True,
            message=f"Top {len(clients_df)} clients by revenue",
            data=clients_df.to_dict('records')
        )
        
    except Exception as e:
        logger.error(f"Error getting top clients: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving top clients")


@router.get("/products/analysis")
async def get_product_analysis(
    limit: int = Query(50, ge=1, le=200, description="Number of top products"),
    period_months: int = Query(12, ge=1, le=60, description="Analysis period in months"),
    min_quantity: Optional[float] = Query(None, description="Minimum quantity filter"),
    invoice_type: Optional[str] = Query(None, description="Filter by invoice type")
):
    """Get product analysis by quantity and value"""
    try:
        from app.core.analysis import get_product_analysis
        
        products_df = get_product_analysis(limit, period_months, min_quantity, invoice_type)
        
        if products_df.empty:
            return APIResponse(
                success=True,
                message="No product data available",
                data=[]
            )
        
        return APIResponse(
            success=True,
            message=f"Product analysis for {len(products_df)} items",
            data=products_df.to_dict('records')
        )
        
    except Exception as e:
        logger.error(f"Error getting product analysis: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving product analysis")


@router.get("/aging/invoices")
async def get_invoices_aging(
    invoice_type: str = Query("Attiva", description="Invoice type: Attiva or Passiva")
):
    """Get aging analysis for invoices"""
    try:
        from app.core.analysis import get_aging_summary
        
        aging_data = get_aging_summary(invoice_type)
        
        # Convert to API response format
        buckets = []
        total_amount = 0.0
        total_count = 0
        
        for label, data in aging_data.items():
            amount = float(data['amount'])
            count = data['count']
            buckets.append({
                "label": label,
                "amount": amount,
                "count": count
            })
            total_amount += amount
            total_count += count
        
        return APIResponse(
            success=True,
            message=f"Aging analysis for {invoice_type} invoices",
            data={
                "buckets": buckets,
                "total_amount": total_amount,
                "total_count": total_count,
                "invoice_type": invoice_type
            }
        )
        
    except Exception as e:
        logger.error(f"Error getting aging analysis: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving aging analysis")


@router.get("/overdue/invoices")
async def get_overdue_invoices(
    limit: int = Query(20, ge=1, le=100, description="Number of invoices to return"),
    priority_sort: bool = Query(True, description="Sort by priority (amount and days overdue)")
):
    """Get overdue invoices with priority scoring"""
    try:
        from app.core.analysis import get_top_overdue_invoices
        
        overdue_df = get_top_overdue_invoices(limit, priority_sort)
        
        if overdue_df.empty:
            return APIResponse(
                success=True,
                message="No overdue invoices found",
                data=[]
            )
        
        return APIResponse(
            success=True,
            message=f"Found {len(overdue_df)} overdue invoices",
            data=overdue_df.to_dict('records')
        )
        
    except Exception as e:
        logger.error(f"Error getting overdue invoices: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving overdue invoices")


@router.get("/trends/revenue")
async def get_revenue_trends(
    period: str = Query("monthly", description="Period: daily, weekly, monthly, quarterly"),
    months_back: int = Query(12, ge=1, le=60, description="Months to look back"),
    compare_previous: bool = Query(True, description="Include previous period comparison")
):
    """Get revenue trends analysis"""
    try:
        # Base query for revenue trends
        if period == "daily":
            date_format = "%Y-%m-%d"
            date_trunc = "date(doc_date)"
        elif period == "weekly":
            date_format = "%Y-W%W"
            date_trunc = "strftime('%Y-W%W', doc_date)"
        elif period == "quarterly":
            date_format = "%Y-Q%q"
            date_trunc = "strftime('%Y', doc_date) || '-Q' || ((CAST(strftime('%m', doc_date) AS INTEGER) - 1) / 3 + 1)"
        else:  # monthly
            date_format = "%Y-%m"
            date_trunc = "strftime('%Y-%m', doc_date)"
        
        trends_query = f"""
            SELECT 
                {date_trunc} as period,
                type,
                COUNT(*) as invoice_count,
                SUM(total_amount) as total_revenue,
                AVG(total_amount) as avg_invoice_amount,
                SUM(paid_amount) as total_paid,
                SUM(total_amount - paid_amount) as total_outstanding
            FROM Invoices
            WHERE doc_date >= date('now', '-{months_back} months')
            GROUP BY {date_trunc}, type
            ORDER BY period, type
        """
        
        trends = await execute_query_async(trends_query)
        
        # Calculate period-over-period changes if requested
        if compare_previous and trends:
            # This would require more complex logic to compare with previous periods
            # For now, return the basic trends
            pass
        
        return APIResponse(
            success=True,
            message=f"Revenue trends for {period} periods",
            data={
                "trends": trends,
                "period": period,
                "months_analyzed": months_back
            }
        )
        
    except Exception as e:
        logger.error(f"Error getting revenue trends: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving revenue trends")


@router.get("/performance/payment")
async def get_payment_performance():
    """Get payment performance metrics"""
    try:
        # Average days to payment
        payment_performance_query = """
            WITH payment_times AS (
                SELECT 
                    i.id,
                    i.doc_date,
                    i.due_date,
                    i.total_amount,
                    i.payment_status,
                    CASE 
                        WHEN i.payment_status = 'Pagata Tot.' THEN
                            -- Use the latest reconciliation date as payment date approximation
                            (SELECT MAX(reconciliation_date) FROM ReconciliationLinks rl WHERE rl.invoice_id = i.id)
                        ELSE NULL
                    END as estimated_payment_date
                FROM Invoices i
                WHERE i.type = 'Attiva'
                  AND i.payment_status IN ('Pagata Tot.', 'Aperta', 'Scaduta', 'Pagata Parz.')
            )
            SELECT 
                payment_status,
                COUNT(*) as count,
                AVG(
                    CASE 
                        WHEN estimated_payment_date IS NOT NULL THEN
                            julianday(estimated_payment_date) - julianday(doc_date)
                        WHEN due_date IS NOT NULL THEN
                            julianday('now') - julianday(due_date)
                        ELSE NULL
                    END
                ) as avg_days,
                SUM(total_amount) as total_amount
            FROM payment_times
            GROUP BY payment_status
        """
        
        performance = await execute_query_async(payment_performance_query)
        
        # Collection efficiency
        collection_query = """
            SELECT 
                strftime('%Y-%m', doc_date) as month,
                COUNT(*) as invoices_issued,
                COUNT(CASE WHEN payment_status = 'Pagata Tot.' THEN 1 END) as invoices_paid,
                SUM(total_amount) as total_issued,
                SUM(paid_amount) as total_collected,
                (CAST(COUNT(CASE WHEN payment_status = 'Pagata Tot.' THEN 1 END) AS REAL) / COUNT(*)) * 100 as collection_rate_pct
            FROM Invoices
            WHERE type = 'Attiva'
              AND doc_date >= date('now', '-12 months')
            GROUP BY strftime('%Y-%m', doc_date)
            ORDER BY month
        """
        
        collection = await execute_query_async(collection_query)
        
        return APIResponse(
            success=True,
            message="Payment performance metrics retrieved",
            data={
                "payment_performance": performance,
                "collection_efficiency": collection
            }
        )
        
    except Exception as e:
        logger.error(f"Error getting payment performance: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving payment performance")


@router.get("/forecasting/cash-flow")
async def get_cash_flow_forecast(
    months_ahead: int = Query(6, ge=1, le=24, description="Months to forecast ahead"),
    include_scheduled: bool = Query(True, description="Include scheduled payments from due dates")
):
    """Get cash flow forecasting based on historical data and scheduled payments"""
    try:
        # Get historical monthly patterns
        historical_query = """
            SELECT 
                strftime('%m', transaction_date) as month_num,
                AVG(monthly_inflow) as avg_inflow,
                AVG(monthly_outflow) as avg_outflow,
                AVG(monthly_net) as avg_net
            FROM (
                SELECT 
                    strftime('%Y-%m', transaction_date) as year_month,
                    transaction_date,
                    SUM(CASE WHEN amount > 0 THEN amount ELSE 0 END) as monthly_inflow,
                    SUM(CASE WHEN amount < 0 THEN ABS(amount) ELSE 0 END) as monthly_outflow,
                    SUM(amount) as monthly_net
                FROM BankTransactions
                WHERE transaction_date >= date('now', '-24 months')
                GROUP BY strftime('%Y-%m', transaction_date)
            ) monthly_data
            GROUP BY strftime('%m', transaction_date)
            ORDER BY month_num
        """
        
        historical = await execute_query_async(historical_query)
        
        # Get scheduled receivables (open invoices with due dates)
        if include_scheduled:
            scheduled_receivables_query = """
                SELECT 
                    strftime('%Y-%m', due_date) as due_month,
                    SUM(total_amount - paid_amount) as expected_inflow,
                    COUNT(*) as invoice_count
                FROM Invoices
                WHERE type = 'Attiva'
                  AND payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.')
                  AND due_date IS NOT NULL
                  AND due_date <= date('now', '+{months_ahead} months')
                  AND (total_amount - paid_amount) > 0
                GROUP BY strftime('%Y-%m', due_date)
                ORDER BY due_month
            """.format(months_ahead=months_ahead)
            
            scheduled_payables_query = """
                SELECT 
                    strftime('%Y-%m', due_date) as due_month,
                    SUM(total_amount - paid_amount) as expected_outflow,
                    COUNT(*) as invoice_count
                FROM Invoices
                WHERE type = 'Passiva'
                  AND payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.')
                  AND due_date IS NOT NULL
                  AND due_date <= date('now', '+{months_ahead} months')
                  AND (total_amount - paid_amount) > 0
                GROUP BY strftime('%Y-%m', due_date)
                ORDER BY due_month
            """.format(months_ahead=months_ahead)
            
            scheduled_receivables = await execute_query_async(scheduled_receivables_query)
            scheduled_payables = await execute_query_async(scheduled_payables_query)
        else:
            scheduled_receivables = []
            scheduled_payables = []
        
        # Generate forecast
        forecast = []
        current_date = datetime.now()
        
        for i in range(months_ahead):
            forecast_date = current_date + timedelta(days=30 * i)
            month_num = forecast_date.strftime('%m')
            year_month = forecast_date.strftime('%Y-%m')
            
            # Find historical pattern for this month
            historical_pattern = next(
                (h for h in historical if h['month_num'] == month_num), 
                {'avg_inflow': 0, 'avg_outflow': 0, 'avg_net': 0}
            )
            
            # Find scheduled amounts for this month
            scheduled_in = next(
                (s['expected_inflow'] for s in scheduled_receivables if s['due_month'] == year_month),
                0
            )
            scheduled_out = next(
                (s['expected_outflow'] for s in scheduled_payables if s['due_month'] == year_month),
                0
            )
            
            # Combine historical and scheduled
            forecast_inflow = (historical_pattern['avg_inflow'] or 0) + scheduled_in
            forecast_outflow = (historical_pattern['avg_outflow'] or 0) + scheduled_out
            forecast_net = forecast_inflow - forecast_outflow
            
            forecast.append({
                'month': year_month,
                'forecasted_inflow': forecast_inflow,
                'forecasted_outflow': forecast_outflow,
                'forecasted_net': forecast_net,
                'scheduled_receivables': scheduled_in,
                'scheduled_payables': scheduled_out,
                'historical_avg_inflow': historical_pattern['avg_inflow'] or 0,
                'historical_avg_outflow': historical_pattern['avg_outflow'] or 0
            })
        
        return APIResponse(
            success=True,
            message=f"Cash flow forecast for {months_ahead} months",
            data={
                'forecast': forecast,
                'historical_patterns': historical,
                'methodology': 'Historical averages + scheduled payments'
            }
        )
        
    except Exception as e:
        logger.error(f"Error getting cash flow forecast: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error generating cash flow forecast")


@router.get("/comparison/year-over-year")
async def get_year_over_year_comparison(
    metric: str = Query("revenue", description="Metric to compare: revenue, profit, volume"),
    current_year: Optional[int] = Query(None, description="Current year (default: current year)")
):
    """Get year-over-year comparison analysis"""
    try:
        if current_year is None:
            current_year = datetime.now().year
        
        previous_year = current_year - 1
        
        if metric == "revenue":
            comparison_query = """
                SELECT 
                    strftime('%m', doc_date) as month,
                    CAST(strftime('%Y', doc_date) AS INTEGER) as year,
                    type,
                    SUM(total_amount) as total_amount,
                    COUNT(*) as invoice_count,
                    AVG(total_amount) as avg_invoice_amount
                FROM Invoices
                WHERE CAST(strftime('%Y', doc_date) AS INTEGER) IN (?, ?)
                GROUP BY strftime('%m', doc_date), CAST(strftime('%Y', doc_date) AS INTEGER), type
                ORDER BY month, year, type
            """
            
            comparison = await execute_query_async(comparison_query, (previous_year, current_year))
            
        elif metric == "profit":
            # Simplified profit calculation (revenue - costs)
            comparison_query = """
                SELECT 
                    strftime('%m', doc_date) as month,
                    CAST(strftime('%Y', doc_date) AS INTEGER) as year,
                    SUM(CASE WHEN type = 'Attiva' THEN total_amount ELSE 0 END) as revenue,
                    SUM(CASE WHEN type = 'Passiva' THEN total_amount ELSE 0 END) as costs,
                    (SUM(CASE WHEN type = 'Attiva' THEN total_amount ELSE 0 END) - 
                     SUM(CASE WHEN type = 'Passiva' THEN total_amount ELSE 0 END)) as profit
                FROM Invoices
                WHERE CAST(strftime('%Y', doc_date) AS INTEGER) IN (?, ?)
                GROUP BY strftime('%m', doc_date), CAST(strftime('%Y', doc_date) AS INTEGER)
                ORDER BY month, year
            """
            
            comparison = await execute_query_async(comparison_query, (previous_year, current_year))
            
        elif metric == "volume":
            comparison_query = """
                SELECT 
                    strftime('%m', doc_date) as month,
                    CAST(strftime('%Y', doc_date) AS INTEGER) as year,
                    type,
                    COUNT(*) as invoice_count,
                    COUNT(DISTINCT anagraphics_id) as unique_clients
                FROM Invoices
                WHERE CAST(strftime('%Y', doc_date) AS INTEGER) IN (?, ?)
                GROUP BY strftime('%m', doc_date), CAST(strftime('%Y', doc_date) AS INTEGER), type
                ORDER BY month, year, type
            """
            
            comparison = await execute_query_async(comparison_query, (previous_year, current_year))
        
        # Calculate percentage changes
        comparison_with_changes = []
        current_year_data = [c for c in comparison if c['year'] == current_year]
        previous_year_data = [c for c in comparison if c['year'] == previous_year]
        
        for current in current_year_data:
            # Find corresponding previous year data
            previous = next(
                (p for p in previous_year_data 
                 if p['month'] == current['month'] and 
                    (p.get('type') == current.get('type') if 'type' in current else True)),
                None
            )
            
            if previous:
                # Calculate changes for numeric fields
                changes = {}
                for key in current:
                    if key not in ['month', 'year', 'type'] and isinstance(current[key], (int, float)):
                        prev_val = previous.get(key, 0) or 0
                        curr_val = current[key] or 0
                        if prev_val != 0:
                            changes[f'{key}_change_pct'] = ((curr_val - prev_val) / prev_val) * 100
                        else:
                            changes[f'{key}_change_pct'] = 0 if curr_val == 0 else 100
                        changes[f'{key}_change_abs'] = curr_val - prev_val
                
                comparison_with_changes.append({
                    **current,
                    'previous_year_data': previous,
                    'changes': changes
                })
            else:
                comparison_with_changes.append({
                    **current,
                    'previous_year_data': None,
                    'changes': {'note': 'No previous year data available'}
                })
        
        return APIResponse(
            success=True,
            message=f"Year-over-year {metric} comparison: {previous_year} vs {current_year}",
            data={
                'metric': metric,
                'current_year': current_year,
                'previous_year': previous_year,
                'comparison': comparison_with_changes,
                'raw_data': comparison
            }
        )
        
    except Exception as e:
        logger.error(f"Error getting year-over-year comparison: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error generating year-over-year comparison")


@router.get("/segmentation/clients")
async def get_client_segmentation(
    segmentation_type: str = Query("revenue", description="Segmentation type: revenue, frequency, recency"),
    period_months: int = Query(12, ge=1, le=60, description="Analysis period in months")
):
    """Get client segmentation analysis"""
    try:
        if segmentation_type == "revenue":
            # RFM-style revenue segmentation
            segmentation_query = """
                WITH client_metrics AS (
                    SELECT 
                        a.id,
                        a.denomination,
                        COUNT(i.id) as invoice_count,
                        SUM(i.total_amount) as total_revenue,
                        AVG(i.total_amount) as avg_order_value,
                        MAX(i.doc_date) as last_order_date,
                        MIN(i.doc_date) as first_order_date
                    FROM Anagraphics a
                    JOIN Invoices i ON a.id = i.anagraphics_id
                    WHERE a.type = 'Cliente'
                      AND i.type = 'Attiva'
                      AND i.doc_date >= date('now', '-{period_months} months')
                    GROUP BY a.id, a.denomination
                ),
                revenue_quartiles AS (
                    SELECT 
                        *,
                        NTILE(4) OVER (ORDER BY total_revenue) as revenue_quartile
                    FROM client_metrics
                )
                SELECT 
                    revenue_quartile,
                    CASE 
                        WHEN revenue_quartile = 4 THEN 'High Value'
                        WHEN revenue_quartile = 3 THEN 'Medium-High Value'
                        WHEN revenue_quartile = 2 THEN 'Medium Value'
                        ELSE 'Low Value'
                    END as segment_name,
                    COUNT(*) as client_count,
                    SUM(total_revenue) as segment_revenue,
                    AVG(total_revenue) as avg_revenue_per_client,
                    AVG(invoice_count) as avg_orders_per_client,
                    AVG(avg_order_value) as avg_order_value
                FROM revenue_quartiles
                GROUP BY revenue_quartile
                ORDER BY revenue_quartile DESC
            """.format(period_months=period_months)
            
        elif segmentation_type == "frequency":
            segmentation_query = """
                WITH client_frequency AS (
                    SELECT 
                        a.id,
                        a.denomination,
                        COUNT(i.id) as order_frequency,
                        SUM(i.total_amount) as total_revenue
                    FROM Anagraphics a
                    JOIN Invoices i ON a.id = i.anagraphics_id
                    WHERE a.type = 'Cliente'
                      AND i.type = 'Attiva'
                      AND i.doc_date >= date('now', '-{period_months} months')
                    GROUP BY a.id, a.denomination
                )
                SELECT 
                    CASE 
                        WHEN order_frequency >= 10 THEN 'Very Frequent (10+)'
                        WHEN order_frequency >= 5 THEN 'Frequent (5-9)'
                        WHEN order_frequency >= 2 THEN 'Regular (2-4)'
                        ELSE 'Occasional (1)'
                    END as segment_name,
                    COUNT(*) as client_count,
                    SUM(total_revenue) as segment_revenue,
                    AVG(total_revenue) as avg_revenue_per_client,
                    AVG(order_frequency) as avg_order_frequency
                FROM client_frequency
                GROUP BY 
                    CASE 
                        WHEN order_frequency >= 10 THEN 'Very Frequent (10+)'
                        WHEN order_frequency >= 5 THEN 'Frequent (5-9)'
                        WHEN order_frequency >= 2 THEN 'Regular (2-4)'
                        ELSE 'Occasional (1)'
                    END
                ORDER BY avg_order_frequency DESC
            """.format(period_months=period_months)
            
        elif segmentation_type == "recency":
            segmentation_query = """
                WITH client_recency AS (
                    SELECT 
                        a.id,
                        a.denomination,
                        MAX(i.doc_date) as last_order_date,
                        julianday('now') - julianday(MAX(i.doc_date)) as days_since_last_order,
                        COUNT(i.id) as total_orders,
                        SUM(i.total_amount) as total_revenue
                    FROM Anagraphics a
                    JOIN Invoices i ON a.id = i.anagraphics_id
                    WHERE a.type = 'Cliente'
                      AND i.type = 'Attiva'
                    GROUP BY a.id, a.denomination
                )
                SELECT 
                    CASE 
                        WHEN days_since_last_order <= 30 THEN 'Active (≤30 days)'
                        WHEN days_since_last_order <= 90 THEN 'Recent (31-90 days)'
                        WHEN days_since_last_order <= 180 THEN 'Lapsed (91-180 days)'
                        WHEN days_since_last_order <= 365 THEN 'At Risk (181-365 days)'
                        ELSE 'Lost (>365 days)'
                    END as segment_name,
                    COUNT(*) as client_count,
                    SUM(total_revenue) as segment_revenue,
                    AVG(total_revenue) as avg_revenue_per_client,
                    AVG(days_since_last_order) as avg_days_since_last_order
                FROM client_recency
                GROUP BY 
                    CASE 
                        WHEN days_since_last_order <= 30 THEN 'Active (≤30 days)'
                        WHEN days_since_last_order <= 90 THEN 'Recent (31-90 days)'
                        WHEN days_since_last_order <= 180 THEN 'Lapsed (91-180 days)'
                        WHEN days_since_last_order <= 365 THEN 'At Risk (181-365 days)'
                        ELSE 'Lost (>365 days)'
                    END
                ORDER BY avg_days_since_last_order
            """
        
        segmentation = await execute_query_async(segmentation_query)
        
        return APIResponse(
            success=True,
            message=f"Client segmentation by {segmentation_type}",
            data={
                'segmentation_type': segmentation_type,
                'period_months': period_months,
                'segments': segmentation
            }
        )
        
    except Exception as e:
        logger.error(f"Error getting client segmentation: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error generating client segmentation")


@router.get("/export/report")
async def export_analytics_report(
    report_type: str = Query("dashboard", description="Report type: dashboard, financial, operational"),
    format: str = Query("json", description="Export format: json, csv"),
    period_months: int = Query(12, ge=1, le=60, description="Analysis period")
):
    """Export comprehensive analytics report"""
    try:
        report_data = {}
        
        if report_type == "dashboard":
            # Get dashboard data
            from app.core.analysis import calculate_main_kpis
            
            report_data['kpis'] = calculate_main_kpis()
            report_data['generated_at'] = datetime.now().isoformat()
            report_data['period_months'] = period_months
            
        elif report_type == "financial":
            # Financial report with revenue, costs, profit
            financial_query = """
                SELECT 
                    strftime('%Y-%m', doc_date) as month,
                    type,
                    COUNT(*) as invoice_count,
                    SUM(total_amount) as total_amount,
                    SUM(paid_amount) as paid_amount,
                    SUM(total_amount - paid_amount) as outstanding_amount
                FROM Invoices
                WHERE doc_date >= date('now', '-{period_months} months')
                GROUP BY strftime('%Y-%m', doc_date), type
                ORDER BY month, type
            """.format(period_months=period_months)
            
            financial_data = await execute_query_async(financial_query)
            report_data['financial_summary'] = financial_data
            
        elif report_type == "operational":
            # Operational metrics
            operational_query = """
                SELECT 
                    'invoices' as metric_type,
                    payment_status as status,
                    COUNT(*) as count,
                    SUM(total_amount) as total_amount
                FROM Invoices
                WHERE doc_date >= date('now', '-{period_months} months')
                GROUP BY payment_status
                
                UNION ALL
                
                SELECT 
                    'transactions' as metric_type,
                    reconciliation_status as status,
                    COUNT(*) as count,
                    SUM(ABS(amount)) as total_amount
                FROM BankTransactions
                WHERE transaction_date >= date('now', '-{period_months} months')
                GROUP BY reconciliation_status
            """.format(period_months=period_months)
            
            operational_data = await execute_query_async(operational_query)
            report_data['operational_metrics'] = operational_data
        
        # Add metadata
        report_data['metadata'] = {
            'report_type': report_type,
            'generated_at': datetime.now().isoformat(),
            'period_months': period_months,
            'format': format
        }
        
        if format == "csv":
            # Convert to CSV format (simplified)
            import io
            import csv
            
            output = io.StringIO()
            if report_data:
                # Flatten the data for CSV
                flattened_data = []
                for key, value in report_data.items():
                    if isinstance(value, list):
                        for item in value:
                            flattened_item = {'section': key}
                            flattened_item.update(item)
                            flattened_data.append(flattened_item)
                
                if flattened_data:
                    writer = csv.DictWriter(output, fieldnames=flattened_data[0].keys())
                    writer.writeheader()
                    writer.writerows(flattened_data)
            
            csv_content = output.getvalue()
            output.close()
            
            return APIResponse(
                success=True,
                message=f"{report_type.title()} report exported as CSV",
                data={
                    'format': 'csv',
                    'content': csv_content,
                    'filename': f"analytics_report_{report_type}_{datetime.now().strftime('%Y%m%d')}.csv"
                }
            )
        else:
            return APIResponse(
                success=True,
                message=f"{report_type.title()} report exported as JSON",
                data=report_data
            )
        
    except Exception as e:
        logger.error(f"Error exporting analytics report: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error exporting analytics report")
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/api/analytics.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/api/import_export.py ---
"""
Import/Export API endpoints - Aggiornato per usare adapter pattern
"""

import logging
import os
import tempfile
from typing import List, Optional
from fastapi import APIRouter, HTTPException, UploadFile, File, Query, BackgroundTasks
from fastapi.responses import FileResponse, StreamingResponse
from io import BytesIO
import pandas as pd

# Usa adapters invece di accesso diretto al core
from app.adapters.database_adapter import db_adapter
from app.adapters.importer_adapter import importer_adapter
from app.models import ImportResult, FileUploadResponse, APIResponse
from app.config import settings

logger = logging.getLogger(__name__)
router = APIRouter()


@router.post("/invoices/xml", response_model=ImportResult)
async def import_invoices_xml(
    files: List[UploadFile] = File(..., description="XML or P7M invoice files"),
    background_tasks: BackgroundTasks = None
):
    """Import invoices from XML or P7M files using core adapter"""
    try:
        if len(files) > 50:
            raise HTTPException(status_code=400, detail="Maximum 50 files allowed per upload")
        
        # Validazione tipi file
        allowed_extensions = ['.xml', '.p7m']
        for file in files:
            if not any(file.filename.lower().endswith(ext) for ext in allowed_extensions):
                raise HTTPException(
                    status_code=400, 
                    detail=f"File {file.filename} has unsupported format. Only XML and P7M files are allowed."
                )
        
        # Crea directory temporanea
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_files = []
            
            # Salva file caricati
            for file in files:
                temp_path = os.path.join(temp_dir, file.filename)
                with open(temp_path, "wb") as temp_file:
                    content = await file.read()
                    temp_file.write(content)
                temp_files.append(temp_path)
            
            # Valida file prima dell'importazione
            validation_results = []
            for temp_path in temp_files:
                validation = await importer_adapter.validate_file_async(temp_path)
                validation_results.append({
                    'path': temp_path,
                    'filename': os.path.basename(temp_path),
                    'validation': validation
                })
            
            # Filtra solo file validi
            valid_files = [
                result['path'] for result in validation_results 
                if result['validation']['valid']
            ]
            
            invalid_files = [
                result for result in validation_results 
                if not result['validation']['valid']
            ]
            
            if not valid_files:
                error_details = []
                for invalid in invalid_files:
                    error_details.append(f"{invalid['filename']}: {', '.join(invalid['validation']['errors'])}")
                
                raise HTTPException(
                    status_code=400, 
                    detail=f"No valid files to import. Errors: {'; '.join(error_details)}"
                )
            
            # Callback per progress tracking
            def progress_callback(current, total):
                logger.info(f"Processing file {current}/{total}")
            
            # Importa file usando adapter
            try:
                result = await importer_adapter.import_multiple_files_async(
                    valid_files, 
                    progress_callback
                )
                
                # Aggiungi informazioni sui file non validi
                for invalid in invalid_files:
                    result['processed'] += 1
                    result['errors'] += 1
                    result['files'].append({
                        'name': invalid['filename'],
                        'status': f"Validation failed: {', '.join(invalid['validation']['errors'])}"
                    })
                
                return ImportResult(**result)
                
            except Exception as import_error:
                logger.error(f"Import error: {import_error}")
                raise HTTPException(
                    status_code=500, 
                    detail=f"Import failed: {str(import_error)}"
                )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error importing XML/P7M files: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error importing files")


@router.post("/invoices/zip", response_model=ImportResult)
async def import_invoices_zip(
    file: UploadFile = File(..., description="ZIP file containing XML/P7M invoices")
):
    """Import invoices from ZIP archive using core adapter"""
    try:
        if not file.filename.lower().endswith('.zip'):
            raise HTTPException(status_code=400, detail="File must be a ZIP archive")
        
        # Crea file temporaneo per ZIP
        with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as temp_zip:
            content = await file.read()
            temp_zip.write(content)
            temp_zip_path = temp_zip.name
        
        try:
            # Valida ZIP
            validation = await importer_adapter.validate_file_async(temp_zip_path)
            if not validation['valid']:
                raise HTTPException(
                    status_code=400, 
                    detail=f"Invalid ZIP file: {', '.join(validation['errors'])}"
                )
            
            # Callback per progress
            def progress_callback(current, total):
                logger.info(f"Processing file {current}/{total} from ZIP")
            
            # Importa usando adapter
            result = await importer_adapter.extract_and_import_zip_async(
                temp_zip_path, 
                progress_callback
            )
            
            return ImportResult(**result)
            
        finally:
            # Pulizia file temporaneo
            try:
                os.unlink(temp_zip_path)
            except:
                pass
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error importing ZIP file: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error importing ZIP file")


@router.post("/transactions/csv", response_model=ImportResult)
async def import_transactions_csv(
    file: UploadFile = File(..., description="CSV file with bank transactions")
):
    """Import bank transactions from CSV file using core adapter"""
    try:
        if not file.filename.lower().endswith('.csv'):
            raise HTTPException(status_code=400, detail="File must be a CSV")
        
        # Leggi contenuto file
        content = await file.read()
        
        # Try decodifica con diversi encoding
        try:
            content_str = content.decode('utf-8')
        except UnicodeDecodeError:
            try:
                content_str = content.decode('latin-1')
            except UnicodeDecodeError:
                content_str = content.decode('cp1252')
        
        # Importa usando adapter
        result = await importer_adapter.import_csv_transactions_async(
            content_str, 
            file.filename
        )
        
        return ImportResult(**result)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error importing CSV transactions: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error importing CSV file")


@router.get("/templates/transactions-csv")
async def download_transactions_csv_template():
    """Download CSV template for bank transactions import"""
    try:
        # Crea template CSV
        template_data = {
            'DATA': ['2024-01-15', '2024-01-16', '2024-01-17'],
            'VALUTA': ['2024-01-15', '2024-01-16', '2024-01-17'],
            'DARE': ['', '150.00', ''],
            'AVERE': ['1000.00', '', '75.50'],
            'DESCRIZIONE OPERAZIONE': [
                'VERSAMENTO DA CLIENTE XYZ SRL',
                'PAGAMENTO FORNITORE ABC SPA',
                'COMMISSIONI BANCARIE'
            ],
            'CAUSALE ABI': ['', '103', '']
        }
        
        df = pd.DataFrame(template_data)
        
        # Crea CSV in memory
        csv_buffer = BytesIO()
        csv_content = df.to_csv(index=False, sep=';', encoding='utf-8')
        csv_buffer.write(csv_content.encode('utf-8'))
        csv_buffer.seek(0)
        
        return StreamingResponse(
            BytesIO(csv_buffer.getvalue()),
            media_type="text/csv",
            headers={"Content-Disposition": "attachment; filename=template_transazioni_bancarie.csv"}
        )
        
    except Exception as e:
        logger.error(f"Error creating CSV template: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error creating template")


@router.post("/validate-file", response_model=APIResponse)
async def validate_file(
    file: UploadFile = File(..., description="File to validate")
):
    """Validate file before import"""
    try:
        # Salva file temporaneo
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_path = temp_file.name
        
        try:
            # Valida usando adapter
            validation = await importer_adapter.validate_file_async(temp_path)
            
            return APIResponse(
                success=validation['valid'],
                message="File validation completed",
                data={
                    'filename': file.filename,
                    'validation': validation
                }
            )
            
        finally:
            # Pulizia
            try:
                os.unlink(temp_path)
            except:
                pass
        
    except Exception as e:
        logger.error(f"Error validating file: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error validating file")


@router.get("/export/invoices")
async def export_invoices(
    format: str = Query("excel", description="Export format: excel, csv, json"),
    invoice_type: Optional[str] = Query(None, description="Filter by type: Attiva, Passiva"),
    status_filter: Optional[str] = Query(None, description="Filter by payment status"),
    start_date: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    include_lines: bool = Query(False, description="Include invoice lines"),
    include_vat: bool = Query(False, description="Include VAT summary")
):
    """Export invoices to various formats using database adapter"""
    try:
        # Ottieni fatture usando adapter
        df_invoices = await db_adapter.get_invoices_async(
            type_filter=invoice_type,
            status_filter=status_filter,
            limit=10000  # Limite alto per export
        )
        
        if df_invoices.empty:
            raise HTTPException(status_code=404, detail="No invoices found with specified filters")
        
        # Applica filtri data se forniti
        if start_date:
            df_invoices = df_invoices[df_invoices['doc_date'] >= start_date]
        if end_date:
            df_invoices = df_invoices[df_invoices['doc_date'] <= end_date]
        
        # Prepara dati export
        export_columns = [
            'id', 'type', 'doc_number', 'doc_date', 'total_amount', 'due_date',
            'payment_status', 'paid_amount', 'counterparty_name'
        ]
        
        # Verifica colonne disponibili
        available_columns = [col for col in export_columns if col in df_invoices.columns]
        export_data = df_invoices[available_columns].copy()
        
        # Rinomina colonne per export
        column_mapping = {
            'id': 'ID',
            'type': 'Tipo',
            'doc_number': 'Numero Doc',
            'doc_date': 'Data Doc',
            'total_amount': 'Importo Totale',
            'due_date': 'Scadenza',
            'payment_status': 'Stato Pagamento',
            'paid_amount': 'Importo Pagato',
            'counterparty_name': 'Controparte'
        }
        
        export_data = export_data.rename(columns=column_mapping)
        
        # Calcola residuo se possibile
        if 'Importo Totale' in export_data.columns and 'Importo Pagato' in export_data.columns:
            export_data['Residuo'] = export_data['Importo Totale'] - export_data['Importo Pagato']
        
        if format == "excel":
            # Crea Excel file
            excel_buffer = BytesIO()
            
            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                export_data.to_excel(writer, sheet_name='Fatture', index=False)
                
                # TODO: Aggiungi righe fatture e riepiloghi IVA se richiesto
                # Questo richiederebbe query aggiuntive via adapter
            
            excel_buffer.seek(0)
            
            return StreamingResponse(
                BytesIO(excel_buffer.getvalue()),
                media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                headers={"Content-Disposition": "attachment; filename=movimenti_export.xlsx"}
            )
            
        elif format == "csv":
            csv_content = export_data.to_csv(index=False, sep=';', encoding='utf-8')
            csv_buffer = BytesIO(csv_content.encode('utf-8'))
            
            return StreamingResponse(
                csv_buffer,
                media_type="text/csv",
                headers={"Content-Disposition": "attachment; filename=movimenti_export.csv"}
            )
            
        elif format == "json":
            json_data = export_data.to_dict('records')
            
            return APIResponse(
                success=True,
                message=f"Exported {len(json_data)} transactions",
                data={
                    'transactions': json_data,
                    'count': len(json_data),
                    'filters_applied': {
                        'status': status_filter,
                        'start_date': start_date,
                        'end_date': end_date
                    }
                }
            )
        
        else:
            raise HTTPException(status_code=400, detail="Unsupported export format")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error exporting transactions: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error exporting transactions")


@router.get("/export/anagraphics")
async def export_anagraphics(
    format: str = Query("excel", description="Export format: excel, csv, json"),
    type_filter: Optional[str] = Query(None, description="Filter by type: Cliente, Fornitore"),
    include_stats: bool = Query(False, description="Include financial statistics")
):
    """Export anagraphics to various formats using database adapter"""
    try:
        # Ottieni anagrafiche usando adapter
        df_anagraphics = await db_adapter.get_anagraphics_async(type_filter=type_filter)
        
        if df_anagraphics.empty:
            raise HTTPException(status_code=404, detail="No anagraphics found")
        
        # Prepara dati export
        export_columns = [
            'id', 'type', 'denomination', 'piva', 'cf', 'city', 'province',
            'email', 'phone', 'score'
        ]
        
        available_columns = [col for col in export_columns if col in df_anagraphics.columns]
        export_data = df_anagraphics[available_columns].copy()
        
        # Rinomina colonne
        column_mapping = {
            'id': 'ID',
            'type': 'Tipo',
            'denomination': 'Denominazione',
            'piva': 'P.IVA',
            'cf': 'Codice Fiscale',
            'city': 'Città',
            'province': 'Provincia',
            'email': 'Email',
            'phone': 'Telefono',
            'score': 'Score'
        }
        
        export_data = export_data.rename(columns=column_mapping)
        
        if format == "excel":
            excel_buffer = BytesIO()
            
            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                export_data.to_excel(writer, sheet_name='Anagrafiche', index=False)
                
                # Aggiungi statistiche se richiesto
                if include_stats:
                    stats_data = await db_adapter.execute_query_async("""
                        SELECT 
                            a.id,
                            a.denomination,
                            COUNT(i.id) as total_invoices,
                            COALESCE(SUM(i.total_amount), 0) as total_revenue,
                            COALESCE(AVG(i.total_amount), 0) as avg_invoice_amount,
                            MAX(i.doc_date) as last_invoice_date
                        FROM Anagraphics a
                        LEFT JOIN Invoices i ON a.id = i.anagraphics_id
                        WHERE a.type = 'Cliente'
                        GROUP BY a.id, a.denomination
                        ORDER BY total_revenue DESC
                    """)
                    
                    if stats_data:
                        stats_df = pd.DataFrame(stats_data)
                        stats_df.columns = [
                            'ID', 'Denominazione', 'Fatture Totali', 
                            'Fatturato Totale', 'Importo Medio', 'Ultima Fattura'
                        ]
                        stats_df.to_excel(writer, sheet_name='Statistiche', index=False)
            
            excel_buffer.seek(0)
            
            return StreamingResponse(
                BytesIO(excel_buffer.getvalue()),
                media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                headers={"Content-Disposition": "attachment; filename=anagrafiche_export.xlsx"}
            )
            
        elif format == "csv":
            csv_content = export_data.to_csv(index=False, sep=';', encoding='utf-8')
            csv_buffer = BytesIO(csv_content.encode('utf-8'))
            
            return StreamingResponse(
                csv_buffer,
                media_type="text/csv",
                headers={"Content-Disposition": "attachment; filename=anagrafiche_export.csv"}
            )
            
        elif format == "json":
            json_data = export_data.to_dict('records')
            
            return APIResponse(
                success=True,
                message=f"Exported {len(json_data)} anagraphics",
                data={
                    'anagraphics': json_data,
                    'count': len(json_data),
                    'type_filter': type_filter
                }
            )
        
        else:
            raise HTTPException(status_code=400, detail="Unsupported export format")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error exporting anagraphics: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error exporting anagraphics")


@router.get("/export/reconciliation-report")
async def export_reconciliation_report(
    format: str = Query("excel", description="Export format: excel, csv, json"),
    period_months: int = Query(12, ge=1, le=60, description="Period in months"),
    include_unmatched: bool = Query(True, description="Include unmatched items")
):
    """Export comprehensive reconciliation report using database adapter"""
    try:
        # Ottieni link riconciliazione
        recon_links = await db_adapter.execute_query_async("""
            SELECT 
                rl.id as link_id,
                rl.reconciled_amount,
                rl.reconciliation_date,
                i.id as invoice_id,
                i.doc_number,
                i.doc_date,
                i.type as invoice_type,
                i.total_amount as invoice_amount,
                bt.id as transaction_id,
                bt.transaction_date,
                bt.amount as transaction_amount,
                bt.description,
                a.denomination as counterparty
            FROM ReconciliationLinks rl
            JOIN Invoices i ON rl.invoice_id = i.id
            JOIN BankTransactions bt ON rl.transaction_id = bt.id
            JOIN Anagraphics a ON i.anagraphics_id = a.id
            WHERE rl.reconciliation_date >= date('now', '-{} months')
            ORDER BY rl.reconciliation_date DESC
        """.format(period_months))
        
        unmatched_invoices = []
        unmatched_transactions = []
        
        if include_unmatched:
            # Ottieni fatture non riconciliate
            unmatched_invoices = await db_adapter.execute_query_async("""
                SELECT 
                    i.id, i.doc_number, i.doc_date, i.type, i.total_amount,
                    i.payment_status, (i.total_amount - i.paid_amount) as open_amount,
                    a.denomination
                FROM Invoices i
                JOIN Anagraphics a ON i.anagraphics_id = a.id
                WHERE i.payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.')
                  AND i.doc_date >= date('now', '-{} months')
                ORDER BY i.doc_date DESC
            """.format(period_months))
            
            # Ottieni transazioni non riconciliate
            unmatched_transactions = await db_adapter.execute_query_async("""
                SELECT 
                    id, transaction_date, amount, description, reconciliation_status,
                    (amount - reconciled_amount) as open_amount
                FROM BankTransactions
                WHERE reconciliation_status IN ('Da Riconciliare', 'Riconciliato Parz.')
                  AND transaction_date >= date('now', '-{} months')
                ORDER BY transaction_date DESC
            """.format(period_months))
        
        # Crea struttura report
        report_data = {
            'reconciled_items': recon_links,
            'unmatched_invoices': unmatched_invoices,
            'unmatched_transactions': unmatched_transactions,
            'summary': {
                'period_months': period_months,
                'total_reconciled_amount': sum(item.get('reconciled_amount', 0) for item in recon_links),
                'total_links': len(recon_links),
                'unmatched_invoices_count': len(unmatched_invoices),
                'unmatched_transactions_count': len(unmatched_transactions)
            }
        }
        
        if format == "json":
            return APIResponse(
                success=True,
                message="Reconciliation report generated",
                data=report_data
            )
            
        elif format == "excel":
            excel_buffer = BytesIO()
            
            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                # Sheet riconciliazioni
                if recon_links:
                    df_recon = pd.DataFrame(recon_links)
                    df_recon.to_excel(writer, sheet_name='Riconciliazioni', index=False)
                
                # Sheet fatture non riconciliate
                if unmatched_invoices:
                    df_unmatched_inv = pd.DataFrame(unmatched_invoices)
                    df_unmatched_inv.to_excel(writer, sheet_name='Fatture Non Riconciliate', index=False)
                
                # Sheet transazioni non riconciliate
                if unmatched_transactions:
                    df_unmatched_trans = pd.DataFrame(unmatched_transactions)
                    df_unmatched_trans.to_excel(writer, sheet_name='Transazioni Non Riconciliate', index=False)
                
                # Sheet riepilogo
                summary_df = pd.DataFrame([report_data['summary']])
                summary_df.to_excel(writer, sheet_name='Riepilogo', index=False)
            
            excel_buffer.seek(0)
            
            return StreamingResponse(
                BytesIO(excel_buffer.getvalue()),
                media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                headers={"Content-Disposition": "attachment; filename=report_riconciliazione.xlsx"}
            )
            
        elif format == "csv":
            # CSV semplificato con solo riconciliazioni
            if recon_links:
                df_recon = pd.DataFrame(recon_links)
                csv_content = df_recon.to_csv(index=False, sep=';', encoding='utf-8')
            else:
                csv_content = "No reconciliation data available\n"
            
            csv_buffer = BytesIO(csv_content.encode('utf-8'))
            
            return StreamingResponse(
                csv_buffer,
                media_type="text/csv",
                headers={"Content-Disposition": "attachment; filename=report_riconciliazione.csv"}
            )
        
        else:
            raise HTTPException(status_code=400, detail="Unsupported export format")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error exporting reconciliation report: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error exporting reconciliation report")


@router.post("/backup/create")
async def create_backup():
    """Create a backup of the database and files using core configuration"""
    try:
        import shutil
        from datetime import datetime
        from pathlib import Path
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"fattura_analyzer_backup_{timestamp}"
        
        # Crea directory temporanea per backup
        with tempfile.TemporaryDirectory() as temp_dir:
            backup_dir = os.path.join(temp_dir, backup_name)
            os.makedirs(backup_dir)
            
            # Copia database usando path da settings
            db_path = settings.get_database_path()
            if os.path.exists(db_path):
                shutil.copy2(db_path, os.path.join(backup_dir, "database.db"))
                logger.info(f"Database copied from {db_path}")
            
            # Copia config se esiste
            config_paths = ["config.ini", "../config.ini", "../../config.ini"]
            for config_path in config_paths:
                if os.path.exists(config_path):
                    shutil.copy2(config_path, backup_dir)
                    logger.info(f"Config copied from {config_path}")
                    break
            
            # Copia file credentials se esistono
            credentials_files = [
                settings.GOOGLE_CREDENTIALS_FILE,
                "google_token.json"
            ]
            for cred_file in credentials_files:
                if os.path.exists(cred_file):
                    shutil.copy2(cred_file, backup_dir)
                    logger.info(f"Credentials file copied: {cred_file}")
            
            # Crea archivio ZIP
            zip_path = os.path.join(temp_dir, f"{backup_name}.zip")
            shutil.make_archive(zip_path[:-4], 'zip', backup_dir)
            
            # Verifica dimensione backup
            backup_size = os.path.getsize(zip_path)
            logger.info(f"Backup created: {backup_size} bytes")
            
            # Restituisci file backup
            return FileResponse(
                zip_path,
                media_type="application/zip",
                filename=f"{backup_name}.zip",
                headers={"Content-Disposition": f"attachment; filename={backup_name}.zip"}
            )
        
    except Exception as e:
        logger.error(f"Error creating backup: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error creating backup")


@router.get("/status/import-history")
async def get_import_history(
    limit: int = Query(50, ge=1, le=200, description="Number of recent imports to show")
):
    """Get import history and statistics using database adapter"""
    try:
        # Ottieni statistiche usando adapter
        invoice_stats = await db_adapter.execute_query_async("""
            SELECT 
                COUNT(*) as total_invoices,
                COUNT(CASE WHEN created_at >= date('now', '-30 days') THEN 1 END) as invoices_last_30_days,
                COUNT(CASE WHEN created_at >= date('now', '-7 days') THEN 1 END) as invoices_last_7_days
            FROM Invoices
        """)
        
        transaction_stats = await db_adapter.execute_query_async("""
            SELECT 
                COUNT(*) as total_transactions,
                COUNT(CASE WHEN created_at >= date('now', '-30 days') THEN 1 END) as transactions_last_30_days,
                COUNT(CASE WHEN created_at >= date('now', '-7 days') THEN 1 END) as transactions_last_7_days
            FROM BankTransactions
        """)
        
        # Combina statistiche
        invoice_data = invoice_stats[0] if invoice_stats else {}
        transaction_data = transaction_stats[0] if transaction_stats else {}
        
        import_stats = {
            "total_invoices": invoice_data.get('total_invoices', 0),
            "total_transactions": transaction_data.get('total_transactions', 0),
            "last_30_days": {
                "invoices": invoice_data.get('invoices_last_30_days', 0),
                "transactions": transaction_data.get('transactions_last_30_days', 0)
            },
            "last_7_days": {
                "invoices": invoice_data.get('invoices_last_7_days', 0),
                "transactions": transaction_data.get('transactions_last_7_days', 0)
            }
        }
        
        # Per ora, storia import fittizia (in futuro si potrebbe implementare una tabella dedicata)
        import_history = []
        
        # Cerca fatture recenti come proxy per import
        recent_invoices = await db_adapter.execute_query_async("""
            SELECT 
                'XML/P7M Import' as type,
                created_at as timestamp,
                COUNT(*) as files_success,
                0 as files_errors
            FROM Invoices 
            WHERE created_at >= date('now', '-30 days')
            GROUP BY date(created_at)
            ORDER BY created_at DESC
            LIMIT ?
        """, (limit // 2,))
        
        # Cerca transazioni recenti come proxy per import CSV
        recent_transactions = await db_adapter.execute_query_async("""
            SELECT 
                'CSV Transactions' as type,
                created_at as timestamp,
                1 as files_success,
                0 as files_errors
            FROM BankTransactions 
            WHERE created_at >= date('now', '-30 days')
            GROUP BY date(created_at)
            ORDER BY created_at DESC
            LIMIT ?
        """, (limit // 2,))
        
        # Combina e formatta history
        all_imports = recent_invoices + recent_transactions
        for i, import_item in enumerate(all_imports[:limit]):
            import_history.append({
                "id": i + 1,
                "timestamp": import_item['timestamp'],
                "type": import_item['type'],
                "files_processed": import_item['files_success'],
                "files_success": import_item['files_success'],
                "files_duplicates": 0,
                "files_errors": import_item['files_errors'],
                "status": "completed"
            })
        
        return APIResponse(
            success=True,
            message="Import history retrieved",
            data={
                "import_history": import_history,
                "statistics": import_stats
            }
        )
        
    except Exception as e:
        logger.error(f"Error getting import history: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving import history"); filename=fatture_export.xlsx"}
            )
            
        elif format == "csv":
            # Crea CSV
            csv_content = export_data.to_csv(index=False, sep=';', encoding='utf-8')
            csv_buffer = BytesIO(csv_content.encode('utf-8'))
            
            return StreamingResponse(
                csv_buffer,
                media_type="text/csv",
                headers={"Content-Disposition": "attachment; filename=fatture_export.csv"}
            )
            
        elif format == "json":
            # Crea JSON
            json_data = export_data.to_dict('records')
            
            return APIResponse(
                success=True,
                message=f"Exported {len(json_data)} invoices",
                data={
                    'invoices': json_data,
                    'count': len(json_data),
                    'filters_applied': {
                        'type': invoice_type,
                        'status': status_filter,
                        'start_date': start_date,
                        'end_date': end_date
                    }
                }
            )
        
        else:
            raise HTTPException(status_code=400, detail="Unsupported export format")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error exporting invoices: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error exporting invoices")


@router.get("/export/transactions")
async def export_transactions(
    format: str = Query("excel", description="Export format: excel, csv, json"),
    status_filter: Optional[str] = Query(None, description="Filter by reconciliation status"),
    start_date: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    include_reconciliation: bool = Query(False, description="Include reconciliation details")
):
    """Export bank transactions to various formats using database adapter"""
    try:
        # Ottieni transazioni usando adapter
        df_transactions = await db_adapter.get_transactions_async(
            start_date=start_date,
            end_date=end_date,
            status_filter=status_filter,
            limit=10000
        )
        
        if df_transactions.empty:
            raise HTTPException(status_code=404, detail="No transactions found with specified filters")
        
        # Prepara dati export
        export_columns = [
            'id', 'transaction_date', 'value_date', 'amount',
            'description', 'causale_abi', 'reconciliation_status'
        ]
        
        available_columns = [col for col in export_columns if col in df_transactions.columns]
        export_data = df_transactions[available_columns].copy()
        
        # Rinomina colonne
        column_mapping = {
            'id': 'ID',
            'transaction_date': 'Data Operazione',
            'value_date': 'Data Valuta',
            'amount': 'Importo',
            'description': 'Descrizione',
            'causale_abi': 'Causale ABI',
            'reconciliation_status': 'Stato Riconciliazione'
        }
        
        export_data = export_data.rename(columns=column_mapping)
        
        if format == "excel":
            excel_buffer = BytesIO()
            
            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                export_data.to_excel(writer, sheet_name='Movimenti', index=False)
                
                # TODO: Aggiungi dettagli riconciliazione se richiesto
            
            excel_buffer.seek(0)
            
            return StreamingResponse(
                BytesIO(excel_buffer.getvalue()),
                media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                headers={"Content-Disposition": "attachment
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/api/import_export.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/api/invoices.py ---
"""
Invoices API endpoints
"""

import logging
from typing import List, Optional
from datetime import date, datetime
from fastapi import APIRouter, HTTPException, Depends, Query, Path
from fastapi.responses import JSONResponse

from app.core.database import get_db_session, execute_query_async, execute_write_async, execute_many_async
from app.models import (
    Invoice, InvoiceCreate, InvoiceUpdate, InvoiceFilter,
    PaginationParams, InvoiceListResponse, APIResponse,
    PaymentStatus, InvoiceType
)

logger = logging.getLogger(__name__)
router = APIRouter()


@router.get("/", response_model=InvoiceListResponse)
async def get_invoices_list(
    type_filter: Optional[InvoiceType] = Query(None, description="Filter by invoice type"),
    status_filter: Optional[PaymentStatus] = Query(None, description="Filter by payment status"),
    anagraphics_id: Optional[int] = Query(None, description="Filter by anagraphics ID"),
    search: Optional[str] = Query(None, description="Search in doc_number or counterparty name"),
    start_date: Optional[date] = Query(None, description="Filter by start date"),
    end_date: Optional[date] = Query(None, description="Filter by end date"),
    min_amount: Optional[float] = Query(None, description="Minimum amount filter"),
    max_amount: Optional[float] = Query(None, description="Maximum amount filter"),
    page: int = Query(1, ge=1, description="Page number"),
    size: int = Query(50, ge=1, le=1000, description="Page size")
):
    """Get paginated list of invoices with filters"""
    try:
        pagination = PaginationParams(page=page, size=size)
        
        # Build query with filters
        base_query = """
            SELECT i.id, i.type, i.doc_number, i.doc_date, i.total_amount, i.due_date,
                   i.payment_status, i.paid_amount, i.payment_method, i.anagraphics_id,
                   i.unique_hash, i.created_at, i.updated_at, i.notes,
                   a.denomination as counterparty_name,
                   (i.total_amount - i.paid_amount) as open_amount
            FROM Invoices i
            JOIN Anagraphics a ON i.anagraphics_id = a.id
        """
        
        count_query = """
            SELECT COUNT(*) as total
            FROM Invoices i
            JOIN Anagraphics a ON i.anagraphics_id = a.id
        """
        
        conditions = []
        params = []
        
        if type_filter:
            conditions.append("i.type = ?")
            params.append(type_filter.value)
        
        if status_filter:
            conditions.append("i.payment_status = ?")
            params.append(status_filter.value)
        
        if anagraphics_id:
            conditions.append("i.anagraphics_id = ?")
            params.append(anagraphics_id)
        
        if search:
            search_condition = "(i.doc_number LIKE ? OR a.denomination LIKE ?)"
            conditions.append(search_condition)
            search_param = f"%{search}%"
            params.extend([search_param, search_param])
        
        if start_date:
            conditions.append("i.doc_date >= ?")
            params.append(start_date.isoformat())
        
        if end_date:
            conditions.append("i.doc_date <= ?")
            params.append(end_date.isoformat())
        
        if min_amount is not None:
            conditions.append("i.total_amount >= ?")
            params.append(min_amount)
        
        if max_amount is not None:
            conditions.append("i.total_amount <= ?")
            params.append(max_amount)
        
        if conditions:
            where_clause = " WHERE " + " AND ".join(conditions)
            base_query += where_clause
            count_query += where_clause
        
        # Get total count
        total_result = await execute_query_async(count_query, tuple(params))
        total = total_result[0]['total'] if total_result else 0
        
        # Get paginated results
        base_query += " ORDER BY i.doc_date DESC, i.id DESC LIMIT ? OFFSET ?"
        params.extend([pagination.limit, pagination.offset])
        
        items = await execute_query_async(base_query, tuple(params))
        
        # Calculate pagination info
        pages = (total + pagination.size - 1) // pagination.size
        
        return InvoiceListResponse(
            items=items,
            total=total,
            page=pagination.page,
            size=pagination.size,
            pages=pages
        )
        
    except Exception as e:
        logger.error(f"Error getting invoices list: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error retrieving invoices: {str(e)}")


@router.get("/{invoice_id}", response_model=Invoice)
async def get_invoice_by_id(
    invoice_id: int = Path(..., description="Invoice ID")
):
    """Get invoice by ID with full details"""
    try:
        # Get main invoice data
        invoice_query = """
            SELECT i.id, i.type, i.doc_type, i.doc_number, i.doc_date, i.total_amount, 
                   i.due_date, i.payment_status, i.paid_amount, i.payment_method,
                   i.notes, i.xml_filename, i.p7m_source_file, i.unique_hash,
                   i.anagraphics_id, i.created_at, i.updated_at,
                   a.denomination as counterparty_name,
                   (i.total_amount - i.paid_amount) as open_amount
            FROM Invoices i
            JOIN Anagraphics a ON i.anagraphics_id = a.id
            WHERE i.id = ?
        """
        
        invoice_result = await execute_query_async(invoice_query, (invoice_id,))
        
        if not invoice_result:
            raise HTTPException(status_code=404, detail="Invoice not found")
        
        invoice_data = invoice_result[0]
        
        # Get invoice lines
        lines_query = """
            SELECT id, line_number, description, quantity, unit_measure, 
                   unit_price, total_price, vat_rate, item_code, item_type
            FROM InvoiceLines
            WHERE invoice_id = ?
            ORDER BY line_number
        """
        
        lines = await execute_query_async(lines_query, (invoice_id,))
        
        # Get VAT summary
        vat_query = """
            SELECT id, vat_rate, taxable_amount, vat_amount
            FROM InvoiceVATSummary
            WHERE invoice_id = ?
            ORDER BY vat_rate
        """
        
        vat_summary = await execute_query_async(vat_query, (invoice_id,))
        
        # Combine all data
        invoice_data['lines'] = lines
        invoice_data['vat_summary'] = vat_summary
        
        return invoice_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting invoice {invoice_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving invoice")


@router.post("/", response_model=Invoice)
async def create_invoice(invoice_data: InvoiceCreate):
    """Create new invoice with lines and VAT summary"""
    try:
        # Verify anagraphics exists
        anag_check = await execute_query_async(
            "SELECT id FROM Anagraphics WHERE id = ?", 
            (invoice_data.anagraphics_id,)
        )
        if not anag_check:
            raise HTTPException(
                status_code=400, 
                detail=f"Anagraphics ID {invoice_data.anagraphics_id} not found"
            )
        
        # Generate unique hash (simplified - in reality would use your existing hash function)
        import hashlib
        from datetime import datetime
        hash_input = f"{invoice_data.anagraphics_id}_{invoice_data.doc_number}_{invoice_data.doc_date}_{datetime.now().isoformat()}"
        unique_hash = hashlib.sha256(hash_input.encode()).hexdigest()
        
        # Insert main invoice
        invoice_insert = """
            INSERT INTO Invoices 
            (anagraphics_id, type, doc_type, doc_number, doc_date, total_amount, 
             due_date, payment_method, notes, xml_filename, p7m_source_file, 
             unique_hash, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'), datetime('now'))
        """
        
        invoice_params = (
            invoice_data.anagraphics_id,
            invoice_data.type.value,
            invoice_data.doc_type,
            invoice_data.doc_number,
            invoice_data.doc_date.isoformat(),
            invoice_data.total_amount,
            invoice_data.due_date.isoformat() if invoice_data.due_date else None,
            invoice_data.payment_method,
            invoice_data.notes,
            invoice_data.xml_filename,
            invoice_data.p7m_source_file,
            unique_hash
        )
        
        invoice_id = await execute_write_async(invoice_insert, invoice_params)
        
        if not invoice_id:
            raise HTTPException(status_code=500, detail="Failed to create invoice")
        
        # Insert invoice lines if provided
        if invoice_data.lines:
            lines_insert = """
                INSERT INTO InvoiceLines 
                (invoice_id, line_number, description, quantity, unit_measure, 
                 unit_price, total_price, vat_rate, item_code, item_type)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """
            
            lines_params = [
                (
                    invoice_id,
                    line.line_number,
                    line.description,
                    line.quantity,
                    line.unit_measure,
                    line.unit_price,
                    line.total_price,
                    line.vat_rate,
                    line.item_code,
                    line.item_type
                )
                for line in invoice_data.lines
            ]
            
            await execute_many_async(lines_insert, lines_params)
        
        # Insert VAT summary if provided
        if invoice_data.vat_summary:
            vat_insert = """
                INSERT INTO InvoiceVATSummary 
                (invoice_id, vat_rate, taxable_amount, vat_amount)
                VALUES (?, ?, ?, ?)
            """
            
            vat_params = [
                (
                    invoice_id,
                    vat.vat_rate,
                    vat.taxable_amount,
                    vat.vat_amount
                )
                for vat in invoice_data.vat_summary
            ]
            
            await execute_many_async(vat_insert, vat_params)
        
        # Return the created invoice
        return await get_invoice_by_id(invoice_id)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating invoice: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error creating invoice")


@router.put("/{invoice_id}", response_model=Invoice)
async def update_invoice(
    invoice_id: int = Path(..., description="Invoice ID"),
    invoice_data: InvoiceUpdate = ...
):
    """Update invoice"""
    try:
        # Check if invoice exists
        existing = await execute_query_async("SELECT id FROM Invoices WHERE id = ?", (invoice_id,))
        if not existing:
            raise HTTPException(status_code=404, detail="Invoice not found")
        
        # Build update query dynamically
        update_fields = []
        params = []
        
        for field, value in invoice_data.model_dump(exclude_unset=True).items():
            if value is not None:
                if field in ['doc_date', 'due_date'] and hasattr(value, 'isoformat'):
                    value = value.isoformat()
                elif hasattr(value, 'value'):  # Enum
                    value = value.value
                update_fields.append(f"{field} = ?")
                params.append(value)
        
        if not update_fields:
            raise HTTPException(status_code=400, detail="No fields to update")
        
        # Add updated_at
        update_fields.append("updated_at = datetime('now')")
        params.append(invoice_id)
        
        update_query = f"UPDATE Invoices SET {', '.join(update_fields)} WHERE id = ?"
        
        rows_affected = await execute_write_async(update_query, tuple(params))
        
        if rows_affected == 0:
            raise HTTPException(status_code=404, detail="Invoice not found")
        
        # Return updated invoice
        return await get_invoice_by_id(invoice_id)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating invoice {invoice_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error updating invoice")


@router.delete("/{invoice_id}", response_model=APIResponse)
async def delete_invoice(
    invoice_id: int = Path(..., description="Invoice ID")
):
    """Delete invoice and related data"""
    try:
        # Check for existing reconciliation links
        links_query = "SELECT COUNT(*) as count FROM ReconciliationLinks WHERE invoice_id = ?"
        links_result = await execute_query_async(links_query, (invoice_id,))
        
        if links_result and links_result[0]['count'] > 0:
            raise HTTPException(
                status_code=409, 
                detail="Cannot delete invoice with existing reconciliation links"
            )
        
        # Delete invoice (lines and VAT summary will be deleted by CASCADE)
        delete_query = "DELETE FROM Invoices WHERE id = ?"
        rows_affected = await execute_write_async(delete_query, (invoice_id,))
        
        if rows_affected == 0:
            raise HTTPException(status_code=404, detail="Invoice not found")
        
        return APIResponse(
            success=True,
            message="Invoice deleted successfully"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting invoice {invoice_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error deleting invoice")


@router.get("/{invoice_id}/reconciliation-links")
async def get_invoice_reconciliation_links(
    invoice_id: int = Path(..., description="Invoice ID")
):
    """Get reconciliation links for invoice"""
    try:
        query = """
            SELECT rl.id, rl.transaction_id, rl.reconciled_amount, rl.reconciliation_date,
                   bt.transaction_date, bt.amount as transaction_amount, bt.description
            FROM ReconciliationLinks rl
            JOIN BankTransactions bt ON rl.transaction_id = bt.id
            WHERE rl.invoice_id = ?
            ORDER BY rl.reconciliation_date DESC
        """
        
        links = await execute_query_async(query, (invoice_id,))
        
        return APIResponse(
            success=True,
            message=f"Found {len(links)} reconciliation links",
            data=links
        )
        
    except Exception as e:
        logger.error(f"Error getting reconciliation links for invoice {invoice_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving reconciliation links")


@router.get("/overdue/list")
async def get_overdue_invoices(
    limit: int = Query(20, ge=1, le=100, description="Limit number of results")
):
    """Get overdue invoices ordered by priority"""
    try:
        from app.core.analysis import get_top_overdue_invoices
        
        overdue_invoices = get_top_overdue_invoices(limit)
        
        return APIResponse(
            success=True,
            message=f"Found {len(overdue_invoices)} overdue invoices",
            data=overdue_invoices.to_dict('records') if not overdue_invoices.empty else []
        )
        
    except Exception as e:
        logger.error(f"Error getting overdue invoices: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving overdue invoices")


@router.get("/aging/summary")
async def get_aging_summary(
    invoice_type: InvoiceType = Query(InvoiceType.ATTIVA, description="Invoice type for aging analysis")
):
    """Get aging summary for invoices"""
    try:
        from app.core.analysis import get_aging_summary
        
        aging_data = get_aging_summary(invoice_type.value)
        
        # Convert to response format
        buckets = []
        total_amount = 0.0
        total_count = 0
        
        for label, data in aging_data.items():
            amount = float(data['amount'])
            count = data['count']
            buckets.append({
                "label": label,
                "amount": amount,
                "count": count
            })
            total_amount += amount
            total_count += count
        
        return APIResponse(
            success=True,
            message="Aging summary calculated",
            data={
                "buckets": buckets,
                "total_amount": total_amount,
                "total_count": total_count,
                "invoice_type": invoice_type.value
            }
        )
        
    except Exception as e:
        logger.error(f"Error getting aging summary: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error calculating aging summary")


@router.get("/stats/summary")
async def get_invoices_stats():
    """Get invoice statistics summary"""
    try:
        stats_query = """
            SELECT 
                type,
                payment_status,
                COUNT(*) as count,
                SUM(total_amount) as total_amount,
                SUM(paid_amount) as paid_amount,
                SUM(total_amount - paid_amount) as open_amount
            FROM Invoices
            GROUP BY type, payment_status
        """
        
        stats = await execute_query_async(stats_query)
        
        # Get recent invoices
        recent_query = """
            SELECT i.id, i.type, i.doc_number, i.doc_date, i.total_amount,
                   i.payment_status, a.denomination as counterparty_name
            FROM Invoices i
            JOIN Anagraphics a ON i.anagraphics_id = a.id
            ORDER BY i.created_at DESC
            LIMIT 10
        """
        
        recent = await execute_query_async(recent_query)
        
        return APIResponse(
            success=True,
            message="Statistics retrieved",
            data={
                "status_stats": stats,
                "recent_invoices": recent
            }
        )
        
    except Exception as e:
        logger.error(f"Error getting invoice stats: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving statistics")


@router.get("/search/{query}")
async def search_invoices(
    query: str = Path(..., description="Search query"),
    type_filter: Optional[InvoiceType] = Query(None, description="Filter by type"),
    limit: int = Query(10, ge=1, le=100)
):
    """Search invoices by document number or counterparty name"""
    try:
        search_query = """
            SELECT i.id, i.type, i.doc_number, i.doc_date, i.total_amount,
                   i.payment_status, a.denomination as counterparty_name,
                   (i.total_amount - i.paid_amount) as open_amount
            FROM Invoices i
            JOIN Anagraphics a ON i.anagraphics_id = a.id
            WHERE (
                i.doc_number LIKE ? OR 
                a.denomination LIKE ?
            )
        """
        
        params = [f"%{query}%", f"%{query}%"]
        
        if type_filter:
            search_query += " AND i.type = ?"
            params.append(type_filter.value)
        
        search_query += " ORDER BY i.doc_date DESC LIMIT ?"
        params.append(limit)
        
        results = await execute_query_async(search_query, tuple(params))
        
        return APIResponse(
            success=True,
            message=f"Found {len(results)} results",
            data={
                "query": query,
                "results": results,
                "total": len(results)
            }
        )
        
    except Exception as e:
        logger.error(f"Error searching invoices: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error searching invoices")


@router.post("/{invoice_id}/update-payment-status", response_model=APIResponse)
async def update_invoice_payment_status(
    invoice_id: int = Path(..., description="Invoice ID"),
    payment_status: PaymentStatus = Query(..., description="New payment status"),
    paid_amount: Optional[float] = Query(None, description="Amount paid")
):
    """Update invoice payment status and paid amount"""
    try:
        # Get current invoice data
        current_query = "SELECT total_amount, paid_amount FROM Invoices WHERE id = ?"
        current_result = await execute_query_async(current_query, (invoice_id,))
        
        if not current_result:
            raise HTTPException(status_code=404, detail="Invoice not found")
        
        current_data = current_result[0]
        
        # If paid_amount not provided, calculate based on status
        if paid_amount is None:
            if payment_status == PaymentStatus.PAGATA_TOT:
                paid_amount = current_data['total_amount']
            elif payment_status == PaymentStatus.APERTA:
                paid_amount = 0.0
            else:
                paid_amount = current_data['paid_amount']
        
        # Validate paid amount
        if paid_amount < 0 or paid_amount > current_data['total_amount']:
            raise HTTPException(
                status_code=400, 
                detail="Paid amount must be between 0 and total amount"
            )
        
        # Update invoice
        update_query = """
            UPDATE Invoices 
            SET payment_status = ?, paid_amount = ?, updated_at = datetime('now')
            WHERE id = ?
        """
        
        rows_affected = await execute_write_async(
            update_query, 
            (payment_status.value, paid_amount, invoice_id)
        )
        
        if rows_affected == 0:
            raise HTTPException(status_code=404, detail="Invoice not found")
        
        return APIResponse(
            success=True,
            message=f"Payment status updated to {payment_status.value}",
            data={
                "invoice_id": invoice_id,
                "new_status": payment_status.value,
                "paid_amount": paid_amount
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating payment status for invoice {invoice_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error updating payment status")
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/api/invoices.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/api/reconciliation.py ---
"""
Reconciliation API endpoints - Aggiornato per usare adapter pattern
"""

import logging
from typing import List, Optional
from fastapi import APIRouter, HTTPException, Query, Path, Body
from fastapi.responses import JSONResponse

# Usa adapters invece di accesso diretto al core
from app.adapters.reconciliation_adapter import reconciliation_adapter
from app.adapters.database_adapter import db_adapter
from app.models import (
    ReconciliationSuggestion, ReconciliationRequest, ReconciliationBatchRequest,
    ReconciliationLink, APIResponse
)

logger = logging.getLogger(__name__)
router = APIRouter()


@router.get("/suggestions")
async def get_reconciliation_suggestions(
    max_suggestions: int = Query(50, ge=1, le=200, description="Maximum number of suggestions"),
    confidence_threshold: float = Query(0.5, ge=0.0, le=1.0, description="Minimum confidence threshold")
):
    """Get reconciliation suggestions using core adapter"""
    try:
        # Usa adapter per ottenere suggerimenti dal core
        suggestions = await reconciliation_adapter.get_reconciliation_suggestions_async(
            max_suggestions=max_suggestions,
            confidence_threshold=confidence_threshold
        )
        
        return APIResponse(
            success=True,
            message=f"Found {len(suggestions)} reconciliation suggestions",
            data={
                "suggestions": suggestions,
                "parameters": {
                    "max_suggestions": max_suggestions,
                    "confidence_threshold": confidence_threshold
                }
            }
        )
        
    except Exception as e:
        logger.error(f"Error getting reconciliation suggestions: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving reconciliation suggestions")


@router.get("/opportunities")
async def get_reconciliation_opportunities(
    limit: int = Query(20, ge=1, le=100, description="Maximum number of opportunities"),
    amount_tolerance: float = Query(0.01, ge=0.0, le=100.0, description="Amount tolerance for matching")
):
    """Get reconciliation opportunities using advanced matching"""
    try:
        opportunities = await reconciliation_adapter.get_reconciliation_opportunities_async(
            limit=limit,
            amount_tolerance=amount_tolerance
        )
        
        return APIResponse(
            success=True,
            message=f"Found {len(opportunities)} reconciliation opportunities",
            data={
                "opportunities": opportunities,
                "parameters": {
                    "limit": limit,
                    "amount_tolerance": amount_tolerance
                }
            }
        )
        
    except Exception as e:
        logger.error(f"Error getting reconciliation opportunities: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving reconciliation opportunities")


@router.get("/client/{anagraphics_id}/suggestions")
async def get_client_reconciliation_suggestions(
    anagraphics_id: int = Path(..., description="Anagraphics ID"),
    max_suggestions: int = Query(10, ge=1, le=50, description="Maximum number of suggestions")
):
    """Get reconciliation suggestions for specific client using smart reconciliation"""
    try:
        # Verifica che l'anagrafica esista
        anag_check = await db_adapter.execute_query_async(
            "SELECT id, denomination FROM Anagraphics WHERE id = ? AND type = 'Cliente'",
            (anagraphics_id,)
        )
        
        if not anag_check:
            raise HTTPException(status_code=404, detail="Client not found")
        
        # Usa adapter per smart reconciliation
        suggestions = await reconciliation_adapter.smart_reconcile_by_client_async(
            anagraphics_id=anagraphics_id,
            max_suggestions=max_suggestions
        )
        
        return APIResponse(
            success=True,
            message=f"Found {len(suggestions)} suggestions for client {anag_check[0]['denomination']}",
            data={
                "client": anag_check[0],
                "suggestions": suggestions
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting client reconciliation suggestions: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving client suggestions")


@router.post("/reconcile")
async def perform_reconciliation(
    reconciliation: ReconciliationRequest
):
    """Perform single reconciliation using adapter"""
    try:
        result = await reconciliation_adapter.perform_reconciliation_async(
            invoice_id=reconciliation.invoice_id,
            transaction_id=reconciliation.transaction_id,
            amount=reconciliation.amount
        )
        
        if result['success']:
            return APIResponse(
                success=True,
                message=result['message'],
                data=result.get('data')
            )
        else:
            raise HTTPException(status_code=400, detail=result['message'])
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error performing reconciliation: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error performing reconciliation")


@router.post("/reconcile/batch")
async def perform_batch_reconciliation(
    reconciliations: List[ReconciliationRequest]
):
    """Perform multiple reconciliations in batch using adapter"""
    try:
        if len(reconciliations) > 100:
            raise HTTPException(status_code=400, detail="Maximum 100 reconciliations per batch")
        
        # Converti in formato per adapter
        recon_data = [
            {
                'invoice_id': r.invoice_id,
                'transaction_id': r.transaction_id,
                'amount': r.amount
            }
            for r in reconciliations
        ]
        
        results = await reconciliation_adapter.perform_batch_reconciliation_async(recon_data)
        
        return APIResponse(
            success=True,
            message=f"Batch reconciliation completed: {results['successful']} successful, {results['failed']} failed",
            data=results
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error performing batch reconciliation: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error performing batch reconciliation")


@router.post("/auto-reconcile")
async def auto_reconcile_high_confidence(
    confidence_threshold: float = Query(0.8, ge=0.5, le=1.0, description="Minimum confidence for auto reconciliation"),
    max_auto_reconcile: int = Query(10, ge=1, le=50, description="Maximum number of auto reconciliations")
):
    """Automatically reconcile high-confidence matches using adapter"""
    try:
        results = await reconciliation_adapter.auto_reconcile_suggestions_async(
            confidence_threshold=confidence_threshold,
            max_auto_reconcile=max_auto_reconcile
        )
        
        if results['success']:
            return APIResponse(
                success=True,
                message=results['message'],
                data=results['data']
            )
        else:
            raise HTTPException(status_code=500, detail=results['message'])
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in auto reconciliation: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error in auto reconciliation")


@router.delete("/undo/invoice/{invoice_id}")
async def undo_invoice_reconciliation(
    invoice_id: int = Path(..., description="Invoice ID")
):
    """Undo all reconciliations for an invoice using adapter"""
    try:
        # Verifica che la fattura esista
        invoice_check = await db_adapter.execute_query_async(
            "SELECT id, doc_number FROM Invoices WHERE id = ?",
            (invoice_id,)
        )
        
        if not invoice_check:
            raise HTTPException(status_code=404, detail="Invoice not found")
        
        result = await reconciliation_adapter.undo_reconciliation_async(invoice_id=invoice_id)
        
        if result['success']:
            return APIResponse(
                success=True,
                message=result['message'],
                data=result.get('data')
            )
        else:
            raise HTTPException(status_code=500, detail=result['message'])
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error undoing invoice reconciliation: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error undoing reconciliation")


@router.delete("/undo/transaction/{transaction_id}")
async def undo_transaction_reconciliation(
    transaction_id: int = Path(..., description="Transaction ID")
):
    """Undo all reconciliations for a transaction using adapter"""
    try:
        # Verifica che la transazione esista
        transaction_check = await db_adapter.execute_query_async(
            "SELECT id, description FROM BankTransactions WHERE id = ?",
            (transaction_id,)
        )
        
        if not transaction_check:
            raise HTTPException(status_code=404, detail="Transaction not found")
        
        result = await reconciliation_adapter.undo_reconciliation_async(transaction_id=transaction_id)
        
        if result['success']:
            return APIResponse(
                success=True,
                message=result['message'],
                data=result.get('data')
            )
        else:
            raise HTTPException(status_code=500, detail=result['message'])
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error undoing transaction reconciliation: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error undoing reconciliation")


@router.get("/status")
async def get_reconciliation_status():
    """Get comprehensive reconciliation status using adapter"""
    try:
        status = await reconciliation_adapter.get_reconciliation_status_async()
        
        if status['success']:
            return APIResponse(
                success=True,
                message="Reconciliation status retrieved",
                data=status['data']
            )
        else:
            raise HTTPException(status_code=500, detail=status['message'])
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting reconciliation status: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving reconciliation status")


@router.get("/links")
async def get_reconciliation_links(
    invoice_id: Optional[int] = Query(None, description="Filter by invoice ID"),
    transaction_id: Optional[int] = Query(None, description="Filter by transaction ID"),
    limit: int = Query(50, ge=1, le=200, description="Maximum number of links"),
    offset: int = Query(0, ge=0, description="Offset for pagination")
):
    """Get reconciliation links with optional filters using database adapter"""
    try:
        # Build query con filtri
        conditions = []
        params = []
        
        base_query = """
            SELECT 
                rl.id,
                rl.invoice_id,
                rl.transaction_id,
                rl.reconciled_amount,
                rl.reconciliation_date,
                rl.notes,
                i.doc_number,
                i.doc_date,
                i.total_amount as invoice_amount,
                bt.transaction_date,
                bt.amount as transaction_amount,
                bt.description as transaction_description,
                a.denomination as counterparty
            FROM ReconciliationLinks rl
            JOIN Invoices i ON rl.invoice_id = i.id
            JOIN BankTransactions bt ON rl.transaction_id = bt.id
            JOIN Anagraphics a ON i.anagraphics_id = a.id
        """
        
        if invoice_id:
            conditions.append("rl.invoice_id = ?")
            params.append(invoice_id)
        
        if transaction_id:
            conditions.append("rl.transaction_id = ?")
            params.append(transaction_id)
        
        if conditions:
            base_query += " WHERE " + " AND ".join(conditions)
        
        base_query += " ORDER BY rl.reconciliation_date DESC LIMIT ? OFFSET ?"
        params.extend([limit, offset])
        
        # Ottieni link
        links = await db_adapter.execute_query_async(base_query, tuple(params))
        
        # Count totale per paginazione
        count_query = "SELECT COUNT(*) as total FROM ReconciliationLinks rl"
        if conditions:
            count_query += " WHERE " + " AND ".join(conditions[:-2])  # Rimuovi limit/offset params
            count_params = params[:-2]
        else:
            count_params = []
        
        total_result = await db_adapter.execute_query_async(count_query, tuple(count_params))
        total = total_result[0]['total'] if total_result else 0
        
        return APIResponse(
            success=True,
            message=f"Retrieved {len(links)} reconciliation links",
            data={
                "links": links,
                "pagination": {
                    "total": total,
                    "limit": limit,
                    "offset": offset,
                    "has_more": (offset + len(links)) < total
                }
            }
        )
        
    except Exception as e:
        logger.error(f"Error getting reconciliation links: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving reconciliation links")


@router.get("/analytics/summary")
async def get_reconciliation_analytics():
    """Get reconciliation analytics and trends using database adapter"""
    try:
        # Trend riconciliazioni per mese
        monthly_trends = await db_adapter.execute_query_async("""
            SELECT 
                strftime('%Y-%m', reconciliation_date) as month,
                COUNT(*) as reconciliation_count,
                SUM(reconciled_amount) as total_amount,
                AVG(reconciled_amount) as avg_amount
            FROM ReconciliationLinks
            WHERE reconciliation_date >= date('now', '-12 months')
            GROUP BY strftime('%Y-%m', reconciliation_date)
            ORDER BY month
        """)
        
        # Top controparti per riconciliazioni
        top_counterparties = await db_adapter.execute_query_async("""
            SELECT 
                a.denomination,
                COUNT(rl.id) as reconciliation_count,
                SUM(rl.reconciled_amount) as total_reconciled
            FROM ReconciliationLinks rl
            JOIN Invoices i ON rl.invoice_id = i.id
            JOIN Anagraphics a ON i.anagraphics_id = a.id
            WHERE rl.reconciliation_date >= date('now', '-6 months')
            GROUP BY a.id, a.denomination
            ORDER BY total_reconciled DESC
            LIMIT 10
        """)
        
        # Tempi medi di riconciliazione
        avg_reconciliation_time = await db_adapter.execute_query_async("""
            SELECT 
                AVG(julianday(rl.reconciliation_date) - julianday(i.doc_date)) as avg_days_to_reconcile,
                AVG(julianday(rl.reconciliation_date) - julianday(bt.transaction_date)) as avg_days_from_transaction
            FROM ReconciliationLinks rl
            JOIN Invoices i ON rl.invoice_id = i.id
            JOIN BankTransactions bt ON rl.transaction_id = bt.id
            WHERE rl.reconciliation_date >= date('now', '-6 months')
        """)
        
        # Efficienza riconciliazione per mese
        efficiency_stats = await db_adapter.execute_query_async("""
            SELECT 
                strftime('%Y-%m', doc_date) as month,
                COUNT(*) as invoices_issued,
                SUM(CASE WHEN payment_status = 'Pagata Tot.' THEN 1 ELSE 0 END) as fully_paid,
                SUM(CASE WHEN payment_status IN ('Pagata Parz.', 'Pagata Tot.') THEN 1 ELSE 0 END) as partially_paid,
                (CAST(SUM(CASE WHEN payment_status = 'Pagata Tot.' THEN 1 ELSE 0 END) AS REAL) / COUNT(*)) * 100 as full_payment_rate
            FROM Invoices
            WHERE type = 'Attiva'
              AND doc_date >= date('now', '-12 months')
            GROUP BY strftime('%Y-%m', doc_date)
            ORDER BY month
        """)
        
        return APIResponse(
            success=True,
            message="Reconciliation analytics retrieved",
            data={
                "monthly_trends": monthly_trends,
                "top_counterparties": top_counterparties,
                "average_reconciliation_time": avg_reconciliation_time[0] if avg_reconciliation_time else {},
                "efficiency_statistics": efficiency_stats
            }
        )
        
    except Exception as e:
        logger.error(f"Error getting reconciliation analytics: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving reconciliation analytics")


@router.post("/validate-match")
async def validate_reconciliation_match(
    invoice_id: int = Body(..., description="Invoice ID"),
    transaction_id: int = Body(..., description="Transaction ID"),
    amount: float = Body(..., description="Amount to reconcile")
):
    """Validate a potential reconciliation match before performing it"""
    try:
        # Ottieni dettagli fattura e transazione
        invoice = await db_adapter.get_item_details_async('invoice', invoice_id)
        if not invoice:
            raise HTTPException(status_code=404, detail="Invoice not found")
        
        transaction = await db_adapter.get_item_details_async('transaction', transaction_id)
        if not transaction:
            raise HTTPException(status_code=404, detail="Transaction not found")
        
        # Calcola importi disponibili
        invoice_open_amount = invoice['total_amount'] - invoice['paid_amount']
        transaction_remaining = transaction['amount'] - transaction['reconciled_amount']
        
        # Validazioni
        validation_results = {
            'valid': True,
            'warnings': [],
            'errors': [],
            'details': {
                'invoice': {
                    'id': invoice_id,
                    'doc_number': invoice['doc_number'],
                    'total_amount': invoice['total_amount'],
                    'paid_amount': invoice['paid_amount'],
                    'open_amount': invoice_open_amount
                },
                'transaction': {
                    'id': transaction_id,
                    'transaction_date': transaction['transaction_date'],
                    'amount': transaction['amount'],
                    'reconciled_amount': transaction['reconciled_amount'],
                    'remaining_amount': transaction_remaining
                },
                'proposed_amount': amount
            }
        }
        
        # Controlli di validazione
        if amount <= 0:
            validation_results['valid'] = False
            validation_results['errors'].append("Amount must be greater than 0")
        
        if amount > invoice_open_amount:
            validation_results['valid'] = False
            validation_results['errors'].append(f"Amount {amount} exceeds invoice open amount {invoice_open_amount}")
        
        if abs(amount) > abs(transaction_remaining):
            validation_results['valid'] = False
            validation_results['errors'].append(f"Amount {amount} exceeds transaction remaining amount {transaction_remaining}")
        
        # Avvertimenti
        if abs(amount - invoice_open_amount) > 0.01:
            validation_results['warnings'].append("Amount does not fully reconcile the invoice")
        
        if abs(abs(amount) - abs(transaction_remaining)) > 0.01:
            validation_results['warnings'].append("Amount does not fully reconcile the transaction")
        
        # Controllo date
        try:
            from datetime import datetime
            invoice_date = datetime.fromisoformat(invoice['doc_date'])
            transaction_date = datetime.fromisoformat(transaction['transaction_date'])
            days_diff = (transaction_date - invoice_date).days
            
            if days_diff < 0:
                validation_results['warnings'].append("Transaction date is before invoice date")
            elif days_diff > 180:
                validation_results['warnings'].append("Transaction date is more than 6 months after invoice date")
                
        except:
            validation_results['warnings'].append("Could not validate date relationship")
        
        return APIResponse(
            success=True,
            message="Validation completed",
            data=validation_results
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error validating reconciliation match: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error validating match")


@router.get("/rules/matching")
async def get_matching_rules():
    """Get current matching rules and configuration"""
    try:
        # Regole di matching attuali (potrebbero essere configurabili in futuro)
        matching_rules = {
            'amount_tolerance': {
                'exact_match': 0.01,
                'close_match': 0.05,  # 5% tolerance
                'max_difference': 100.0  # Max 100€ difference
            },
            'date_constraints': {
                'max_days_before_invoice': 0,
                'max_days_after_invoice': 180,
                'preferred_window_days': 60
            },
            'description_matching': {
                'enabled': True,
                'minimum_similarity': 0.7,
                'keywords_boost': ['pagamento', 'bonifico', 'fattura']
            },
            'confidence_thresholds': {
                'high': 0.8,
                'medium': 0.6,
                'low': 0.4
            },
            'auto_reconciliation': {
                'enabled': True,
                'min_confidence': 0.8,
                'exact_match_only': True,
                'max_per_batch': 10
            }
        }
        
        return APIResponse(
            success=True,
            message="Matching rules retrieved",
            data={
                'rules': matching_rules,
                'last_updated': '2025-06-03T00:00:00Z',
                'version': '2.0'
            }
        )
        
    except Exception as e:
        logger.error(f"Error getting matching rules: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving matching rules")


@router.get("/health-check")
async def reconciliation_health_check():
    """Health check for reconciliation system"""
    try:
        # Verifica accesso ai dati
        invoice_count = await db_adapter.execute_query_async(
            "SELECT COUNT(*) as count FROM Invoices WHERE payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.')"
        )
        
        transaction_count = await db_adapter.execute_query_async(
            "SELECT COUNT(*) as count FROM BankTransactions WHERE reconciliation_status IN ('Da Riconciliare', 'Riconciliato Parz.')"
        )
        
        recent_reconciliations = await db_adapter.execute_query_async(
            "SELECT COUNT(*) as count FROM ReconciliationLinks WHERE reconciliation_date >= date('now', '-7 days')"
        )
        
        # Test funzionalità core
        try:
            # Test rapido del core reconciliation
            suggestions = await reconciliation_adapter.get_reconciliation_suggestions_async(
                max_suggestions=1,
                confidence_threshold=0.9
            )
            core_status = "healthy"
        except Exception as core_error:
            logger.error(f"Core reconciliation test failed: {core_error}")
            core_status = f"error: {str(core_error)}"
        
        health_data = {
            'status': 'healthy' if core_status == 'healthy' else 'degraded',
            'core_reconciliation': core_status,
            'data_availability': {
                'open_invoices': invoice_count[0]['count'] if invoice_count else 0,
                'unreconciled_transactions': transaction_count[0]['count'] if transaction_count else 0,
                'recent_reconciliations': recent_reconciliations[0]['count'] if recent_reconciliations else 0
            },
            'system_info': {
                'adapter_version': '2.0',
                'database_connection': 'active',
                'last_check': '2025-06-03T00:00:00Z'
            }
        }
        
        return APIResponse(
            success=True,
            message="Reconciliation system health check completed",
            data=health_data
        )
        
    except Exception as e:
        logger.error(f"Error in reconciliation health check: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error performing health check")


@router.post("/suggestions/manual")
async def create_manual_suggestion(
    invoice_id: int = Body(..., description="Invoice ID"),
    transaction_id: int = Body(..., description="Transaction ID"),
    confidence: str = Body(..., description="Confidence level: Alta, Media, Bassa"),
    notes: Optional[str] = Body(None, description="Optional notes")
):
    """Create a manual reconciliation suggestion"""
    try:
        # Valida input
        if confidence not in ['Alta', 'Media', 'Bassa']:
            raise HTTPException(status_code=400, detail="Confidence must be 'Alta', 'Media', or 'Bassa'")
        
        # Verifica che fattura e transazione esistano
        invoice = await db_adapter.get_item_details_async('invoice', invoice_id)
        if not invoice:
            raise HTTPException(status_code=404, detail="Invoice not found")
        
        transaction = await db_adapter.get_item_details_async('transaction', transaction_id)
        if not transaction:
            raise HTTPException(status_code=404, detail="Transaction not found")
        
        # Verifica che non sia già riconciliata
        existing_link = await db_adapter.execute_query_async(
            "SELECT id FROM ReconciliationLinks WHERE invoice_id = ? AND transaction_id = ?",
            (invoice_id, transaction_id)
        )
        
        if existing_link:
            raise HTTPException(status_code=409, detail="Reconciliation link already exists")
        
        # Calcola importi
        invoice_open_amount = invoice['total_amount'] - invoice['paid_amount']
        transaction_remaining = transaction['amount'] - transaction['reconciled_amount']
        suggested_amount = min(abs(invoice_open_amount), abs(transaction_remaining))
        
        # Crea suggerimento manuale
        manual_suggestion = {
            'confidence': confidence,
            'confidence_score': {'Alta': 0.9, 'Media': 0.7, 'Bassa': 0.5}[confidence],
            'invoice_ids': [invoice_id],
            'transaction_ids': [transaction_id],
            'description': f"Manual suggestion: {invoice['doc_number']} <-> {transaction.get('description', 'Transaction')}",
            'total_amount': suggested_amount,
            'match_details': {
                'type': 'manual',
                'invoice_open_amount': invoice_open_amount,
                'transaction_remaining': transaction_remaining,
                'suggested_amount': suggested_amount,
                'notes': notes
            },
            'reasons': ['Manual suggestion by user']
        }
        
        return APIResponse(
            success=True,
            message="Manual suggestion created",
            data=manual_suggestion
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating manual suggestion: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error creating manual suggestion")


@router.get("/export/report")
async def export_reconciliation_report(
    format: str = Query("json", description="Export format: json, csv"),
    period_months: int = Query(12, ge=1, le=60, description="Period in months"),
    include_unmatched: bool = Query(True, description="Include unmatched items")
):
    """Export comprehensive reconciliation report"""
    try:
        # Usa la funzionalità esistente dall'import/export API
        from app.api.import_export import export_reconciliation_report as export_func
        
        # Delega alla funzione di export esistente
        return await export_func(format, period_months, include_unmatched)
        
    except Exception as e:
        logger.error(f"Error exporting reconciliation report: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error exporting reconciliation report")


@router.get("/dashboard")
async def get_reconciliation_dashboard():
    """Get comprehensive reconciliation dashboard data"""
    try:
        # Combina varie chiamate per dashboard completa
        status_result = await reconciliation_adapter.get_reconciliation_status_async()
        
        if not status_result['success']:
            raise HTTPException(status_code=500, detail="Error getting reconciliation status")
        
        # Ottieni top suggerimenti
        opportunities = await reconciliation_adapter.get_reconciliation_opportunities_async(limit=5)
        
        # Link recenti
        recent_links = await db_adapter.execute_query_async("""
            SELECT 
                rl.reconciled_amount,
                rl.reconciliation_date,
                i.doc_number,
                i.doc_date,
                bt.description,
                a.denomination
            FROM ReconciliationLinks rl
            JOIN Invoices i ON rl.invoice_id = i.id
            JOIN BankTransactions bt ON rl.transaction_id = bt.id
            JOIN Anagraphics a ON i.anagraphics_id = a.id
            ORDER BY rl.reconciliation_date DESC
            LIMIT 10
        """)
        
        # Statistiche veloci
        quick_stats = await db_adapter.execute_query_async("""
            SELECT 
                (SELECT COUNT(*) FROM Invoices WHERE payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.')) as open_invoices,
                (SELECT COUNT(*) FROM BankTransactions WHERE reconciliation_status IN ('Da Riconciliare', 'Riconciliato Parz.')) as unreconciled_transactions,
                (SELECT COUNT(*) FROM ReconciliationLinks WHERE DATE(reconciliation_date) = DATE('now')) as today_reconciliations,
                (SELECT COALESCE(SUM(reconciled_amount), 0) FROM ReconciliationLinks WHERE DATE(reconciliation_date) = DATE('now')) as today_amount
        """)
        
        dashboard_data = {
            'summary': status_result['data']['summary'],
            'quick_stats': quick_stats[0] if quick_stats else {},
            'top_opportunities': opportunities[:5],
            'recent_reconciliations': recent_links,
            'status_breakdown': {
                'invoices': status_result['data']['invoice_statistics'],
                'transactions': status_result['data']['transaction_statistics']
            }
        }
        
        return APIResponse(
            success=True,
            message="Reconciliation dashboard data retrieved",
            data=dashboard_data
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting reconciliation dashboard: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving dashboard data")
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/api/reconciliation.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/api/sync.py ---
"""
Cloud Sync API endpoints for Google Drive integration
"""

import logging
from typing import Optional
from fastapi import APIRouter, HTTPException, Query
from fastapi.responses import JSONResponse

from app.models import SyncStatus, SyncResult, APIResponse
from app.config import settings

logger = logging.getLogger(__name__)
router = APIRouter()


@router.get("/status", response_model=SyncStatus)
async def get_sync_status():
    """Get current cloud sync status"""
    try:
        from app.core.cloud_sync import get_sync_manager
        
        sync_manager = get_sync_manager()
        status = sync_manager.get_sync_status()
        
        return SyncStatus(
            enabled=status['enabled'],
            service_available=status['service_available'],
            remote_file_id=status.get('remote_file_id'),
            last_sync_time=status.get('last_sync_time'),
            auto_sync_running=status['auto_sync_running']
        )
        
    except Exception as e:
        logger.error(f"Error getting sync status: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving sync status")


@router.post("/enable", response_model=APIResponse)
async def enable_sync():
    """Enable cloud synchronization"""
    try:
        from app.core.cloud_sync import get_sync_manager
        
        sync_manager = get_sync_manager()
        
        # Check if Google Drive service is available
        if not sync_manager.service:
            raise HTTPException(
                status_code=400, 
                detail="Google Drive service not available. Please check credentials configuration."
            )
        
        # Enable sync in configuration
        sync_manager.sync_enabled = True
        sync_manager._save_config()
        
        # Start auto-sync if enabled
        sync_manager.start_auto_sync()
        
        return APIResponse(
            success=True,
            message="Cloud synchronization enabled successfully"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error enabling sync: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error enabling synchronization")


@router.post("/disable", response_model=APIResponse)
async def disable_sync():
    """Disable cloud synchronization"""
    try:
        from app.core.cloud_sync import get_sync_manager
        
        sync_manager = get_sync_manager()
        
        # Stop auto-sync
        sync_manager.stop_auto_sync()
        
        # Disable sync in configuration
        sync_manager.sync_enabled = False
        sync_manager._save_config()
        
        return APIResponse(
            success=True,
            message="Cloud synchronization disabled successfully"
        )
        
    except Exception as e:
        logger.error(f"Error disabling sync: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error disabling synchronization")


@router.post("/manual", response_model=SyncResult)
async def manual_sync(
    force_direction: Optional[str] = Query(None, description="Force sync direction: upload, download")
):
    """Perform manual synchronization"""
    try:
        from app.core.cloud_sync import get_sync_manager
        
        sync_manager = get_sync_manager()
        
        if not sync_manager.service:
            raise HTTPException(
                status_code=400,
                detail="Google Drive service not available. Please check credentials configuration."
            )
        
        # Validate force_direction parameter
        if force_direction and force_direction not in ['upload', 'download']:
            raise HTTPException(
                status_code=400,
                detail="force_direction must be 'upload' or 'download'"
            )
        
        # Perform synchronization
        result = sync_manager.sync_database(force_direction=force_direction)
        
        return SyncResult(
            success=result['success'],
            action=result.get('action'),
            message=result['message'],
            timestamp=result['timestamp']
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error performing manual sync: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error performing synchronization")


@router.post("/upload", response_model=SyncResult)
async def force_upload():
    """Force upload local database to cloud"""
    try:
        from app.core.cloud_sync import get_sync_manager
        
        sync_manager = get_sync_manager()
        
        if not sync_manager.service:
            raise HTTPException(
                status_code=400,
                detail="Google Drive service not available"
            )
        
        result = sync_manager.sync_database(force_direction='upload')
        
        return SyncResult(
            success=result['success'],
            action=result.get('action'),
            message=result['message'],
            timestamp=result['timestamp']
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error forcing upload: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error uploading to cloud")


@router.post("/download", response_model=SyncResult)
async def force_download():
    """Force download database from cloud"""
    try:
        from app.core.cloud_sync import get_sync_manager
        
        sync_manager = get_sync_manager()
        
        if not sync_manager.service:
            raise HTTPException(
                status_code=400,
                detail="Google Drive service not available"
            )
        
        result = sync_manager.sync_database(force_direction='download')
        
        return SyncResult(
            success=result['success'],
            action=result.get('action'),
            message=result['message'],
            timestamp=result['timestamp']
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error forcing download: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error downloading from cloud")


@router.get("/history")
async def get_sync_history(
    limit: int = Query(20, ge=1, le=100, description="Number of sync events to return")
):
    """Get synchronization history"""
    try:
        # In a real implementation, you would store sync events in a database table
        # For now, we'll return mock data
        
        sync_history = [
            {
                "id": 1,
                "timestamp": "2024-01-15T10:30:00Z",
                "action": "upload",
                "success": True,
                "message": "Database uploaded successfully",
                "file_size": "2.5 MB",
                "duration_ms": 3500
            },
            {
                "id": 2,
                "timestamp": "2024-01-15T08:15:00Z",
                "action": "download",
                "success": True,
                "message": "Database downloaded successfully",
                "file_size": "2.4 MB",
                "duration_ms": 2800
            },
            {
                "id": 3,
                "timestamp": "2024-01-14T16:45:00Z",
                "action": "auto",
                "success": False,
                "message": "Network error during sync",
                "file_size": None,
                "duration_ms": 15000
            }
        ]
        
        return APIResponse(
            success=True,
            message=f"Retrieved {len(sync_history)} sync events",
            data={
                "history": sync_history[:limit],
                "total": len(sync_history)
            }
        )
        
    except Exception as e:
        logger.error(f"Error getting sync history: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving sync history")


@router.post("/auto-sync/start", response_model=APIResponse)
async def start_auto_sync():
    """Start automatic synchronization"""
    try:
        from app.core.cloud_sync import get_sync_manager
        
        sync_manager = get_sync_manager()
        
        if not sync_manager.sync_enabled:
            raise HTTPException(
                status_code=400,
                detail="Sync must be enabled before starting auto-sync"
            )
        
        if not sync_manager.service:
            raise HTTPException(
                status_code=400,
                detail="Google Drive service not available"
            )
        
        sync_manager.start_auto_sync()
        
        return APIResponse(
            success=True,
            message=f"Auto-sync started with {sync_manager.auto_sync_interval} second interval"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error starting auto-sync: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error starting auto-sync")


@router.post("/auto-sync/stop", response_model=APIResponse)
async def stop_auto_sync():
    """Stop automatic synchronization"""
    try:
        from app.core.cloud_sync import get_sync_manager
        
        sync_manager = get_sync_manager()
        sync_manager.stop_auto_sync()
        
        return APIResponse(
            success=True,
            message="Auto-sync stopped successfully"
        )
        
    except Exception as e:
        logger.error(f"Error stopping auto-sync: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error stopping auto-sync")


@router.put("/auto-sync/interval", response_model=APIResponse)
async def update_auto_sync_interval(
    interval_seconds: int = Query(..., ge=60, le=3600, description="Auto-sync interval in seconds (60-3600)")
):
    """Update auto-sync interval"""
    try:
        from app.core.cloud_sync import get_sync_manager
        
        sync_manager = get_sync_manager()
        
        # Update interval
        sync_manager.auto_sync_interval = interval_seconds
        sync_manager._save_config()
        
        # Restart auto-sync if it was running
        if sync_manager.sync_thread and sync_manager.sync_thread.is_alive():
            sync_manager.stop_auto_sync()
            sync_manager.start_auto_sync()
        
        return APIResponse(
            success=True,
            message=f"Auto-sync interval updated to {interval_seconds} seconds"
        )
        
    except Exception as e:
        logger.error(f"Error updating auto-sync interval: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error updating auto-sync interval")


@router.get("/remote-info")
async def get_remote_file_info():
    """Get information about the remote file on Google Drive"""
    try:
        from app.core.cloud_sync import get_sync_manager
        
        sync_manager = get_sync_manager()
        
        if not sync_manager.service:
            raise HTTPException(
                status_code=400,
                detail="Google Drive service not available"
            )
        
        if not sync_manager.remote_file_id:
            return APIResponse(
                success=True,
                message="No remote file found",
                data={"exists": False}
            )
        
        try:
            # Get file metadata from Google Drive
            file_metadata = sync_manager.service.files().get(
                fileId=sync_manager.remote_file_id,
                fields='id,name,size,modifiedTime,createdTime,md5Checksum'
            ).execute()
            
            remote_modified_time = sync_manager._get_remote_modified_time()
            local_modified_time = sync_manager._get_local_modified_time()
            
            return APIResponse(
                success=True,
                message="Remote file information retrieved",
                data={
                    "exists": True,
                    "file_id": file_metadata['id'],
                    "name": file_metadata['name'],
                    "size": int(file_metadata.get('size', 0)),
                    "size_mb": round(int(file_metadata.get('size', 0)) / (1024*1024), 2),
                    "created_time": file_metadata.get('createdTime'),
                    "modified_time": file_metadata.get('modifiedTime'),
                    "md5_checksum": file_metadata.get('md5Checksum'),
                    "local_modified_time": local_modified_time.isoformat() if local_modified_time else None,
                    "remote_modified_time": remote_modified_time.isoformat() if remote_modified_time else None,
                    "sync_needed": (local_modified_time and remote_modified_time and 
                                  local_modified_time != remote_modified_time)
                }
            )
            
        except Exception as file_error:
            logger.warning(f"Error getting remote file info: {file_error}")
            return APIResponse(
                success=True,
                message="Remote file not accessible or doesn't exist",
                data={"exists": False, "error": str(file_error)}
            )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting remote file info: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving remote file information")


@router.delete("/remote-file", response_model=APIResponse)
async def delete_remote_file():
    """Delete the remote database file from Google Drive"""
    try:
        from app.core.cloud_sync import get_sync_manager
        
        sync_manager = get_sync_manager()
        
        if not sync_manager.service:
            raise HTTPException(
                status_code=400,
                detail="Google Drive service not available"
            )
        
        if not sync_manager.remote_file_id:
            return APIResponse(
                success=True,
                message="No remote file to delete"
            )
        
        try:
            # Delete file from Google Drive
            sync_manager.service.files().delete(fileId=sync_manager.remote_file_id).execute()
            
            # Clear remote file ID from config
            sync_manager.remote_file_id = None
            sync_manager._save_config()
            
            return APIResponse(
                success=True,
                message="Remote file deleted successfully"
            )
            
        except Exception as delete_error:
            logger.error(f"Error deleting remote file: {delete_error}")
            raise HTTPException(
                status_code=500,
                detail=f"Error deleting remote file: {str(delete_error)}"
            )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting remote file: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error deleting remote file")


@router.post("/test-connection", response_model=APIResponse)
async def test_google_drive_connection():
    """Test Google Drive API connection and permissions"""
    try:
        from app.core.cloud_sync import get_sync_manager
        
        sync_manager = get_sync_manager()
        
        if not sync_manager.service:
            return APIResponse(
                success=False,
                message="Google Drive service not available. Check credentials configuration."
            )
        
        try:
            # Test basic API access
            about = sync_manager.service.about().get(fields='user,storageQuota').execute()
            
            # Test file listing permission
            files_list = sync_manager.service.files().list(
                pageSize=1,
                fields='files(id,name)'
            ).execute()
            
            user_info = about.get('user', {})
            storage_info = about.get('storageQuota', {})
            
            return APIResponse(
                success=True,
                message="Google Drive connection successful",
                data={
                    "user": {
                        "email": user_info.get('emailAddress'),
                        "display_name": user_info.get('displayName')
                    },
                    "storage": {
                        "limit": storage_info.get('limit'),
                        "usage": storage_info.get('usage'),
                        "usage_in_drive": storage_info.get('usageInDrive')
                    },
                    "permissions": {
                        "can_list_files": len(files_list.get('files', [])) >= 0,
                        "can_access_about": True
                    }
                }
            )
            
        except Exception as api_error:
            logger.error(f"Google Drive API test failed: {api_error}")
            return APIResponse(
                success=False,
                message=f"Google Drive API test failed: {str(api_error)}"
            )
        
    except Exception as e:
        logger.error(f"Error testing Google Drive connection: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error testing connection")


@router.get("/config")
async def get_sync_configuration():
    """Get current sync configuration"""
    try:
        from app.core.cloud_sync import get_sync_manager
        
        sync_manager = get_sync_manager()
        
        config_data = {
            "sync_enabled": sync_manager.sync_enabled,
            "auto_sync_interval": sync_manager.auto_sync_interval,
            "remote_file_name": sync_manager.remote_db_name,
            "credentials_file_exists": os.path.exists(sync_manager.credentials_file),
            "token_file_exists": os.path.exists(sync_manager.token_file),
            "service_available": sync_manager.service is not None,
            "remote_file_id": sync_manager.remote_file_id
        }
        
        return APIResponse(
            success=True,
            message="Sync configuration retrieved",
            data=config_data
        )
        
    except Exception as e:
        logger.error(f"Error getting sync configuration: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving sync configuration")


@router.post("/setup-credentials", response_model=APIResponse)
async def setup_credentials_guide():
    """Get instructions for setting up Google Drive credentials"""
    try:
        instructions = {
            "steps": [
                {
                    "step": 1,
                    "title": "Create Google Cloud Project",
                    "description": "Go to Google Cloud Console and create a new project or select existing one"
                },
                {
                    "step": 2,
                    "title": "Enable Google Drive API",
                    "description": "In the API Library, search for and enable the Google Drive API"
                },
                {
                    "step": 3,
                    "title": "Create Credentials",
                    "description": "Go to Credentials > Create Credentials > OAuth 2.0 Client ID"
                },
                {
                    "step": 4,
                    "title": "Configure OAuth Consent",
                    "description": "Set up OAuth consent screen with your application information"
                },
                {
                    "step": 5,
                    "title": "Download Credentials",
                    "description": "Download the credentials JSON file and save as 'google_credentials.json'"
                },
                {
                    "step": 6,
                    "title": "Authorize Application",
                    "description": "Run the sync setup to authorize the application and generate token"
                }
            ],
            "required_files": [
                {
                    "filename": "google_credentials.json",
                    "description": "OAuth 2.0 client credentials from Google Cloud Console",
                    "location": "Same directory as config.ini"
                },
                {
                    "filename": "google_token.json",
                    "description": "Generated automatically after first authorization",
                    "location": "Same directory as config.ini"
                }
            ],
            "scopes_needed": [
                "https://www.googleapis.com/auth/drive.file"
            ]
        }
        
        return APIResponse(
            success=True,
            message="Credentials setup instructions",
            data=instructions
        )
        
    except Exception as e:
        logger.error(f"Error getting setup instructions: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving setup instructions")


@router.post("/reset-authorization", response_model=APIResponse)
async def reset_authorization():
    """Reset Google Drive authorization (delete token file)"""
    try:
        from app.core.cloud_sync import get_sync_manager
        
        sync_manager = get_sync_manager()
        
        # Stop auto-sync first
        sync_manager.stop_auto_sync()
        
        # Delete token file if it exists
        if os.path.exists(sync_manager.token_file):
            os.remove(sync_manager.token_file)
            logger.info("Token file deleted")
        
        # Reset service
        sync_manager.service = None
        
        return APIResponse(
            success=True,
            message="Authorization reset. Please re-authorize the application to use sync features."
        )
        
    except Exception as e:
        logger.error(f"Error resetting authorization: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error resetting authorization")


import os
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/api/sync.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/api/transactions.py ---
"""
Transactions API endpoints - Aggiornato per usare adapter pattern
"""

import logging
from typing import List, Optional
from datetime import date, datetime
from fastapi import APIRouter, HTTPException, Query, Path, Body
from fastapi.responses import JSONResponse

# Usa adapters invece di accesso diretto al core
from app.adapters.database_adapter import db_adapter
from app.adapters.reconciliation_adapter import reconciliation_adapter
from app.models import (
    BankTransaction, BankTransactionCreate, BankTransactionUpdate,
    TransactionFilter, PaginationParams, TransactionListResponse,
    APIResponse, ReconciliationStatus
)

logger = logging.getLogger(__name__)
router = APIRouter()


@router.get("/", response_model=TransactionListResponse)
async def get_transactions_list(
    status_filter: Optional[ReconciliationStatus] = Query(None, description="Filter by reconciliation status"),
    search: Optional[str] = Query(None, description="Search in description"),
    start_date: Optional[date] = Query(None, description="Filter by start date"),
    end_date: Optional[date] = Query(None, description="Filter by end date"),
    min_amount: Optional[float] = Query(None, description="Minimum amount filter"),
    max_amount: Optional[float] = Query(None, description="Maximum amount filter"),
    anagraphics_id_heuristic: Optional[int] = Query(None, description="Filter by likely anagraphics ID"),
    hide_pos: bool = Query(False, description="Hide POS transactions"),
    hide_worldline: bool = Query(False, description="Hide Worldline transactions"),
    hide_cash: bool = Query(False, description="Hide cash transactions"),
    hide_commissions: bool = Query(False, description="Hide bank commissions"),
    page: int = Query(1, ge=1, description="Page number"),
    size: int = Query(50, ge=1, le=1000, description="Page size")
):
    """Get paginated list of bank transactions with advanced filters"""
    try:
        pagination = PaginationParams(page=page, size=size)
        
        # Usa adapter per ottenere transazioni dal core
        df_transactions = await db_adapter.get_transactions_async(
            start_date=start_date.isoformat() if start_date else None,
            end_date=end_date.isoformat() if end_date else None,
            status_filter=status_filter.value if status_filter else None,
            limit=None,  # Applicheremo filtri e paginazione qui
            anagraphics_id_heuristic_filter=anagraphics_id_heuristic,
            hide_pos=hide_pos,
            hide_worldline=hide_worldline,
            hide_cash=hide_cash,
            hide_commissions=hide_commissions
        )
        
        if df_transactions.empty:
            return TransactionListResponse(
                items=[],
                total=0,
                page=page,
                size=size,
                pages=0
            )
        
        # Applica filtri aggiuntivi
        if search:
            search_mask = df_transactions['description'].str.contains(
                search, case=False, na=False
            )
            df_transactions = df_transactions[search_mask]
        
        if min_amount is not None:
            df_transactions = df_transactions[
                df_transactions['amount'].abs() >= min_amount
            ]
        
        if max_amount is not None:
            df_transactions = df_transactions[
                df_transactions['amount'].abs() <= max_amount
            ]
        
        total = len(df_transactions)
        
        # Paginazione
        start_idx = (page - 1) * size
        end_idx = start_idx + size
        df_paginated = df_transactions.iloc[start_idx:end_idx]
        
        # Converti in formato API con campi aggiuntivi
        items = []
        for _, row in df_paginated.iterrows():
            item = row.to_dict()
            # Aggiungi campi computati
            item['remaining_amount'] = item['amount'] - item.get('reconciled_amount', 0)
            item['is_income'] = item['amount'] > 0
            item['is_expense'] = item['amount'] < 0
            items.append(item)
        
        pages = (total + size - 1) // size
        
        return TransactionListResponse(
            items=items,
            total=total,
            page=page,
            size=size,
            pages=pages
        )
        
    except Exception as e:
        logger.error(f"Error getting transactions list: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error retrieving transactions: {str(e)}")


@router.get("/{transaction_id}", response_model=BankTransaction)
async def get_transaction_by_id(
    transaction_id: int = Path(..., description="Transaction ID")
):
    """Get transaction by ID with full details"""
    try:
        # Usa adapter per ottenere dettagli
        transaction = await db_adapter.get_item_details_async('transaction', transaction_id)
        
        if not transaction:
            raise HTTPException(status_code=404, detail="Transaction not found")
        
        # Aggiungi campi computati
        transaction['remaining_amount'] = transaction['amount'] - transaction.get('reconciled_amount', 0)
        transaction['is_income'] = transaction['amount'] > 0
        transaction['is_expense'] = transaction['amount'] < 0
        
        return transaction
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting transaction {transaction_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving transaction")


@router.post("/", response_model=BankTransaction)
async def create_transaction(transaction_data: BankTransactionCreate):
    """Create new bank transaction"""
    try:
        # Verifica duplicati usando hash unico
        existing = await db_adapter.check_duplicate_async(
            'BankTransactions', 'unique_hash', transaction_data.unique_hash
        )
        
        if existing:
            raise HTTPException(
                status_code=409, 
                detail=f"Transaction with hash {transaction_data.unique_hash} already exists"
            )
        
        # Crea transazione usando adapter
        insert_query = """
            INSERT INTO BankTransactions 
            (transaction_date, value_date, amount, description, causale_abi, 
             unique_hash, reconciled_amount, reconciliation_status, created_at)
            VALUES (?, ?, ?, ?, ?, ?, 0.0, 'Da Riconciliare', datetime('now'))
        """
        
        params = (
            transaction_data.transaction_date.isoformat(),
            transaction_data.value_date.isoformat() if transaction_data.value_date else None,
            transaction_data.amount,
            transaction_data.description,
            transaction_data.causale_abi,
            transaction_data.unique_hash
        )
        
        new_id = await db_adapter.execute_write_async(insert_query, params)
        
        if not new_id:
            raise HTTPException(status_code=500, detail="Failed to create transaction")
        
        # Restituisci la transazione creata
        return await get_transaction_by_id(new_id)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating transaction: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error creating transaction")


@router.put("/{transaction_id}", response_model=BankTransaction)
async def update_transaction(
    transaction_id: int = Path(..., description="Transaction ID"),
    transaction_data: BankTransactionUpdate = ...
):
    """Update transaction"""
    try:
        # Verifica esistenza
        existing = await db_adapter.execute_query_async(
            "SELECT id FROM BankTransactions WHERE id = ?", (transaction_id,)
        )
        if not existing:
            raise HTTPException(status_code=404, detail="Transaction not found")
        
        # Build update query dinamica
        update_fields = []
        params = []
        
        for field, value in transaction_data.model_dump(exclude_unset=True).items():
            if value is not None:
                if field in ['transaction_date', 'value_date'] and hasattr(value, 'isoformat'):
                    value = value.isoformat()
                elif hasattr(value, 'value'):  # Enum
                    value = value.value
                update_fields.append(f"{field} = ?")
                params.append(value)
        
        if not update_fields:
            raise HTTPException(status_code=400, detail="No fields to update")
        
        # Aggiungi updated_at
        update_fields.append("updated_at = datetime('now')")
        params.append(transaction_id)
        
        update_query = f"UPDATE BankTransactions SET {', '.join(update_fields)} WHERE id = ?"
        
        rows_affected = await db_adapter.execute_write_async(update_query, tuple(params))
        
        if rows_affected == 0:
            raise HTTPException(status_code=404, detail="Transaction not found")
        
        # Restituisci transazione aggiornata
        return await get_transaction_by_id(transaction_id)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating transaction {transaction_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error updating transaction")


@router.delete("/{transaction_id}", response_model=APIResponse)
async def delete_transaction(
    transaction_id: int = Path(..., description="Transaction ID")
):
    """Delete transaction and related reconciliation links"""
    try:
        # Verifica se ha link di riconciliazione
        links_query = "SELECT COUNT(*) as count FROM ReconciliationLinks WHERE transaction_id = ?"
        links_result = await db_adapter.execute_query_async(links_query, (transaction_id,))
        
        if links_result and links_result[0]['count'] > 0:
            # Prima rimuovi i link
            await reconciliation_adapter.undo_reconciliation_async(transaction_id=transaction_id)
        
        # Elimina transazione
        delete_query = "DELETE FROM BankTransactions WHERE id = ?"
        rows_affected = await db_adapter.execute_write_async(delete_query, (transaction_id,))
        
        if rows_affected == 0:
            raise HTTPException(status_code=404, detail="Transaction not found")
        
        return APIResponse(
            success=True,
            message="Transaction deleted successfully"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting transaction {transaction_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error deleting transaction")


@router.get("/{transaction_id}/reconciliation-links")
async def get_transaction_reconciliation_links(
    transaction_id: int = Path(..., description="Transaction ID")
):
    """Get reconciliation links for transaction"""
    try:
        query = """
            SELECT rl.id, rl.invoice_id, rl.reconciled_amount, rl.reconciliation_date,
                   i.doc_number, i.doc_date, i.total_amount as invoice_amount, 
                   a.denomination as counterparty
            FROM ReconciliationLinks rl
            JOIN Invoices i ON rl.invoice_id = i.id
            JOIN Anagraphics a ON i.anagraphics_id = a.id
            WHERE rl.transaction_id = ?
            ORDER BY rl.reconciliation_date DESC
        """
        
        links = await db_adapter.execute_query_async(query, (transaction_id,))
        
        return APIResponse(
            success=True,
            message=f"Found {len(links)} reconciliation links",
            data=links
        )
        
    except Exception as e:
        logger.error(f"Error getting reconciliation links for transaction {transaction_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving reconciliation links")


@router.get("/{transaction_id}/potential-matches")
async def get_transaction_potential_matches(
    transaction_id: int = Path(..., description="Transaction ID"),
    limit: int = Query(10, ge=1, le=50, description="Maximum number of matches")
):
    """Get potential invoice matches for transaction"""
    try:
        # Ottieni dettagli transazione
        transaction = await db_adapter.get_item_details_async('transaction', transaction_id)
        if not transaction:
            raise HTTPException(status_code=404, detail="Transaction not found")
        
        # Se non è un incasso, non cercare match
        if transaction['amount'] <= 0:
            return APIResponse(
                success=True,
                message="No matches for expense transactions",
                data=[]
            )
        
        # Usa reconciliation adapter per trovare opportunità
        opportunities = await reconciliation_adapter.get_reconciliation_opportunities_async(
            limit=limit * 2  # Ottieni più opportunità per filtrare
        )
        
        # Filtra solo quelle per questa transazione
        transaction_opportunities = [
            opp for opp in opportunities 
            if opp['transaction_id'] == transaction_id
        ]
        
        return APIResponse(
            success=True,
            message=f"Found {len(transaction_opportunities)} potential matches",
            data=transaction_opportunities[:limit]
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error finding potential matches for transaction {transaction_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error finding potential matches")


@router.post("/{transaction_id}/update-status", response_model=APIResponse)
async def update_transaction_reconciliation_status(
    transaction_id: int = Path(..., description="Transaction ID"),
    reconciliation_status: ReconciliationStatus = Body(..., description="New reconciliation status"),
    reconciled_amount: Optional[float] = Body(None, description="Amount reconciled")
):
    """Update transaction reconciliation status and amount"""
    try:
        # Ottieni dati transazione correnti
        current_query = "SELECT amount, reconciled_amount FROM BankTransactions WHERE id = ?"
        current_result = await db_adapter.execute_query_async(current_query, (transaction_id,))
        
        if not current_result:
            raise HTTPException(status_code=404, detail="Transaction not found")
        
        current_data = current_result[0]
        
        # Se reconciled_amount non fornito, mantieni quello attuale
        if reconciled_amount is None:
            reconciled_amount = current_data['reconciled_amount']
        
        # Validazioni
        if reconciled_amount < 0:
            raise HTTPException(status_code=400, detail="Reconciled amount cannot be negative")
        
        if reconciled_amount > abs(current_data['amount']):
            raise HTTPException(
                status_code=400, 
                detail="Reconciled amount cannot exceed transaction amount"
            )
        
        # Aggiorna usando adapter
        success = await db_adapter.update_transaction_state_async(
            transaction_id, 
            reconciliation_status.value, 
            reconciled_amount
        )
        
        if not success:
            raise HTTPException(status_code=500, detail="Failed to update transaction status")
        
        return APIResponse(
            success=True,
            message=f"Transaction status updated to {reconciliation_status.value}",
            data={
                "transaction_id": transaction_id,
                "new_status": reconciliation_status.value,
                "reconciled_amount": reconciled_amount,
                "remaining_amount": abs(current_data['amount']) - reconciled_amount
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating transaction status {transaction_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error updating transaction status")


@router.get("/stats/summary")
async def get_transactions_stats():
    """Get transaction statistics summary"""
    try:
        # Statistiche per stato
        status_stats_query = """
            SELECT 
                reconciliation_status,
                COUNT(*) as count,
                SUM(ABS(amount)) as total_amount,
                SUM(ABS(amount) - reconciled_amount) as remaining_amount,
                AVG(ABS(amount)) as avg_amount
            FROM BankTransactions
            GROUP BY reconciliation_status
        """
        
        status_stats = await db_adapter.execute_query_async(status_stats_query)
        
        # Statistiche per tipo (entrate/uscite)
        type_stats_query = """
            SELECT 
                CASE WHEN amount > 0 THEN 'Income' ELSE 'Expense' END as type,
                COUNT(*) as count,
                SUM(ABS(amount)) as total_amount,
                AVG(ABS(amount)) as avg_amount
            FROM BankTransactions
            GROUP BY CASE WHEN amount > 0 THEN 'Income' ELSE 'Expense' END
        """
        
        type_stats = await db_adapter.execute_query_async(type_stats_query)
        
        # Trend mensili
        monthly_trends_query = """
            SELECT 
                strftime('%Y-%m', transaction_date) as month,
                COUNT(*) as transaction_count,
                SUM(CASE WHEN amount > 0 THEN amount ELSE 0 END) as total_income,
                SUM(CASE WHEN amount < 0 THEN ABS(amount) ELSE 0 END) as total_expenses,
                SUM(amount) as net_flow
            FROM BankTransactions
            WHERE transaction_date >= date('now', '-12 months')
            GROUP BY strftime('%Y-%m', transaction_date)
            ORDER BY month
        """
        
        monthly_trends = await db_adapter.execute_query_async(monthly_trends_query)
        
        # Transazioni recenti
        recent_query = """
            SELECT id, transaction_date, amount, description, reconciliation_status
            FROM BankTransactions
            ORDER BY transaction_date DESC, id DESC
            LIMIT 10
        """
        
        recent = await db_adapter.execute_query_async(recent_query)
        
        # Statistiche causali ABI più frequenti
        causali_stats_query = """
            SELECT 
                causale_abi,
                COUNT(*) as count,
                SUM(ABS(amount)) as total_amount
            FROM BankTransactions
            WHERE causale_abi IS NOT NULL
            GROUP BY causale_abi
            ORDER BY count DESC
            LIMIT 10
        """
        
        causali_stats = await db_adapter.execute_query_async(causali_stats_query)
        
        return APIResponse(
            success=True,
            message="Transaction statistics retrieved",
            data={
                "status_statistics": status_stats,
                "type_statistics": type_stats,
                "monthly_trends": monthly_trends,
                "recent_transactions": recent,
                "top_causali_abi": causali_stats
            }
        )
        
    except Exception as e:
        logger.error(f"Error getting transaction stats: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving statistics")


@router.get("/search/{query}")
async def search_transactions(
    query: str = Path(..., description="Search query"),
    limit: int = Query(10, ge=1, le=100),
    include_reconciled: bool = Query(False, description="Include fully reconciled transactions")
):
    """Search transactions by description or amount"""
    try:
        # Build search query
        search_conditions = ["description LIKE ?"]
        search_params = [f"%{query}%"]
        
        # Try to parse as number for amount search
        try:
            amount_value = float(query.replace(',', '.'))
            search_conditions.append("ABS(amount) = ?")
            search_params.append(abs(amount_value))
        except ValueError:
            pass
        
        # Build final query
        base_query = """
            SELECT id, transaction_date, amount, description, reconciliation_status,
                   (amount - reconciled_amount) as remaining_amount
            FROM BankTransactions
            WHERE ({})
        """.format(" OR ".join(search_conditions))
        
        if not include_reconciled:
            base_query += " AND reconciliation_status != 'Riconciliato Tot.'"
        
        base_query += " ORDER BY transaction_date DESC LIMIT ?"
        search_params.append(limit)
        
        results = await db_adapter.execute_query_async(base_query, tuple(search_params))
        
        return APIResponse(
            success=True,
            message=f"Found {len(results)} results",
            data={
                "query": query,
                "results": results,
                "total": len(results)
            }
        )
        
    except Exception as e:
        logger.error(f"Error searching transactions: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error searching transactions")


@router.get("/analysis/cash-flow")
async def get_cash_flow_analysis(
    months: int = Query(12, ge=1, le=60, description="Number of months to analyze"),
    group_by: str = Query("month", description="Group by: month, week, day")
):
    """Get cash flow analysis for transactions"""
    try:
        # Usa analytics adapter per analisi cash flow
        from app.adapters.analytics_adapter import analytics_adapter
        
        cash_flow_df = await analytics_adapter.get_monthly_cash_flow_analysis_async(months)
        
        if cash_flow_df.empty:
            return APIResponse(
                success=True,
                message="No cash flow data available",
                data=[]
            )
        
        return APIResponse(
            success=True,
            message=f"Cash flow analysis for {months} months",
            data=cash_flow_df.to_dict('records')
        )
        
    except Exception as e:
        logger.error(f"Error getting cash flow analysis: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error retrieving cash flow analysis")


@router.post("/batch/update-status")
async def batch_update_transaction_status(
    transaction_ids: List[int] = Body(..., description="List of transaction IDs"),
    reconciliation_status: ReconciliationStatus = Body(..., description="New status for all transactions")
):
    """Update reconciliation status for multiple transactions"""
    try:
        if len(transaction_ids) > 100:
            raise HTTPException(status_code=400, detail="Maximum 100 transactions per batch")
        
        results = {
            'total': len(transaction_ids),
            'successful': 0,
            'failed': 0,
            'details': []
        }
        
        for transaction_id in transaction_ids:
            try:
                # Ottieni transazione corrente
                current = await db_adapter.execute_query_async(
                    "SELECT amount, reconciled_amount FROM BankTransactions WHERE id = ?",
                    (transaction_id,)
                )
                
                if not current:
                    results['failed'] += 1
                    results['details'].append({
                        'transaction_id': transaction_id,
                        'success': False,
                        'message': 'Transaction not found'
                    })
                    continue
                
                # Aggiorna stato
                success = await db_adapter.update_transaction_state_async(
                    transaction_id,
                    reconciliation_status.value,
                    current[0]['reconciled_amount']  # Mantieni importo riconciliato attuale
                )
                
                if success:
                    results['successful'] += 1
                    results['details'].append({
                        'transaction_id': transaction_id,
                        'success': True,
                        'message': f'Status updated to {reconciliation_status.value}'
                    })
                else:
                    results['failed'] += 1
                    results['details'].append({
                        'transaction_id': transaction_id,
                        'success': False,
                        'message': 'Update failed'
                    })
                    
            except Exception as e:
                results['failed'] += 1
                results['details'].append({
                    'transaction_id': transaction_id,
                    'success': False,
                    'message': f'Error: {str(e)}'
                })
        
        return APIResponse(
            success=True,
            message=f"Batch update completed: {results['successful']} successful, {results['failed']} failed",
            data=results
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in batch update: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error in batch update")


@router.get("/export/reconciliation-ready")
async def export_reconciliation_ready_transactions(
    format: str = Query("json", description="Export format: json, csv"),
    limit: int = Query(1000, ge=1, le=5000, description="Maximum transactions to export")
):
    """Export transactions ready for reconciliation"""
    try:
        # Ottieni transazioni pronte per riconciliazione
        query = """
            SELECT 
                bt.id,
                bt.transaction_date,
                bt.amount,
                bt.description,
                bt.reconciliation_status,
                (bt.amount - bt.reconciled_amount) as remaining_amount
            FROM BankTransactions bt
            WHERE bt.reconciliation_status IN ('Da Riconciliare', 'Riconciliato Parz.')
              AND bt.amount > 0  -- Solo entrate
              AND (bt.amount - bt.reconciled_amount) > 0.01
            ORDER BY bt.transaction_date DESC
            LIMIT ?
        """
        
        transactions = await db_adapter.execute_query_async(query, (limit,))
        
        if format == "csv":
            import pandas as pd
            from io import StringIO
            
            df = pd.DataFrame(transactions)
            csv_buffer = StringIO()
            df.to_csv(csv_buffer, index=False, sep=';')
            csv_content = csv_buffer.getvalue()
            
            from fastapi.responses import Response
            return Response(
                content=csv_content,
                media_type="text/csv",
                headers={"Content-Disposition": "attachment; filename=transactions_reconciliation_ready.csv"}
            )
        else:
            return APIResponse(
                success=True,
                message=f"Exported {len(transactions)} transactions ready for reconciliation",
                data={
                    'transactions': transactions,
                    'count': len(transactions),
                    'export_date': datetime.now().isoformat()
                }
            )
        
    except Exception as e:
        logger.error(f"Error exporting reconciliation-ready transactions: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error exporting transactions")

--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/api/transactions.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/config.py ---
"""
Configuration management per FastAPI che usa il core esistente
"""

import os
import configparser
from pathlib import Path
from typing import Optional

class Settings:
    """Configurazioni FastAPI che integrano con il core esistente"""
    
    def __init__(self):
        self.load_config()
    
    def load_config(self):
        """Carica configurazione dal core esistente"""
        
        # Settings FastAPI
        self.DEBUG = os.getenv("DEBUG", "false").lower() == "true"
        self.HOST = os.getenv("HOST", "127.0.0.1")
        self.PORT = int(os.getenv("PORT", "8000"))
        
        # Usa la configurazione del core esistente
        self._load_from_core_config()
        
        # API settings
        self.MAX_UPLOAD_SIZE = int(os.getenv("MAX_UPLOAD_SIZE", "100"))  # MB
        self.PAGINATION_SIZE = int(os.getenv("PAGINATION_SIZE", "50"))
    
    def _load_from_core_config(self):
        """Carica configurazione dal config.ini del core esistente"""
        
        # Trova il percorso del config.ini del core
        config_paths = [
            "config.ini",
            "../config.ini", 
            "../../config.ini",
            os.path.expanduser("~/.fattura_analyzer/config.ini")
        ]
        
        config = configparser.ConfigParser()
        config_found = False
        
        for config_path in config_paths:
            if os.path.exists(config_path):
                try:
                    config.read(config_path, encoding='utf-8')
                    print(f"📄 Loaded config from: {config_path}")
                    config_found = True
                    break
                except Exception as e:
                    print(f"⚠️ Error reading config from {config_path}: {e}")
                    continue
        
        if not config_found:
            print("⚠️ No config.ini found, using defaults")
        
        # Database settings (dal core)
        if config.has_section('Paths'):
            self.DATABASE_PATH = config.get('Paths', 'DatabaseFile', fallback='database.db')
        else:
            self.DATABASE_PATH = 'database.db'
        
        # Company settings (dal core)
        if config.has_section('Azienda'):
            self.COMPANY_NAME = config.get('Azienda', 'RagioneSociale', fallback='')
            self.COMPANY_VAT = config.get('Azienda', 'PartitaIVA', fallback='')
            self.COMPANY_CF = config.get('Azienda', 'CodiceFiscale', fallback='')
        else:
            self.COMPANY_NAME = ''
            self.COMPANY_VAT = ''
            self.COMPANY_CF = ''
        
        # Cloud sync settings (dal core)
        if config.has_section('CloudSync'):
            self.SYNC_ENABLED = config.getboolean('CloudSync', 'enabled', fallback=False)
            self.GOOGLE_CREDENTIALS_FILE = config.get('CloudSync', 'credentials_file', fallback='google_credentials.json')
        else:
            self.SYNC_ENABLED = False
            self.GOOGLE_CREDENTIALS_FILE = 'google_credentials.json'
    
    @property
    def company_data(self) -> dict:
        """Dati azienda per il processing delle fatture (compatibile con core)"""
        return {
            'name': self.COMPANY_NAME,
            'piva': self.COMPANY_VAT, 
            'cf': self.COMPANY_CF
        }
    
    def get_database_path(self) -> str:
        """Percorso database (usa la logica del core)"""
        if os.path.isabs(self.DATABASE_PATH):
            return self.DATABASE_PATH
        else:
            # Relativo al progetto root
            project_root = Path(__file__).parent.parent.parent
            return str(project_root / self.DATABASE_PATH)

# Istanza settings globale
settings = Settings()

# Configurazioni per diversi ambienti
class DevelopmentConfig(Settings):
    """Configurazione sviluppo"""
    def __init__(self):
        super().__init__()
        self.DEBUG = True
        self.HOST = "127.0.0.1"
        self.PORT = 8000

class ProductionConfig(Settings):
    """Configurazione produzione"""
    def __init__(self):
        super().__init__()
        self.DEBUG = False
        self.HOST = "127.0.0.1"
        self.PORT = 8000

class TestConfig(Settings):
    """Configurazione test"""
    def __init__(self):
        super().__init__()
        self.DEBUG = True
        self.DATABASE_PATH = ":memory:"  # In-memory per test

def get_config(env: str = None) -> Settings:
    """Ottiene configurazione basata su ambiente"""
    env = env or os.getenv("ENVIRONMENT", "development")
    
    if env == "production":
        return ProductionConfig()
    elif env == "test":
        return TestConfig()
    else:
        return DevelopmentConfig()

# Export
__all__ = ["settings", "get_config"]
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/config.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/core/__init__.py ---
# Path: backend/app/core/__init__.py

# Questo file rende la directory un package Python

--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/core/__init__.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/core/analysis.py ---
# core/analysis.py - Analisi ottimizzate per business ingrosso frutta e verdura

import logging
import pandas as pd
from decimal import Decimal, ROUND_HALF_UP
from datetime import date, timedelta, datetime
import sqlite3
from dateutil.relativedelta import relativedelta
import re
from typing import Dict, List, Optional, Tuple, Any
import numpy as np
from collections import defaultdict

try:
    from .database import get_connection, DB_PATH
    from .utils import to_decimal, quantize, AMOUNT_TOLERANCE, normalize_product_name
except ImportError:
    logging.warning("Import relativo fallito in analysis.py, tento import assoluto.")
    try:
        from database import get_connection, DB_PATH
        from utils import to_decimal, quantize, AMOUNT_TOLERANCE, normalize_product_name
    except ImportError as e:
        logging.critical(f"Impossibile importare dipendenze database/utils in analysis.py: {e}")
        raise ImportError(f"Impossibile importare dipendenze database/utils in analysis.py: {e}") from e

logger = logging.getLogger(__name__)

# ===== ANALISI CORE BUSINESS =====

def get_monthly_revenue_costs(start_date=None, end_date=None):
    """Analisi ricavi e costi mensili con margini di profitto"""
    conn = None
    try:
        conn = get_connection()
        end_date_obj = pd.to_datetime(end_date, errors='coerce').date() if end_date else date.today()
        start_date_obj = pd.to_datetime(start_date, errors='coerce').date() if start_date else (end_date_obj - relativedelta(years=1) + relativedelta(days=1))

        if pd.isna(start_date_obj) or pd.isna(end_date_obj):
            logger.error("Date di inizio o fine non valide per get_monthly_revenue_costs.")
            return pd.DataFrame(columns=['month', 'revenue', 'cost', 'gross_margin', 'margin_percent'])

        start_str, end_str = start_date_obj.isoformat(), end_date_obj.isoformat()
        query = """
            SELECT strftime('%Y-%m', doc_date) AS month, type, SUM(total_amount) AS monthly_total
            FROM Invoices
            WHERE doc_date BETWEEN ? AND ?
            GROUP BY month, type
            ORDER BY month, type;
        """
        df = pd.read_sql_query(query, conn, params=(start_str, end_str))

        if df.empty:
            logger.info("Nessuna fattura trovata nel periodo per revenue/costs.")
            return pd.DataFrame(columns=['month', 'revenue', 'cost', 'gross_margin', 'margin_percent'])

        df_pivot = df.pivot(index='month', columns='type', values='monthly_total').fillna(0.0)
        df_pivot.rename(columns={'Attiva': 'revenue', 'Passiva': 'cost'}, inplace=True)
        if 'revenue' not in df_pivot: df_pivot['revenue'] = 0.0
        if 'cost' not in df_pivot: df_pivot['cost'] = 0.0

        # Converti in Decimal e poi float per calcoli
        df_pivot['revenue'] = df_pivot['revenue'].apply(lambda x: float(quantize(to_decimal(x))))
        df_pivot['cost'] = df_pivot['cost'].apply(lambda x: float(quantize(to_decimal(x))))
        
        # Calcola margine lordo e percentuale
        df_pivot['gross_margin'] = df_pivot['revenue'] - df_pivot['cost']
        df_pivot['margin_percent'] = np.where(
            df_pivot['revenue'] > 0, 
            (df_pivot['gross_margin'] / df_pivot['revenue']) * 100, 
            0
        ).round(2)
        
        df_pivot.reset_index(inplace=True)
        df_pivot['month'] = pd.to_datetime(df_pivot['month'], format='%Y-%m').dt.strftime('%Y-%m')

        return df_pivot[['month', 'revenue', 'cost', 'gross_margin', 'margin_percent']]
    except Exception as e:
        logger.error(f"Errore calcolo revenue/cost: {e}", exc_info=True)
        return pd.DataFrame(columns=['month', 'revenue', 'cost', 'gross_margin', 'margin_percent'])
    finally:
        if conn: conn.close()

def get_seasonal_product_analysis(product_category='all', years_back=3):
    """Analisi stagionalità prodotti specifici per frutta e verdura"""
    conn = None
    try:
        conn = get_connection()
        end_date = date.today()
        start_date = end_date - relativedelta(years=years_back)
        
        query = """
            SELECT 
                strftime('%m', i.doc_date) as month_num,
                strftime('%Y', i.doc_date) as year,
                il.description,
                SUM(il.quantity) as total_quantity,
                SUM(il.total_price) as total_value,
                COUNT(DISTINCT i.id) as invoice_count,
                AVG(il.unit_price) as avg_unit_price
            FROM InvoiceLines il
            JOIN Invoices i ON il.invoice_id = i.id
            WHERE i.type = 'Attiva' 
              AND i.doc_date BETWEEN ? AND ?
              AND il.description IS NOT NULL 
              AND TRIM(il.description) != ''
              AND il.quantity > 0
            GROUP BY month_num, year, il.description
            ORDER BY month_num, year, total_value DESC
        """
        
        df = pd.read_sql_query(query, conn, params=(start_date.isoformat(), end_date.isoformat()))
        
        if df.empty:
            return pd.DataFrame()
        
        # Normalizza nomi prodotti
        df['normalized_product'] = df['description'].apply(normalize_product_name)
        df = df.dropna(subset=['normalized_product'])
        
        # Filtro per categoria se specificato
        if product_category != 'all':
            fruit_keywords = ['mela', 'pera', 'banana', 'arancia', 'limone', 'uva', 'pesca', 'albicocca', 'ciliegia', 'fragola', 'kiwi', 'ananas', 'cocco', 'melone', 'anguria', 'pompelmo', 'mandarino']
            vegetable_keywords = ['pomodoro', 'lattuga', 'carota', 'cipolla', 'patata', 'zucchina', 'melanzana', 'peperone', 'broccoli', 'cavolo', 'spinaci', 'finocchio', 'sedano', 'rucola', 'basilico', 'prezzemolo']
            
            if product_category == 'frutta':
                df = df[df['normalized_product'].str.contains('|'.join(fruit_keywords), case=False, na=False)]
            elif product_category == 'verdura':
                df = df[df['normalized_product'].str.contains('|'.join(vegetable_keywords), case=False, na=False)]
        
        # Aggrega per prodotto normalizzato e mese
        seasonal_data = df.groupby(['normalized_product', 'month_num']).agg({
            'total_quantity': 'sum',
            'total_value': 'sum',
            'invoice_count': 'sum',
            'avg_unit_price': 'mean'
        }).reset_index()
        
        # Calcola indice di stagionalità (media del mese / media annuale)
        product_yearly_avg = seasonal_data.groupby('normalized_product')['total_value'].mean()
        seasonal_data['seasonality_index'] = seasonal_data.apply(
            lambda row: row['total_value'] / product_yearly_avg[row['normalized_product']] 
            if product_yearly_avg[row['normalized_product']] > 0 else 0, axis=1
        )
        
        # Formatta per output
        seasonal_data['month_name'] = seasonal_data['month_num'].map({
            '01': 'Gen', '02': 'Feb', '03': 'Mar', '04': 'Apr',
            '05': 'Mag', '06': 'Giu', '07': 'Lug', '08': 'Ago',
            '09': 'Set', '10': 'Ott', '11': 'Nov', '12': 'Dic'
        })
        
        return seasonal_data.sort_values(['normalized_product', 'month_num'])
        
    except Exception as e:
        logger.error(f"Errore analisi stagionalità: {e}", exc_info=True)
        return pd.DataFrame()
    finally:
        if conn: conn.close()

def get_top_clients_performance(start_date=None, end_date=None, limit=20):
    """Analisi performance clienti con metriche specifiche per ingrosso"""
    conn = None
    try:
        conn = get_connection()
        end_date_obj = pd.to_datetime(end_date, errors='coerce').date() if end_date else date.today()
        start_date_obj = pd.to_datetime(start_date, errors='coerce').date() if start_date else (end_date_obj - relativedelta(years=1))

        if pd.isna(start_date_obj) or pd.isna(end_date_obj):
            return pd.DataFrame()

        query = """
            SELECT 
                a.id,
                a.denomination,
                a.city,
                a.score,
                COUNT(i.id) as total_invoices,
                SUM(i.total_amount) as total_revenue,
                AVG(i.total_amount) as avg_order_value,
                MIN(i.doc_date) as first_order_date,
                MAX(i.doc_date) as last_order_date,
                COUNT(DISTINCT strftime('%Y-%m', i.doc_date)) as active_months,
                SUM(CASE WHEN i.payment_status = 'Pagata Tot.' THEN i.total_amount ELSE 0 END) as paid_revenue,
                AVG(CASE 
                    WHEN i.due_date IS NOT NULL AND i.payment_status = 'Pagata Tot.' 
                    THEN julianday(MAX(bt.transaction_date)) - julianday(i.due_date)
                    ELSE NULL 
                END) as avg_payment_delay
            FROM Anagraphics a
            JOIN Invoices i ON a.id = i.anagraphics_id
            LEFT JOIN ReconciliationLinks rl ON i.id = rl.invoice_id
            LEFT JOIN BankTransactions bt ON rl.transaction_id = bt.id
            WHERE a.type = 'Cliente'
              AND i.type = 'Attiva'
              AND i.doc_date BETWEEN ? AND ?
            GROUP BY a.id, a.denomination, a.city, a.score
            HAVING total_revenue > 0
            ORDER BY total_revenue DESC
            LIMIT ?
        """
        
        df = pd.read_sql_query(query, conn, params=(start_date_obj.isoformat(), end_date_obj.isoformat(), limit))
        
        if df.empty:
            return pd.DataFrame()
        
        # Calcola metriche aggiuntive
        df['payment_ratio'] = (df['paid_revenue'] / df['total_revenue'] * 100).round(1)
        df['order_frequency'] = (df['total_invoices'] / df['active_months']).round(2)
        df['customer_lifetime_days'] = (pd.to_datetime(df['last_order_date']) - pd.to_datetime(df['first_order_date'])).dt.days
        
        # Classifica clienti
        df['customer_segment'] = pd.cut(df['total_revenue'], 
                                      bins=[0, 10000, 50000, 100000, float('inf')],
                                      labels=['Bronze', 'Silver', 'Gold', 'Platinum'])
        
        # Formatta importi
        df['total_revenue_fmt'] = df['total_revenue'].apply(lambda x: f"{x:,.2f}€")
        df['avg_order_value_fmt'] = df['avg_order_value'].apply(lambda x: f"{x:,.2f}€")
        
        return df
        
    except Exception as e:
        logger.error(f"Errore analisi performance clienti: {e}", exc_info=True)
        return pd.DataFrame()
    finally:
        if conn: conn.close()

def get_supplier_analysis(start_date=None, end_date=None):
    """Analisi fornitori con focus su affidabilità e costi"""
    conn = None
    try:
        conn = get_connection()
        end_date_obj = pd.to_datetime(end_date, errors='coerce').date() if end_date else date.today()
        start_date_obj = pd.to_datetime(start_date, errors='coerce').date() if start_date else (end_date_obj - relativedelta(years=1))

        query = """
            SELECT 
                a.id,
                a.denomination,
                a.city,
                a.province,
                COUNT(i.id) as total_orders,
                SUM(i.total_amount) as total_cost,
                AVG(i.total_amount) as avg_order_value,
                COUNT(DISTINCT strftime('%Y-%m', i.doc_date)) as active_months,
                MIN(i.doc_date) as first_order_date,
                MAX(i.doc_date) as last_order_date,
                SUM(CASE WHEN i.payment_status = 'Pagata Tot.' THEN i.total_amount ELSE 0 END) as paid_amount,
                COUNT(CASE WHEN i.payment_status = 'Scaduta' THEN 1 END) as overdue_invoices,
                COUNT(DISTINCT il.description) as product_variety
            FROM Anagraphics a
            JOIN Invoices i ON a.id = i.anagraphics_id
            LEFT JOIN InvoiceLines il ON i.id = il.invoice_id
            WHERE a.type = 'Fornitore'
              AND i.type = 'Passiva'
              AND i.doc_date BETWEEN ? AND ?
            GROUP BY a.id, a.denomination, a.city, a.province
            HAVING total_cost > 0
            ORDER BY total_cost DESC
        """
        
        df = pd.read_sql_query(query, conn, params=(start_date_obj.isoformat(), end_date_obj.isoformat()))
        
        if df.empty:
            return pd.DataFrame()
        
        # Calcola reliability score (0-100)
        df['payment_ratio'] = df['paid_amount'] / df['total_cost']
        df['order_consistency'] = df['total_orders'] / df['active_months']
        df['overdue_ratio'] = df['overdue_invoices'] / df['total_orders']
        
        df['reliability_score'] = (
            (df['payment_ratio'] * 40) +  # 40% peso pagamenti
            (np.minimum(df['order_consistency'] / 2, 1) * 30) +  # 30% peso consistenza ordini
            ((1 - df['overdue_ratio']) * 20) +  # 20% peso puntualità
            (np.minimum(df['product_variety'] / 10, 1) * 10)  # 10% peso varietà
        ).round(1)
        
        # Classifica fornitori
        df['supplier_tier'] = pd.cut(df['reliability_score'], 
                                   bins=[0, 60, 75, 85, 100],
                                   labels=['Basic', 'Good', 'Excellent', 'Premium'])
        
        # Formatta importi
        df['total_cost_fmt'] = df['total_cost'].apply(lambda x: f"{x:,.2f}€")
        df['avg_order_value_fmt'] = df['avg_order_value'].apply(lambda x: f"{x:,.2f}€")
        
        return df
        
    except Exception as e:
        logger.error(f"Errore analisi fornitori: {e}", exc_info=True)
        return pd.DataFrame()
    finally:
        if conn: conn.close()

# ===== ANALISI CASH FLOW AVANZATO =====

def get_advanced_cashflow_analysis(start_date=None, end_date=None):
    """Analisi cash flow avanzato con proiezioni per business stagionale"""
    conn = None
    try:
        conn = get_connection()
        end_date_obj = pd.to_datetime(end_date, errors='coerce').date() if end_date else date.today()
        start_date_obj = pd.to_datetime(start_date, errors='coerce').date() if start_date else (end_date_obj - relativedelta(years=1))

        # Analisi dettagliata dei flussi
        query = """
            SELECT 
                strftime('%Y-%m', bt.transaction_date) as month,
                bt.amount,
                bt.description,
                CASE 
                    WHEN bt.amount > 0 AND rl.id IS NOT NULL THEN 'Incassi_Clienti'
                    WHEN bt.amount > 0 AND bt.description LIKE '%VERSAMENTO CONTANT%' THEN 'Incassi_Contanti'
                    WHEN bt.amount > 0 THEN 'Altri_Incassi'
                    WHEN bt.amount < 0 AND rl.id IS NOT NULL AND i.type = 'Passiva' THEN 'Pagamenti_Fornitori'
                    WHEN bt.amount < 0 AND (bt.description LIKE '%POS%' OR bt.description LIKE '%CARTA%') THEN 'Spese_Carte'
                    WHEN bt.amount < 0 AND bt.description LIKE '%COMMISSIONI%' THEN 'Commissioni_Bancarie'
                    WHEN bt.amount < 0 AND (bt.description LIKE '%F24%' OR bt.description LIKE '%TRIBUTI%') THEN 'Tasse_Tributi'
                    WHEN bt.amount < 0 AND bt.description LIKE '%STIPENDI%' THEN 'Stipendi'
                    ELSE 'Altri_Pagamenti'
                END as cash_category
            FROM BankTransactions bt
            LEFT JOIN ReconciliationLinks rl ON bt.id = rl.transaction_id
            LEFT JOIN Invoices i ON rl.invoice_id = i.id
            WHERE bt.transaction_date BETWEEN ? AND ?
              AND bt.reconciliation_status != 'Ignorato'
        """
        
        df = pd.read_sql_query(query, conn, params=(start_date_obj.isoformat(), end_date_obj.isoformat()))
        
        if df.empty:
            return pd.DataFrame()
        
        # Aggrega per mese e categoria
        df_monthly = df.groupby(['month', 'cash_category'])['amount'].sum().unstack(fill_value=0).reset_index()
        
        # Calcola metriche chiave
        df_monthly['net_cash_flow'] = df_monthly.sum(axis=1, numeric_only=True)
        df_monthly['cash_conversion_cycle'] = df_monthly.get('Incassi_Clienti', 0) / (df_monthly.get('Pagamenti_Fornitori', 1) * -1 + 1)
        
        # Trend analysis
        df_monthly['month_date'] = pd.to_datetime(df_monthly['month'])
        df_monthly = df_monthly.sort_values('month_date')
        
        # Rolling averages per stabilità
        for col in df_monthly.select_dtypes(include=[np.number]).columns:
            if col not in ['month_date']:
                df_monthly[f'{col}_3m_avg'] = df_monthly[col].rolling(window=3, min_periods=1).mean()
        
        return df_monthly
        
    except Exception as e:
        logger.error(f"Errore analisi cash flow avanzato: {e}", exc_info=True)
        return pd.DataFrame()
    finally:
        if conn: conn.close()

def get_waste_and_spoilage_analysis(start_date=None, end_date=None):
    """Analisi perdite e deterioramento prodotti deperibili"""
    conn = None
    try:
        conn = get_connection()
        end_date_obj = pd.to_datetime(end_date, errors='coerce').date() if end_date else date.today()
        start_date_obj = pd.to_datetime(start_date, errors='coerce').date() if start_date else (end_date_obj - relativedelta(months=6))

        # Identifica note di credito e resi
        query = """
            SELECT 
                strftime('%Y-%m', i.doc_date) as month,
                il.description,
                SUM(CASE WHEN i.doc_type IN ('TD04', 'TD08') THEN il.total_price ELSE 0 END) as credit_notes,
                SUM(CASE WHEN i.doc_type NOT IN ('TD04', 'TD08') THEN il.total_price ELSE 0 END) as normal_sales,
                COUNT(CASE WHEN i.doc_type IN ('TD04', 'TD08') THEN 1 END) as credit_count,
                AVG(il.unit_price) as avg_unit_price
            FROM InvoiceLines il
            JOIN Invoices i ON il.invoice_id = i.id
            WHERE i.type = 'Attiva'
              AND i.doc_date BETWEEN ? AND ?
              AND il.description IS NOT NULL
            GROUP BY month, il.description
            HAVING (credit_notes > 0 OR normal_sales > 0)
        """
        
        df = pd.read_sql_query(query, conn, params=(start_date_obj.isoformat(), end_date_obj.isoformat()))
        
        if df.empty:
            return pd.DataFrame()
        
        # Normalizza prodotti
        df['normalized_product'] = df['description'].apply(normalize_product_name)
        
        # Aggrega per prodotto normalizzato
        df_agg = df.groupby(['month', 'normalized_product']).agg({
            'credit_notes': 'sum',
            'normal_sales': 'sum',
            'credit_count': 'sum',
            'avg_unit_price': 'mean'
        }).reset_index()
        
        # Calcola tasso di perdita
        df_agg['waste_ratio'] = np.where(
            df_agg['normal_sales'] > 0,
            (df_agg['credit_notes'] / df_agg['normal_sales']) * 100,
            0
        )
        
        # Identifica prodotti ad alto deterioramento
        high_waste_products = df_agg[df_agg['waste_ratio'] > 5]['normalized_product'].unique()
        
        # Analisi stagionale del deterioramento
        df_agg['month_num'] = pd.to_datetime(df_agg['month']).dt.month
        seasonal_waste = df_agg.groupby(['month_num', 'normalized_product'])['waste_ratio'].mean().reset_index()
        
        return {
            'monthly_waste': df_agg,
            'high_waste_products': high_waste_products.tolist(),
            'seasonal_patterns': seasonal_waste
        }
        
    except Exception as e:
        logger.error(f"Errore analisi perdite: {e}", exc_info=True)
        return {}
    finally:
        if conn: conn.close()

def get_inventory_turnover_analysis(start_date=None, end_date=None):
    """Analisi rotazione inventario per prodotti freschi"""
    conn = None
    try:
        conn = get_connection()
        end_date_obj = pd.to_datetime(end_date, errors='coerce').date() if end_date else date.today()
        start_date_obj = pd.to_datetime(start_date, errors='coerce').date() if start_date else (end_date_obj - relativedelta(months=3))

        # Query per analisi acquisti vs vendite per prodotto
        query = """
            SELECT 
                il.description,
                SUM(CASE WHEN i.type = 'Passiva' THEN il.quantity ELSE 0 END) as total_purchased,
                SUM(CASE WHEN i.type = 'Attiva' THEN il.quantity ELSE 0 END) as total_sold,
                SUM(CASE WHEN i.type = 'Passiva' THEN il.total_price ELSE 0 END) as purchase_value,
                SUM(CASE WHEN i.type = 'Attiva' THEN il.total_price ELSE 0 END) as sales_value,
                COUNT(DISTINCT CASE WHEN i.type = 'Passiva' THEN i.doc_date END) as purchase_days,
                COUNT(DISTINCT CASE WHEN i.type = 'Attiva' THEN i.doc_date END) as sales_days,
                AVG(CASE WHEN i.type = 'Passiva' THEN il.unit_price END) as avg_purchase_price,
                AVG(CASE WHEN i.type = 'Attiva' THEN il.unit_price END) as avg_sales_price
            FROM InvoiceLines il
            JOIN Invoices i ON il.invoice_id = i.id
            WHERE i.doc_date BETWEEN ? AND ?
              AND il.description IS NOT NULL
              AND il.quantity > 0
            GROUP BY il.description
            HAVING total_purchased > 0 OR total_sold > 0
        """
        
        df = pd.read_sql_query(query, conn, params=(start_date_obj.isoformat(), end_date_obj.isoformat()))
        
        if df.empty:
            return pd.DataFrame()
        
        # Normalizza prodotti
        df['normalized_product'] = df['description'].apply(normalize_product_name)
        
        # Aggrega per prodotto normalizzato
        df_agg = df.groupby('normalized_product').agg({
            'total_purchased': 'sum',
            'total_sold': 'sum',
            'purchase_value': 'sum',
            'sales_value': 'sum',
            'purchase_days': 'sum',
            'sales_days': 'sum',
            'avg_purchase_price': 'mean',
            'avg_sales_price': 'mean'
        }).reset_index()
        
        # Calcola metriche di rotazione
        period_days = (end_date_obj - start_date_obj).days
        
        df_agg['inventory_turnover'] = np.where(
            df_agg['total_purchased'] > 0,
            df_agg['total_sold'] / df_agg['total_purchased'],
            0
        )
        
        df_agg['days_to_sell'] = np.where(
            df_agg['sales_days'] > 0,
            period_days / df_agg['sales_days'],
            period_days
        )
        
        df_agg['gross_margin'] = df_agg['sales_value'] - df_agg['purchase_value']
        df_agg['margin_percent'] = np.where(
            df_agg['sales_value'] > 0,
            (df_agg['gross_margin'] / df_agg['sales_value']) * 100,
            0
        )
        
        # Classifica prodotti per velocità di rotazione
        df_agg['turnover_category'] = pd.cut(
            df_agg['inventory_turnover'],
            bins=[0, 0.5, 1.0, 2.0, float('inf')],
            labels=['Lento', 'Normale', 'Veloce', 'Molto Veloce']
        )
        
        return df_agg.sort_values('inventory_turnover', ascending=False)
        
    except Exception as e:
        logger.error(f"Errore analisi rotazione inventario: {e}", exc_info=True)
        return pd.DataFrame()
    finally:
        if conn: conn.close()

def get_price_trend_analysis(product_name=None, start_date=None, end_date=None):
    """Analisi andamento prezzi per prodotti specifici"""
    conn = None
    try:
        conn = get_connection()
        end_date_obj = pd.to_datetime(end_date, errors='coerce').date() if end_date else date.today()
        start_date_obj = pd.to_datetime(start_date, errors='coerce').date() if start_date else (end_date_obj - relativedelta(years=2))

        where_clause = ""
        params = [start_date_obj.isoformat(), end_date_obj.isoformat()]
        
        if product_name:
            where_clause = "AND il.description LIKE ?"
            params.append(f"%{product_name}%")

        query = f"""
            SELECT 
                i.doc_date,
                il.description,
                i.type as invoice_type,
                AVG(il.unit_price) as avg_price,
                SUM(il.quantity) as total_quantity,
                COUNT(*) as transaction_count
            FROM InvoiceLines il
            JOIN Invoices i ON il.invoice_id = i.id
            WHERE i.doc_date BETWEEN ? AND ?
              AND il.unit_price > 0
              AND il.quantity > 0
              {where_clause}
            GROUP BY i.doc_date, il.description, i.type
            ORDER BY i.doc_date, il.description
        """
        
        df = pd.read_sql_query(query, conn, params=params)
        
        if df.empty:
            return pd.DataFrame()
        
        # Normalizza prodotti
        df['normalized_product'] = df['description'].apply(normalize_product_name)
        df['doc_date'] = pd.to_datetime(df['doc_date'])
        df['week'] = df['doc_date'].dt.isocalendar().week
        df['month'] = df['doc_date'].dt.to_period('M')
        
        # Aggrega per settimana/mese
        weekly_prices = df.groupby(['week', 'normalized_product', 'invoice_type']).agg({
            'avg_price': 'mean',
            'total_quantity': 'sum',
            'transaction_count': 'sum'
        }).reset_index()
        
        monthly_prices = df.groupby(['month', 'normalized_product', 'invoice_type']).agg({
            'avg_price': 'mean',
            'total_quantity': 'sum',
            'transaction_count': 'sum'
        }).reset_index()
        
        # Calcola volatilità prezzi
        price_volatility = df.groupby(['normalized_product', 'invoice_type'])['avg_price'].agg(['std', 'mean']).reset_index()
        price_volatility['volatility_coefficient'] = (price_volatility['std'] / price_volatility['mean']) * 100
        
        return {
            'daily_prices': df,
            'weekly_prices': weekly_prices,
            'monthly_prices': monthly_prices,
            'price_volatility': price_volatility
        }
        
    except Exception as e:
        logger.error(f"Errore analisi trend prezzi: {e}", exc_info=True)
        return {}
    finally:
        if conn: conn.close()

def get_market_basket_analysis(start_date=None, end_date=None, min_support=0.01):
    """Analisi prodotti frequentemente venduti insieme"""
    conn = None
    try:
        conn = get_connection()
        end_date_obj = pd.to_datetime(end_date, errors='coerce').date() if end_date else date.today()
        start_date_obj = pd.to_datetime(start_date, errors='coerce').date() if start_date else (end_date_obj - relativedelta(months=6))

        # Query per ottenere prodotti per fattura
        query = """
            SELECT 
                i.id as invoice_id,
                i.doc_date,
                a.denomination as customer,
                il.description,
                il.quantity,
                il.total_price
            FROM InvoiceLines il
            JOIN Invoices i ON il.invoice_id = i.id
            JOIN Anagraphics a ON i.anagraphics_id = a.id
            WHERE i.type = 'Attiva'
              AND i.doc_date BETWEEN ? AND ?
              AND il.description IS NOT NULL
              AND il.quantity > 0
        """
        
        df = pd.read_sql_query(query, conn, params=(start_date_obj.isoformat(), end_date_obj.isoformat()))
        
        if df.empty:
            return pd.DataFrame()
        
        # Normalizza prodotti
        df['normalized_product'] = df['description'].apply(normalize_product_name)
        
        # Crea matrice transazioni
        basket_df = df.groupby(['invoice_id', 'normalized_product'])['quantity'].sum().unstack(fill_value=0)
        basket_df = basket_df.applymap(lambda x: 1 if x > 0 else 0)
        
        # Calcola support per singoli prodotti
        product_support = basket_df.mean()
        frequent_products = product_support[product_support >= min_support].index.tolist()
        
        # Analisi associazioni per coppie di prodotti
        associations = []
        for i, prod1 in enumerate(frequent_products):
            for prod2 in frequent_products[i+1:]:
                # Support per coppia
                support_both = (basket_df[prod1] & basket_df[prod2]).mean()
                
                if support_both >= min_support:
                    # Confidence A -> B e B -> A
                    confidence_1_2 = support_both / product_support[prod1]
                    confidence_2_1 = support_both / product_support[prod2]
                    
                    # Lift
                    lift = support_both / (product_support[prod1] * product_support[prod2])
                    
                    associations.append({
                        'product_1': prod1,
                        'product_2': prod2,
                        'support': support_both,
                        'confidence_1_2': confidence_1_2,
                        'confidence_2_1': confidence_2_1,
                        'lift': lift
                    })
        
        associations_df = pd.DataFrame(associations)
        if not associations_df.empty:
            associations_df = associations_df.sort_values('lift', ascending=False)
        
        return {
            'product_support': product_support.sort_values(ascending=False),
            'associations': associations_df,
            'basket_matrix': basket_df
        }
        
    except Exception as e:
        logger.error(f"Errore market basket analysis: {e}", exc_info=True)
        return {}
    finally:
        if conn: conn.close()

def get_customer_rfm_analysis(analysis_date=None):
    """Analisi RFM (Recency, Frequency, Monetary) per segmentazione clienti"""
    conn = None
    try:
        conn = get_connection()
        analysis_date_obj = pd.to_datetime(analysis_date, errors='coerce').date() if analysis_date else date.today()

        query = """
            SELECT 
                a.id as customer_id,
                a.denomination,
                MAX(i.doc_date) as last_purchase_date,
                COUNT(i.id) as frequency,
                SUM(i.total_amount) as monetary_value,
                AVG(i.total_amount) as avg_order_value,
                MIN(i.doc_date) as first_purchase_date
            FROM Anagraphics a
            JOIN Invoices i ON a.id = i.anagraphics_id
            WHERE a.type = 'Cliente'
              AND i.type = 'Attiva'
              AND i.payment_status != 'Annullata'
            GROUP BY a.id, a.denomination
            HAVING frequency > 0
        """
        
        df = pd.read_sql_query(query, conn)
        
        if df.empty:
            return pd.DataFrame()
        
        # Calcola Recency (giorni dall'ultimo acquisto)
        df['last_purchase_date'] = pd.to_datetime(df['last_purchase_date'])
        df['recency'] = (analysis_date_obj - df['last_purchase_date'].dt.date).dt.days
        
        # Calcola Customer Lifetime (giorni)
        df['first_purchase_date'] = pd.to_datetime(df['first_purchase_date'])
        df['customer_lifetime'] = (df['last_purchase_date'] - df['first_purchase_date']).dt.days + 1
        
        # Score RFM (1-5, dove 5 è il migliore)
        df['recency_score'] = pd.qcut(df['recency'], q=5, labels=[5, 4, 3, 2, 1], duplicates='drop')
        df['frequency_score'] = pd.qcut(df['frequency'].rank(method='first'), q=5, labels=[1, 2, 3, 4, 5], duplicates='drop')
        df['monetary_score'] = pd.qcut(df['monetary_value'].rank(method='first'), q=5, labels=[1, 2, 3, 4, 5], duplicates='drop')
        
        # Converti in numerico
        df['recency_score'] = pd.to_numeric(df['recency_score'])
        df['frequency_score'] = pd.to_numeric(df['frequency_score'])
        df['monetary_score'] = pd.to_numeric(df['monetary_score'])
        
        # Score RFM combinato
        df['rfm_score'] = df['recency_score'].astype(str) + df['frequency_score'].astype(str) + df['monetary_score'].astype(str)
        df['rfm_score_numeric'] = df['recency_score'] + df['frequency_score'] + df['monetary_score']
        
        # Segmentazione clienti
        def assign_segment(row):
            r, f, m = row['recency_score'], row['frequency_score'], row['monetary_score']
            
            if r >= 4 and f >= 4 and m >= 4:
                return 'Champions'
            elif r >= 3 and f >= 3 and m >= 3:
                return 'Loyal Customers'
            elif r >= 4 and f <= 2:
                return 'New Customers'
            elif r <= 2 and f >= 3 and m >= 3:
                return 'At Risk'
            elif r <= 2 and f <= 2 and m >= 3:
                return 'Cannot Lose Them'
            elif r <= 2 and f <= 2 and m <= 2:
                return 'Lost Customers'
            elif f >= 3 and m <= 2:
                return 'Potential Loyalists'
            else:
                return 'Others'
        
        df['customer_segment'] = df.apply(assign_segment, axis=1)
        
        # Statistiche per segmento
        segment_stats = df.groupby('customer_segment').agg({
            'customer_id': 'count',
            'recency': 'mean',
            'frequency': 'mean',
            'monetary_value': ['mean', 'sum'],
            'avg_order_value': 'mean'
        }).round(2)
        
        return {
            'rfm_data': df.sort_values('rfm_score_numeric', ascending=False),
            'segment_stats': segment_stats
        }
        
    except Exception as e:
        logger.error(f"Errore analisi RFM: {e}", exc_info=True)
        return {}
    finally:
        if conn: conn.close()

# ===== ANALISI CASH FLOW AGGIORNATO =====

def _get_categorized_transactions(start_date_str, end_date_str):
    """Categorizzazione transazioni migliorata per business frutta e verdura"""
    conn = None
    try:
        conn = get_connection()
        query_trans = """
            SELECT
                bt.id as transaction_id,
                bt.transaction_date,
                bt.amount,
                bt.description,
                GROUP_CONCAT(rl.id) as link_ids,
                GROUP_CONCAT(i.type) as linked_invoice_types,
                SUM(rl.reconciled_amount) as total_linked_amount
            FROM BankTransactions bt
            LEFT JOIN ReconciliationLinks rl ON bt.id = rl.transaction_id
            LEFT JOIN Invoices i ON rl.invoice_id = i.id
            WHERE bt.transaction_date BETWEEN ? AND ?
              AND bt.reconciliation_status != 'Ignorato'
            GROUP BY bt.id, bt.transaction_date, bt.amount, bt.description
            ORDER BY bt.transaction_date;
        """
        df = pd.read_sql_query(query_trans, conn, params=(start_date_str, end_date_str), parse_dates=['transaction_date'])

        if df.empty:
            logger.info("Nessuna transazione trovata nel periodo per la categorizzazione.")
            return pd.DataFrame(columns=['transaction_id', 'transaction_date', 'amount', 'description', 'category', 'amount_dec'])

        df['amount_dec'] = df['amount'].apply(lambda x: quantize(to_decimal(x)))
        df['category'] = 'Altro'

        # Pattern migliorati per business frutta e verdura
        pos_pattern = r'\b(POS|PAGOBANCOMAT|CIRRUS|MAESTRO|VISA|MASTERCARD|AMEX|WORLDLINE|ESE COMM)\b'
        cash_pattern = r'(VERSAMENTO CONTANT|CONTANTI|CASSA)'
        commission_pattern = r'^(COMMISSIONI|COMPETENZE BANC|SPESE TENUTA CONTO|IMPOSTA DI BOLLO)'
        fuel_pattern = r'(BENZINA|GASOLIO|CARBURANTE|DISTRIBUTORE|ENI|AGIP|Q8)'
        transport_pattern = r'(AUTOSTRADA|PEDAGGI|TELEPASS|TRASPORT)'
        utilities_pattern = r'(ENEL|GAS|ACQUA|TELEFON|INTERNET|TIM|VODAFONE)'
        taxes_pattern = r'(F24|TRIBUTI|INPS|INAIL|AGENZIA ENTRATE)'
        bank_fees_pattern = r'(CANONE|BOLLO|COMMISSIONI)'

        for index, row in df.iterrows():
            trans_id = row['transaction_id']
            amount = row['amount_dec']
            desc = str(row['description'] or '').upper()
            has_links = pd.notna(row['link_ids'])
            linked_types = set(str(row['linked_invoice_types'] or '').split(',')) if has_links else set()

            if has_links:
                if amount > 0 and 'Attiva' in linked_types:
                    df.loc[index, 'category'] = 'Incassi_Clienti'
                elif amount < 0 and 'Passiva' in linked_types:
                    df.loc[index, 'category'] = 'Pagamenti_Fornitori'
                else:
                    df.loc[index, 'category'] = 'Riconciliati_Altri'
            else:
                if amount > 0:
                    if re.search(cash_pattern, desc, re.IGNORECASE):
                        df.loc[index, 'category'] = 'Incassi_Contanti'
                    else:
                        df.loc[index, 'category'] = 'Altri_Incassi'
                elif amount < 0:
                    if re.search(commission_pattern, desc, re.IGNORECASE):
                        df.loc[index, 'category'] = 'Commissioni_Bancarie'
                    elif re.search(pos_pattern, desc, re.IGNORECASE):
                        df.loc[index, 'category'] = 'Spese_Carte'
                    elif re.search(fuel_pattern, desc, re.IGNORECASE):
                        df.loc[index, 'category'] = 'Carburanti'
                    elif re.search(transport_pattern, desc, re.IGNORECASE):
                        df.loc[index, 'category'] = 'Trasporti'
                    elif re.search(utilities_pattern, desc, re.IGNORECASE):
                        df.loc[index, 'category'] = 'Utenze'
                    elif re.search(taxes_pattern, desc, re.IGNORECASE):
                        df.loc[index, 'category'] = 'Tasse_Tributi'
                    elif re.search(bank_fees_pattern, desc, re.IGNORECASE):
                        df.loc[index, 'category'] = 'Commissioni_Bancarie'
                    else:
                        df.loc[index, 'category'] = 'Altri_Pagamenti'

        return df

    except Exception as e:
        logger.error(f"Errore durante la categorizzazione delle transazioni: {e}", exc_info=True)
        return pd.DataFrame(columns=['transaction_id', 'transaction_date', 'amount', 'description', 'category', 'amount_dec'])
    finally:
        if conn: conn.close()

def get_cashflow_data(start_date=None, end_date=None):
    """Cash flow data con categorizzazione migliorata"""
    cols_out = ['month', 'incassi_clienti', 'incassi_contanti', 'altri_incassi', 
                'pagamenti_fornitori', 'spese_carte', 'carburanti', 'trasporti', 
                'utenze', 'tasse_tributi', 'commissioni_bancarie', 'altri_pagamenti',
                'net_operational_flow', 'total_inflows', 'total_outflows', 'net_cash_flow']
    
    empty_df = pd.DataFrame(columns=cols_out).astype(float)
    empty_df['month'] = pd.Series(dtype='str')

    try:
        end_date_obj = pd.to_datetime(end_date, errors='coerce').date() if end_date else date.today()
        start_date_obj = pd.to_datetime(start_date, errors='coerce').date() if start_date else (end_date_obj - relativedelta(years=1) + relativedelta(days=1))

        if pd.isna(start_date_obj) or pd.isna(end_date_obj):
            logger.error("Date inizio/fine non valide per get_cashflow_data")
            return empty_df

        start_str, end_str = start_date_obj.isoformat(), end_date_obj.isoformat()
        df_categorized = _get_categorized_transactions(start_str, end_str)

        if df_categorized.empty:
            logger.info("Nessuna transazione categorizzata trovata per il cash flow.")
            return empty_df

        df_categorized['month'] = df_categorized['transaction_date'].dt.strftime('%Y-%m')
        
        # Aggrega per mese e categoria
        df_monthly = df_categorized.groupby(['month', 'category'])['amount_dec'].sum().unstack(fill_value=Decimal('0.0')).reset_index()

        # Mappa categorie alle colonne di output
        category_mapping = {
            'Incassi_Clienti': 'incassi_clienti',
            'Incassi_Contanti': 'incassi_contanti',
            'Altri_Incassi': 'altri_incassi',
            'Riconciliati_Altri': 'altri_incassi',
            'Pagamenti_Fornitori': 'pagamenti_fornitori',
            'Spese_Carte': 'spese_carte',
            'Carburanti': 'carburanti',
            'Trasporti': 'trasporti',
            'Utenze': 'utenze',
            'Tasse_Tributi': 'tasse_tributi',
            'Commissioni_Bancarie': 'commissioni_bancarie',
            'Altri_Pagamenti': 'altri_pagamenti'
        }

        df_out = pd.DataFrame()
        df_out['month'] = df_monthly['month']

        # Inizializza tutte le colonne
        for col in cols_out[1:-4]:  # Escludi month e le colonne calcolate
            df_out[col] = 0.0

        # Popola le colonne con i dati
        for category, column in category_mapping.items():
            if category in df_monthly.columns:
                if column in ['pagamenti_fornitori', 'spese_carte', 'carburanti', 'trasporti', 
                             'utenze', 'tasse_tributi', 'commissioni_bancarie', 'altri_pagamenti']:
                    # Converti importi negativi in positivi per le uscite
                    df_out[column] = df_monthly[category].apply(lambda x: float(abs(x)) if x < 0 else 0.0)
                else:
                    # Mantieni solo importi positivi per le entrate
                    df_out[column] = df_monthly[category].apply(lambda x: float(x) if x > 0 else 0.0)

        # Calcola totali e flussi netti
        inflow_cols = ['incassi_clienti', 'incassi_contanti', 'altri_incassi']
        outflow_cols = ['pagamenti_fornitori', 'spese_carte', 'carburanti', 'trasporti', 
                       'utenze', 'tasse_tributi', 'commissioni_bancarie', 'altri_pagamenti']
        
        df_out['total_inflows'] = df_out[inflow_cols].sum(axis=1)
        df_out['total_outflows'] = df_out[outflow_cols].sum(axis=1)
        df_out['net_cash_flow'] = df_out['total_inflows'] - df_out['total_outflows']
        df_out['net_operational_flow'] = df_out['incassi_clienti'] + df_out['incassi_contanti'] - df_out['pagamenti_fornitori']

        df_out['month'] = pd.to_datetime(df_out['month'], format='%Y-%m').dt.strftime('%Y-%m')

        return df_out[cols_out]

    except Exception as e:
        logger.error(f"Errore recupero dati cash flow categorizzato: {e}", exc_info=True)
        return empty_df

def get_cashflow_table(start_date=None, end_date=None):
    """Tabella cash flow formattata per UI"""
    cols_out = ['Mese', 'Incassi Clienti', 'Incassi Contanti', 'Altri Incassi', 'Tot. Entrate',
                'Pagam. Fornitori', 'Spese Varie', 'Carburanti', 'Utenze', 'Tasse', 'Commissioni', 'Tot. Uscite',
                'Flusso Operativo', 'Flusso Netto']
    empty_df = pd.DataFrame(columns=cols_out)
    
    try:
        df_data = get_cashflow_data(start_date, end_date)
        if df_data.empty: 
            return empty_df

        df_table = pd.DataFrame()
        try:
            df_table['Mese'] = pd.to_datetime(df_data['month'], format='%Y-%m').dt.strftime('%b %Y')
        except ValueError:
            df_table['Mese'] = df_data['month']

        def format_currency(val):
            if pd.isna(val): 
                return 'N/A'
            try:
                d = quantize(Decimal(str(val)))
                return f"{d:,.2f}".replace(",", "X").replace(".", ",").replace("X", ".")
            except (InvalidOperation, TypeError):
                return "0,00"

        # Mappiamo i dati alle colonne della tabella
        df_table['Incassi Clienti'] = df_data['incassi_clienti'].apply(format_currency)
        df_table['Incassi Contanti'] = df_data['incassi_contanti'].apply(format_currency)
        df_table['Altri Incassi'] = df_data['altri_incassi'].apply(format_currency)
        df_table['Tot. Entrate'] = df_data['total_inflows'].apply(format_currency)
        
        df_table['Pagam. Fornitori'] = df_data['pagamenti_fornitori'].apply(format_currency)
        # Raggruppiamo alcune spese per semplicità
        df_data['spese_varie'] = (df_data['spese_carte'] + df_data['trasporti'] + df_data['altri_pagamenti'])
        df_table['Spese Varie'] = df_data['spese_varie'].apply(format_currency)
        df_table['Carburanti'] = df_data['carburanti'].apply(format_currency)
        df_table['Utenze'] = df_data['utenze'].apply(format_currency)
        df_table['Tasse'] = df_data['tasse_tributi'].apply(format_currency)
        df_table['Commissioni'] = df_data['commissioni_bancarie'].apply(format_currency)
        df_table['Tot. Uscite'] = df_data['total_outflows'].apply(format_currency)
        
        df_table['Flusso Operativo'] = df_data['net_operational_flow'].apply(format_currency)
        df_table['Flusso Netto'] = df_data['net_cash_flow'].apply(format_currency)

        return df_table[cols_out]

    except Exception as e:
        logger.error(f"Errore creazione tabella cash flow: {e}", exc_info=True)
        return empty_df

# ===== MANTENUTE LE FUNZIONI ORIGINALI (aggiornate) =====

def get_products_analysis(invoice_type='Attiva', start_date=None, end_date=None):
    """Analisi prodotti con normalizzazione migliorata"""
    conn = None
    cols_out = ['Prodotto Normalizzato', 'Quantità Tot.', 'Valore Totale', 'N. Fatture', 'Prezzo Medio', 'Descrizioni Originali']
    empty_df = pd.DataFrame(columns=cols_out)

    try:
        conn = get_connection()
        end_date_obj = pd.to_datetime(end_date, errors='coerce').date() if end_date else date.today()
        start_date_obj = pd.to_datetime(start_date, errors='coerce').date() if start_date else (end_date_obj - timedelta(days=90))

        if pd.isna(start_date_obj) or pd.isna(end_date_obj):
            return empty_df

        start_str, end_str = start_date_obj.isoformat(), end_date_obj.isoformat()

        query_lines = """
            SELECT
                il.description,
                il.quantity,
                il.total_price,
                il.unit_price,
                i.id as invoice_id
            FROM InvoiceLines il
            JOIN Invoices i ON il.invoice_id = i.id
            WHERE i.type = ? AND i.doc_date BETWEEN ? AND ?
              AND il.description IS NOT NULL AND TRIM(il.description) != '';
        """
        df_lines = pd.read_sql_query(query_lines, conn, params=(invoice_type, start_str, end_str))

        if df_lines.empty:
            return empty_df

        df_lines['normalized_product'] = df_lines['description'].apply(normalize_product_name)
        df_lines.dropna(subset=['normalized_product'], inplace=True)

        if df_lines.empty:
            return empty_df

        # Converti in Decimal prima di aggregare
        df_lines['quantity_dec'] = df_lines['quantity'].apply(lambda x: to_decimal(x, default='NaN'))
        df_lines['total_price_dec'] = df_lines['total_price'].apply(lambda x: to_decimal(x, default='NaN'))
        df_lines['unit_price_dec'] = df_lines['unit_price'].apply(lambda x: to_decimal(x, default='NaN'))

        # Aggrega
        df_agg = df_lines.groupby('normalized_product').agg(
            TotalQuantityDec=('quantity_dec', lambda x: x.dropna().sum()),
            TotalValueDec=('total_price_dec', lambda x: x.dropna().sum()),
            AvgUnitPriceDec=('unit_price_dec', lambda x: x.dropna().mean()),
            NumInvoices=('invoice_id', 'nunique'),
            OriginalDescriptions=('description', lambda x: '|'.join(sorted(list(set(x)))))
        ).reset_index()

        df_agg.rename(columns={'normalized_product': 'Prodotto Normalizzato',
                               'NumInvoices': 'N. Fatture'}, inplace=True)

        # Formatta DOPO l'aggregazione
        df_agg['Valore Totale'] = df_agg['TotalValueDec'].apply(lambda x: f"{quantize(x):,.2f}".replace(",", "X").replace(".", ",").replace("X", ".") if x.is_finite() else '0,00')
        df_agg['Quantità Tot.'] = df_agg['TotalQuantityDec'].apply(lambda x: f"{quantize(x):,.2f}".replace(",", "X").replace(".", ",").replace("X", ".") if x.is_finite() else '')
        df_agg['Prezzo Medio'] = df_agg['AvgUnitPriceDec'].apply(lambda x: f"{quantize(x):,.2f}".replace(",", "X").replace(".", ",").replace("X", ".") if x.is_finite() else '0,00')
        df_agg['Descrizioni Originali'] = df_agg['OriginalDescriptions']

        # Ordina per valore numerico prima di formattare
        df_agg = df_agg.sort_values(by='TotalValueDec', ascending=False)

        return df_agg[cols_out]

    except Exception as e:
        logging.error(f"Errore analisi prodotti ({invoice_type}): {e}", exc_info=True)
        return empty_df
    finally:
        if conn: conn.close()

def calculate_and_update_client_scores():
    """Calcolo score clienti con logica migliorata"""
    conn = None
    today = date.today()
    client_delays = {}
    
    try:
        conn = get_connection()
        cursor = conn.cursor()
        
        # Query migliorata per includere più fattori
        query = """
            SELECT 
                i.anagraphics_id, 
                i.due_date, 
                MAX(bt.transaction_date) AS payment_date,
                i.total_amount,
                COUNT(*) as payment_frequency
            FROM Invoices i
            JOIN ReconciliationLinks rl ON i.id = rl.invoice_id
            JOIN BankTransactions bt ON rl.transaction_id = bt.id
            WHERE i.type = 'Attiva'
              AND i.payment_status IN ('Pagata Tot.', 'Riconciliata')
              AND i.due_date IS NOT NULL
              AND DATE(i.due_date) IS NOT NULL 
              AND DATE(bt.transaction_date) IS NOT NULL
            GROUP BY i.id, i.anagraphics_id, i.due_date, i.total_amount;
        """
        
        cursor.execute(query)
        rows = cursor.fetchall()

        for row in rows:
            client_id = row['anagraphics_id']
            try:
                due_date_obj = date.fromisoformat(row['due_date'])
                payment_date_obj = date.fromisoformat(row['payment_date'])
                delay = (payment_date_obj - due_date_obj).days
                amount = float(row['total_amount'])
                
                if client_id not in client_delays:
                    client_delays[client_id] = []
                    
                # Pesare i ritardi per importo (ritardi su fatture maggiori pesano di più)
                weighted_delay = delay * (amount / 1000.0)  # Normalizza per 1000€
                client_delays[client_id].append({
                    'delay': delay,
                    'weighted_delay': weighted_delay,
                    'amount': amount
                })
                
            except (ValueError, TypeError) as date_ex:
                logging.warning(f"Errore conversione data per score client {client_id}: {date_ex}")

        conn.execute("BEGIN TRANSACTION")
        updated_count = 0
        
        for client_id, delay_data in client_delays.items():
            if not delay_data:
                continue
                
            # Calcola metriche avanzate
            delays = [d['delay'] for d in delay_data]
            weighted_delays = [d['weighted_delay'] for d in delay_data]
            amounts = [d['amount'] for d in delay_data]
            
            avg_delay = sum(delays) / len(delays)
            avg_weighted_delay = sum(weighted_delays) / len(weighted_delays)
            total_amount = sum(amounts)
            
            # Score basato su multiple componenti
            delay_penalty = max(0, avg_delay) * 1.5
            weighted_penalty = max(0, avg_weighted_delay) * 0.5
            
            # Bonus per clienti con fatturato alto
            volume_bonus = min(10, total_amount / 10000)  # Max 10 punti per 100k€+
            
            # Bonus per frequenza pagamenti
            frequency_bonus = min(5, len(delays) / 10)  # Max 5 punti per 10+ fatture
            
            score = max(0.0, min(100.0, round(100.0 - delay_penalty - weighted_penalty + volume_bonus + frequency_bonus, 1)))
            
            cursor.execute("UPDATE Anagraphics SET score = ?, updated_at = ? WHERE id = ?", 
                         (score, datetime.now(), client_id))
            if cursor.rowcount > 0:
                updated_count += 1

        conn.commit()
        logging.info(f"Score calculation completed. Updated: {updated_count} clients.")
        return True
        
    except Exception as e:
        logging.error(f"Errore calcolo score: {e}", exc_info=True)
        if conn:
            try:
                conn.rollback()
            except Exception:
                pass
        return False
    finally:
        if conn:
            conn.close()

def get_commission_summary(start_date=None, end_date=None):
    """Analisi commissioni bancarie migliorata"""
    conn = None
    cols_out = ['Mese', 'Totale Commissioni', 'N. Operazioni', 'Commissione Media', 'Tipo Principale']
    empty_df = pd.DataFrame(columns=cols_out)
    
    try:
        conn = get_connection()
        end_date_obj = pd.to_datetime(end_date, errors='coerce').date() if end_date else date.today()
        start_date_obj = pd.to_datetime(start_date, errors='coerce').date() if start_date else (end_date_obj - relativedelta(years=1))

        if pd.isna(start_date_obj) or pd.isna(end_date_obj):
            return empty_df

        start_str, end_str = start_date_obj.isoformat(), end_date_obj.isoformat()

        query = """
            SELECT
                strftime('%Y-%m', transaction_date) AS month,
                SUM(CASE WHEN amount < 0 THEN amount ELSE 0 END) AS total_commissions,
                COUNT(id) as operation_count,
                AVG(CASE WHEN amount < 0 THEN amount ELSE NULL END) as avg_commission,
                description
            FROM BankTransactions
            WHERE transaction_date BETWEEN ? AND ?
              AND amount < 0
              AND reconciliation_status != 'Ignorato'
              AND (LOWER(description) LIKE 'commissioni%' OR
                   LOWER(description) LIKE 'competenze banc%' OR
                   LOWER(description) LIKE 'spese tenuta conto%' OR
                   LOWER(description) LIKE 'imposta di bollo%' OR
                   LOWER(description) LIKE 'canone%')
            GROUP BY month, description
            ORDER BY month;
        """
        df = pd.read_sql_query(query, conn, params=(start_str, end_str))

        if df.empty:
            return empty_df

        # Aggrega per mese
        df_monthly = df.groupby('month').agg({
            'total_commissions': 'sum',
            'operation_count': 'sum',
            'avg_commission': 'mean',
            'description': lambda x: x.value_counts().index[0]  # Tipo più frequente
        }).reset_index()

        df_monthly['TotalDec'] = df_monthly['total_commissions'].apply(lambda x: quantize(to_decimal(x)))
        df_monthly['AvgDec'] = df_monthly['avg_commission'].apply(lambda x: quantize(to_decimal(x)))

        try:
            df_monthly['Mese'] = pd.to_datetime(df_monthly['month'], format='%Y-%m').dt.strftime('%b %Y')
        except ValueError:
            df_monthly['Mese'] = df_monthly['month']

        df_monthly['Totale Commissioni'] = df_monthly['TotalDec'].apply(lambda x: f"{x.copy_abs():,.2f}".replace(",", "X").replace(".", ",").replace("X", "."))
        df_monthly['N. Operazioni'] = df_monthly['operation_count']
        df_monthly['Commissione Media'] = df_monthly['AvgDec'].apply(lambda x: f"{x.copy_abs():,.2f}".replace(",", "X").replace(".", ",").replace("X", "."))
        df_monthly['Tipo Principale'] = df_monthly['description'].str.title()

        return df_monthly[cols_out]

    except Exception as e:
        logger.error(f"Errore sommario commissioni: {e}", exc_info=True)
        return empty_df
    finally:
        if conn:
            conn.close()

def get_top_clients_by_revenue(start_date=None, end_date=None, limit=20):
    """Top clienti con metriche aggiuntive"""
    conn = None
    cols_out = ['ID', 'Denominazione', 'Fatturato Totale', 'N. Fatture', 'Score', 'Ticket Medio', 'Margine %']
    empty_df = pd.DataFrame(columns=cols_out)
    
    try:
        conn = get_connection()
        end_date_obj = pd.to_datetime(end_date, errors='coerce').date() if end_date else date.today()
        start_date_obj = pd.to_datetime(start_date, errors='coerce').date() if start_date else (end_date_obj - relativedelta(years=1))

        if pd.isna(start_date_obj) or pd.isna(end_date_obj):
            return empty_df

        start_str, end_str = start_date_obj.isoformat(), end_date_obj.isoformat()
        
        # Query estesa con calcolo del margine
        query = """
            SELECT 
                a.id, 
                a.denomination AS Denominazione, 
                SUM(i.total_amount) AS FatturatoTotale,
                COUNT(i.id) AS NumeroFatture, 
                a.score AS ScoreCliente,
                AVG(i.total_amount) AS TicketMedio,
                -- Stima margine basata su differenza prezzi vendita vs acquisto per prodotti simili
                (SELECT AVG(
                    CASE WHEN il_sell.unit_price > 0 AND il_buy.unit_price > 0 
                    THEN ((il_sell.unit_price - il_buy.unit_price) / il_sell.unit_price) * 100 
                    ELSE NULL END
                ) FROM InvoiceLines il_sell 
                JOIN Invoices i_sell ON il_sell.invoice_id = i_sell.id
                LEFT JOIN InvoiceLines il_buy ON il_sell.description = il_buy.description
                LEFT JOIN Invoices i_buy ON il_buy.invoice_id = i_buy.id AND i_buy.type = 'Passiva'
                WHERE i_sell.anagraphics_id = a.id AND i_sell.type = 'Attiva'
                AND i_sell.doc_date BETWEEN ? AND ?
                ) AS MargineStimato
            FROM Invoices i 
            JOIN Anagraphics a ON i.anagraphics_id = a.id
            WHERE i.type = 'Attiva' AND i.doc_date BETWEEN ? AND ?
            GROUP BY a.id, a.denomination, a.score 
            ORDER BY FatturatoTotale DESC LIMIT ?;
        """
        
        df = pd.read_sql_query(query, conn, params=(start_str, end_str, start_str, end_str, limit))

        if df.empty:
            return empty_df

        df['Fatturato Totale'] = df['FatturatoTotale'].apply(lambda x: f"{quantize(to_decimal(x)):,.2f}€")
        df['N. Fatture'] = df['NumeroFatture']
        df['Score'] = df['ScoreCliente'].apply(lambda x: f"{x:.1f}" if pd.notna(x) else 'N/D')
        df['Ticket Medio'] = df['TicketMedio'].apply(lambda x: f"{quantize(to_decimal(x)):,.2f}€")
        df['Margine %'] = df['MargineStimato'].apply(lambda x: f"{x:.1f}%" if pd.notna(x) and x > 0 else 'N/D')
        df['ID'] = df['id']
        
        return df[cols_out]
        
    except Exception as e:
        logging.error(f"Errore top clienti: {e}", exc_info=True)
        return empty_df
    finally:
        if conn:
            conn.close()

def get_clients_by_score(order='DESC', limit=20):
    """Clienti ordinati per score con info aggiuntive"""
    conn = None
    cols_out = ['ID', 'Denominazione', 'P.IVA', 'C.F.', 'Città', 'Score', 'Ultimo Ordine', 'Fatturato YTD']
    empty_df = pd.DataFrame(columns=cols_out)
    
    try:
        conn = get_connection()
        sort_order = "ASC" if order.upper() == 'ASC' else "DESC"
        nulls_placement = "NULLS LAST" if sort_order == "DESC" else "NULLS FIRST"
        
        current_year = date.today().year
        
        query = f"""
            SELECT 
                a.id, 
                a.denomination, 
                a.piva, 
                a.cf, 
                a.city, 
                a.score,
                MAX(i.doc_date) as ultimo_ordine,
                SUM(CASE WHEN strftime('%Y', i.doc_date) = '{current_year}' THEN i.total_amount ELSE 0 END) as fatturato_ytd
            FROM Anagraphics a
            LEFT JOIN Invoices i ON a.id = i.anagraphics_id AND i.type = 'Attiva'
            WHERE a.type = 'Cliente'
            GROUP BY a.id, a.denomination, a.piva, a.cf, a.city, a.score
            ORDER BY a.score {sort_order} {nulls_placement}, a.denomination COLLATE NOCASE
            LIMIT ?;
        """
        
        df = pd.read_sql_query(query, conn, params=(limit,))

        if df.empty:
            return empty_df

        df['ID'] = df['id']
        df['Denominazione'] = df['denomination']
        df['P.IVA'] = df['piva'].fillna('')
        df['C.F.'] = df['cf'].fillna('')
        df['Città'] = df['city'].fillna('')
        df['Score'] = df['score'].apply(lambda x: f"{x:.1f}" if pd.notna(x) else 'N/D')
        df['Ultimo Ordine'] = pd.to_datetime(df['ultimo_ordine'], errors='coerce').dt.strftime('%d/%m/%Y').fillna('Mai')
        df['Fatturato YTD'] = df['fatturato_ytd'].apply(lambda x: f"{quantize(to_decimal(x)):,.0f}€" if pd.notna(x) and x > 0 else '0€')
        
        return df[cols_out]
        
    except Exception as e:
        logging.error(f"Errore clienti per score: {e}", exc_info=True)
        return empty_df
    finally:
        if conn:
            conn.close()

def get_product_monthly_sales(normalized_description, start_date=None, end_date=None):
    """Vendite mensili prodotto con trend analysis"""
    conn = None
    cols_out = ['Mese', 'Quantità', 'Valore', 'Prezzo Medio', 'Trend %']
    empty_df = pd.DataFrame(columns=cols_out)
    
    try:
        conn = get_connection()
        end_date_obj = pd.to_datetime(end_date, errors='coerce').date() if end_date else date.today()
        start_date_obj = pd.to_datetime(start_date, errors='coerce').date() if start_date else (end_date_obj - relativedelta(years=2))

        if pd.isna(start_date_obj) or pd.isna(end_date_obj):
            return empty_df

        start_str, end_str = start_date_obj.isoformat(), end_date_obj.isoformat()

        query_all_lines = """
            SELECT 
                i.doc_date, 
                il.description, 
                il.quantity, 
                il.total_price,
                il.unit_price
            FROM InvoiceLines il 
            JOIN Invoices i ON il.invoice_id = i.id
            WHERE i.type = 'Attiva' AND i.doc_date BETWEEN ? AND ?
              AND il.description IS NOT NULL AND TRIM(il.description) != '';
        """
        
        df_lines = pd.read_sql_query(query_all_lines, conn, params=(start_str, end_str), parse_dates=['doc_date'])

        if df_lines.empty:
            return empty_df

        df_lines['normalized_product'] = df_lines['description'].apply(normalize_product_name)
        df_filtered = df_lines[df_lines['normalized_product'] == normalized_description].copy()

        if df_filtered.empty:
            return empty_df

        df_filtered['month'] = df_filtered['doc_date'].dt.strftime('%Y-%m')
        
        # Converti a Decimal
        df_filtered['quantity_dec'] = df_filtered['quantity'].apply(lambda x: to_decimal(x, default='NaN'))
        df_filtered['total_price_dec'] = df_filtered['total_price'].apply(lambda x: to_decimal(x, default='NaN'))
        df_filtered['unit_price_dec'] = df_filtered['unit_price'].apply(lambda x: to_decimal(x, default='NaN'))

        df_monthly = df_filtered.groupby('month').agg(
            monthly_quantity=('quantity_dec', lambda x: x.dropna().sum()),
            monthly_value=('total_price_dec', lambda x: x.dropna().sum()),
            avg_unit_price=('unit_price_dec', lambda x: x.dropna().mean())
        ).reset_index().sort_values(by='month')

        # Calcola trend percentuale mese su mese
        df_monthly['trend_percent'] = df_monthly['monthly_value'].pct_change() * 100

        try:
            df_monthly['Mese'] = pd.to_datetime(df_monthly['month'], format='%Y-%m').dt.strftime('%b %Y')
        except:
            df_monthly['Mese'] = df_monthly['month']
            
        df_monthly['Quantità'] = df_monthly['monthly_quantity'].apply(lambda x: f"{quantize(x):,.2f}".replace(",", "X").replace(".", ",").replace("X", ".") if x.is_finite() else '0,00')
        df_monthly['Valore'] = df_monthly['monthly_value'].apply(lambda x: f"{quantize(x):,.2f}€")
        df_monthly['Prezzo Medio'] = df_monthly['avg_unit_price'].apply(lambda x: f"{quantize(x):,.2f}€" if x.is_finite() else '0,00€')
        df_monthly['Trend %'] = df_monthly['trend_percent'].apply(lambda x: f"{x:+.1f}%" if pd.notna(x) else 'N/D')
        
        return df_monthly[cols_out]
        
    except Exception as e:
        logger.error(f"Errore vendite mensili '{normalized_description}': {e}", exc_info=True)
        return empty_df
    finally:
        if conn:
            conn.close()

def get_product_sales_comparison(normalized_description, year1, year2):
    """Confronto vendite prodotto tra due anni con analisi dettagliata"""
    conn = None
    qty_col1, val_col1 = f'Q.tà {year1}', f'Valore {year1}'
    qty_col2, val_col2 = f'Q.tà {year2}', f'Valore {year2}'
    cols_final = ['Mese', qty_col1, val_col1, qty_col2, val_col2, 'Var. Valore %', 'Var. Q.tà %']
    empty_df = pd.DataFrame(columns=cols_final)

    if not normalized_description or not year1 or not year2:
        return empty_df

    try:
        conn = get_connection()
        
        query = """
            SELECT 
                strftime('%m', i.doc_date) AS month_num,
                strftime('%Y', i.doc_date) AS year,
                il.description,
                il.quantity,
                il.total_price
            FROM InvoiceLines il 
            JOIN Invoices i ON il.invoice_id = i.id
            WHERE i.type = 'Attiva'
              AND strftime('%Y', i.doc_date) IN (?, ?)
              AND il.description IS NOT NULL AND TRIM(il.description) != '';
        """
        
        df = pd.read_sql_query(query, conn, params=(str(year1), str(year2)))

        if df.empty:
            return empty_df

        df['normalized_product'] = df['description'].apply(normalize_product_name)
        df_filtered = df[df['normalized_product'] == normalized_description].copy()

        if df_filtered.empty:
            return empty_df

        # Converti a Decimal
        df_filtered['quantity_dec'] = df_filtered['quantity'].apply(lambda x: to_decimal(x, default='NaN'))
        df_filtered['total_price_dec'] = df_filtered['total_price'].apply(lambda x: to_decimal(x, default='NaN'))

        df_agg = df_filtered.groupby(['year', 'month_num']).agg(
            total_quantity=('quantity_dec', lambda x: x.dropna().sum()),
            total_value=('total_price_dec', lambda x: x.dropna().sum())
        ).reset_index()

        df_pivot_qty = df_agg.pivot_table(index='month_num', columns='year', values='total_quantity', fill_value=Decimal('0.0'))
        df_pivot_val = df_agg.pivot_table(index='month_num', columns='year', values='total_value', fill_value=Decimal('0.0'))

        df_pivot_qty.rename(columns={str(year1): qty_col1, str(year2): qty_col2}, inplace=True)
        df_pivot_val.rename(columns={str(year1): val_col1, str(year2): val_col2}, inplace=True)

        df_comparison = pd.merge(df_pivot_qty, df_pivot_val, left_index=True, right_index=True, how='outer').fillna(Decimal('0.0'))

        # Assicura che tutte le colonne esistano
        for col in [qty_col1, val_col1, qty_col2, val_col2]:
            if col not in df_comparison.columns:
                df_comparison[col] = Decimal('0.0')

        # Calcola variazioni percentuali prima di formattare
        df_comparison['var_valore'] = np.where(
            df_comparison[val_col1] > 0,
            ((df_comparison[val_col2] - df_comparison[val_col1]) / df_comparison[val_col1]) * 100,
            np.nan
        )
        
        df_comparison['var_quantita'] = np.where(
            df_comparison[qty_col1] > 0,
            ((df_comparison[qty_col2] - df_comparison[qty_col1]) / df_comparison[qty_col1]) * 100,
            np.nan
        )

        # Formatta le colonne numeriche
        for col in [qty_col1, qty_col2]:
            df_comparison[col] = df_comparison[col].apply(lambda x: f"{quantize(x):,.2f}".replace(",", "X").replace(".", ",").replace("X", ".") if x.is_finite() else '0,00')
        for col in [val_col1, val_col2]:
            df_comparison[col] = df_comparison[col].apply(lambda x: f"{quantize(x):,.2f}€")

        df_comparison['Var. Valore %'] = df_comparison['var_valore'].apply(lambda x: f"{x:+.1f}%" if pd.notna(x) else 'N/D')
        df_comparison['Var. Q.tà %'] = df_comparison['var_quantita'].apply(lambda x: f"{x:+.1f}%" if pd.notna(x) else 'N/D')

        # Mappa mesi
        month_map = {'01': 'Gen', '02': 'Feb', '03': 'Mar', '04': 'Apr', '05': 'Mag', '06': 'Giu', 
                    '07': 'Lug', '08': 'Ago', '09': 'Set', '10': 'Ott', '11': 'Nov', '12': 'Dic'}
        df_comparison['Mese'] = df_comparison.index.map(month_map)
        
        # Ordina per mese
        df_comparison['month_num_int'] = df_comparison.index.astype(int)
        df_comparison = df_comparison.sort_values(by='month_num_int').drop(columns=['month_num_int', 'var_valore', 'var_quantita'])

        return df_comparison[cols_final]

    except Exception as e:
        logger.error(f"Errore comparazione vendite '{normalized_description}' ({year1} vs {year2}): {e}", exc_info=True)
        return empty_df
    finally:
        if conn:
            conn.close()

def get_aging_summary(invoice_type='Attiva'):
    """Aging analysis con classificazione migliorata"""
    conn = None
    today = date.today()
    
    # Buckets più granulari per business frutta e verdura
    buckets = {
        'Non Scaduto': 0, 
        '1-7 gg': 7, 
        '8-15 gg': 15,
        '16-30 gg': 30, 
        '31-60 gg': 60, 
        '61-90 gg': 90, 
        '>90 gg': float('inf')
    }
    
    aging_summary = {label: {'amount': Decimal('0.0'), 'count': 0} for label in buckets}

    try:
        conn = get_connection()
        
        query = """
            SELECT id, due_date, total_amount, paid_amount, doc_number, anagraphics_id
            FROM Invoices
            WHERE type = ? AND payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.')
        """
        
        cursor = conn.cursor()
        cursor.execute(query, (invoice_type,))
        rows = cursor.fetchall()

        for row in rows:
            total_dec = to_decimal(row['total_amount'])
            paid_dec = to_decimal(row['paid_amount'])
            open_amount = quantize(total_dec - paid_dec)

            if open_amount.copy_abs() <= AMOUNT_TOLERANCE / 2:
                continue

            days_overdue = -1
            due_date_value = row['due_date']

            if due_date_value:
                try:
                    if isinstance(due_date_value, str):
                        due_date_obj = date.fromisoformat(due_date_value)
                    elif isinstance(due_date_value, date):
                        due_date_obj = due_date_value
                    elif isinstance(due_date_value, datetime):
                        due_date_obj = due_date_value.date()
                    else:
                        pd_date = pd.to_datetime(due_date_value, errors='coerce')
                        if pd.notna(pd_date):
                            due_date_obj = pd_date.date()
                        else:
                            continue

                    if due_date_obj and due_date_obj <= today:
                        days_overdue = (today - due_date_obj).days

                except (ValueError, TypeError) as date_err:
                    logger.warning(f"Errore conversione data scadenza per aging (ID: {row['id']}): {date_err}")
                    continue

            # Assegna al bucket appropriato
            assigned_bucket = None
            if days_overdue < 0:
                assigned_bucket = 'Non Scaduto'
            else:
                for label, upper_limit in buckets.items():
                    if label != 'Non Scaduto' and days_overdue <= upper_limit:
                        assigned_bucket = label
                        break

            if assigned_bucket:
                aging_summary[assigned_bucket]['amount'] += open_amount
                aging_summary[assigned_bucket]['count'] += 1

        return aging_summary
        
    except Exception as e:
        logging.error(f"Errore calcolo aging ({invoice_type}): {e}", exc_info=True)
        return {}
    finally:
        if conn:
            conn.close()

def get_top_suppliers_by_cost(start_date=None, end_date=None, limit=20):
    """Top fornitori con analisi performance"""
    conn = None
    cols_out = ['ID', 'Denominazione', 'Costo Totale', 'N. Fatture', 'Ticket Medio', 'Giorni Medi Pag.', 'Affidabilità']
    empty_df = pd.DataFrame(columns=cols_out)
    
    try:
        conn = get_connection()
        end_date_obj = pd.to_datetime(end_date, errors='coerce').date() if end_date else date.today()
        start_date_obj = pd.to_datetime(start_date, errors='coerce').date() if start_date else (end_date_obj - relativedelta(years=1))

        if pd.isna(start_date_obj) or pd.isna(end_date_obj):
            return empty_df

        start_str, end_str = start_date_obj.isoformat(), end_date_obj.isoformat()
        
        query = """
            SELECT 
                a.id, 
                a.denomination AS Denominazione, 
                SUM(i.total_amount) AS CostoTotale,
                COUNT(i.id) AS NumeroFatture,
                AVG(i.total_amount) AS TicketMedio,
                AVG(CASE 
                    WHEN i.payment_status = 'Pagata Tot.' AND i.due_date IS NOT NULL 
                    THEN julianday(
                        (SELECT MAX(bt.transaction_date) FROM BankTransactions bt 
                         JOIN ReconciliationLinks rl ON bt.id = rl.transaction_id 
                         WHERE rl.invoice_id = i.id)
                    ) - julianday(i.due_date)
                    ELSE NULL 
                END) AS GiorniMediPagamento,
                COUNT(CASE WHEN i.payment_status = 'Pagata Tot.' THEN 1 END) * 100.0 / COUNT(i.id) AS PercentualePagata
            FROM Invoices i 
            JOIN Anagraphics a ON i.anagraphics_id = a.id
            WHERE i.type = 'Passiva' AND i.doc_date BETWEEN ? AND ?
            GROUP BY a.id, a.denomination 
            ORDER BY CostoTotale DESC LIMIT ?;
        """
        
        df = pd.read_sql_query(query, conn, params=(start_str, end_str, limit))

        if df.empty:
            return empty_df

        df['Costo Totale'] = df['CostoTotale'].apply(lambda x: f"{quantize(to_decimal(x)):,.2f}€")
        df['N. Fatture'] = df['NumeroFatture']
        df['Ticket Medio'] = df['TicketMedio'].apply(lambda x: f"{quantize(to_decimal(x)):,.2f}€")
        df['Giorni Medi Pag.'] = df['GiorniMediPagamento'].apply(lambda x: f"{x:.0f}" if pd.notna(x) else 'N/D')
        
        # Score affidabilità basato su % pagamento e puntualità
        df['Affidabilità'] = df.apply(lambda row: 
            'Eccellente' if row['PercentualePagata'] >= 95 and (pd.isna(row['GiorniMediPagamento']) or row['GiorniMediPagamento'] <= 0)
            else 'Buona' if row['PercentualePagata'] >= 80
            else 'Media' if row['PercentualePagata'] >= 60
            else 'Scarsa', axis=1)
        
        df['ID'] = df['id']
        
        return df[cols_out]
        
    except Exception as e:
        logging.error(f"Errore top fornitori: {e}", exc_info=True)
        return empty_df
    finally:
        if conn:
            conn.close()

def get_dashboard_kpis():
    """KPI dashboard con metriche specifiche per ingrosso frutta e verdura"""
    conn = None
    kpis = {
        'total_receivables': Decimal('0.0'),
        'total_payables': Decimal('0.0'),
        'overdue_receivables_count': 0,
        'overdue_receivables_amount': Decimal('0.0'),
        'overdue_payables_count': 0,
        'overdue_payables_amount': Decimal('0.0'),
        'revenue_ytd': Decimal('0.0'),
        'revenue_prev_year_ytd': Decimal('0.0'),
        'revenue_yoy_change_ytd': None,
        'gross_margin_ytd': Decimal('0.0'),
        'margin_percent_ytd': None,
        'avg_days_to_payment': None,
        'inventory_turnover_estimate': None,
        'active_customers_month': 0,
        'new_customers_month': 0
    }
    
    today = date.today()
    today_str = today.isoformat()
    start_of_year = date(today.year, 1, 1).isoformat()
    start_of_prev_year_period = date(today.year - 1, 1, 1).isoformat()
    end_of_prev_year_period = date(today.year - 1, today.month, today.day).isoformat()
    start_of_month = date(today.year, today.month, 1).isoformat()

    try:
        conn = get_connection()
        cursor = conn.cursor()

        # KPI esistenti - crediti e debiti aperti
        cursor.execute("""
            SELECT type, SUM(total_amount - paid_amount) as open_balance
            FROM Invoices
            WHERE payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.')
            GROUP BY type;
        """)
        for row in cursor.fetchall():
            open_bal = quantize(to_decimal(row['open_balance']))
            if row['type'] == 'Attiva':
                kpis['total_receivables'] = open_bal
            elif row['type'] == 'Passiva':
                kpis['total_payables'] = open_bal

        # Scaduti
        cursor.execute("""
            SELECT type,
                   SUM(CASE WHEN due_date < ? THEN (total_amount - paid_amount) ELSE 0 END) as overdue_amount,
                   SUM(CASE WHEN due_date < ? THEN 1 ELSE 0 END) as overdue_count
            FROM Invoices
            WHERE payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.')
              AND due_date IS NOT NULL
            GROUP BY type;
        """, (today_str, today_str))
        for row in cursor.fetchall():
            overdue_bal = quantize(to_decimal(row['overdue_amount']))
            overdue_count = row['overdue_count'] if row['overdue_count'] else 0
            if row['type'] == 'Attiva':
                kpis['overdue_receivables_count'] = overdue_count
                kpis['overdue_receivables_amount'] = overdue_bal
            elif row['type'] == 'Passiva':
                kpis['overdue_payables_count'] = overdue_count
                kpis['overdue_payables_amount'] = overdue_bal

        # Revenue YTD
        cursor.execute("""
            SELECT SUM(total_amount) as revenue_ytd
            FROM Invoices
            WHERE type = 'Attiva' AND doc_date >= ? AND doc_date <= ?
        """, (start_of_year, today_str))
        ytd_data = cursor.fetchone()
        if ytd_data and ytd_data['revenue_ytd'] is not None:
            kpis['revenue_ytd'] = quantize(to_decimal(ytd_data['revenue_ytd']))

        # Revenue anno precedente
        cursor.execute("""
            SELECT SUM(total_amount) as revenue_prev_year_ytd
            FROM Invoices
            WHERE type = 'Attiva' AND doc_date BETWEEN ? AND ?
        """, (start_of_prev_year_period, end_of_prev_year_period))
        prev_ytd_data = cursor.fetchone()
        if prev_ytd_data and prev_ytd_data['revenue_prev_year_ytd'] is not None:
            kpis['revenue_prev_year_ytd'] = quantize(to_decimal(prev_ytd_data['revenue_prev_year_ytd']))

        # Calcola YoY change
        if kpis['revenue_prev_year_ytd'] != Decimal('0.0'):
            try:
                change = ((kpis['revenue_ytd'] - kpis['revenue_prev_year_ytd']) / kpis['revenue_prev_year_ytd']) * 100
                kpis['revenue_yoy_change_ytd'] = round(float(change), 1)
            except Exception:
                kpis['revenue_yoy_change_ytd'] = None

        # Costi YTD per calcolare margine
        cursor.execute("""
            SELECT SUM(total_amount) as costs_ytd
            FROM Invoices
            WHERE type = 'Passiva' AND doc_date >= ? AND doc_date <= ?
        """, (start_of_year, today_str))
        costs_data = cursor.fetchone()
        if costs_data and costs_data['costs_ytd'] is not None:
            costs_ytd = quantize(to_decimal(costs_data['costs_ytd']))
            kpis['gross_margin_ytd'] = kpis['revenue_ytd'] - costs_ytd
            if kpis['revenue_ytd'] > 0:
                kpis['margin_percent_ytd'] = round(float((kpis['gross_margin_ytd'] / kpis['revenue_ytd']) * 100), 1)

        # Giorni medi di pagamento - QUERY CORRETTA
        cursor.execute("""
            SELECT AVG(payment_delay) as avg_payment_days
            FROM (
                SELECT 
                    i.id,
                    julianday(MAX(bt.transaction_date)) - julianday(i.due_date) as payment_delay
                FROM Invoices i
                JOIN ReconciliationLinks rl ON i.id = rl.invoice_id
                JOIN BankTransactions bt ON rl.transaction_id = bt.id
                WHERE i.type = 'Attiva' 
                  AND i.payment_status = 'Pagata Tot.'
                  AND i.due_date IS NOT NULL
                  AND i.doc_date >= ?
                GROUP BY i.id, i.due_date
            )
        """, (start_of_year,))
        payment_days_data = cursor.fetchone()
        if payment_days_data and payment_days_data['avg_payment_days'] is not None:
            kpis['avg_days_to_payment'] = round(payment_days_data['avg_payment_days'], 1)

        # Stima rotazione inventario (vendite/acquisti ratio)
        cursor.execute("""
            SELECT 
                SUM(CASE WHEN i.type = 'Attiva' THEN il.quantity ELSE 0 END) as total_sold,
                SUM(CASE WHEN i.type = 'Passiva' THEN il.quantity ELSE 0 END) as total_purchased
            FROM InvoiceLines il
            JOIN Invoices i ON il.invoice_id = i.id
            WHERE i.doc_date >= ?
              AND il.quantity > 0
        """, (start_of_year,))
        inventory_data = cursor.fetchone()
        if inventory_data and inventory_data['total_purchased'] and inventory_data['total_purchased'] > 0:
            turnover = inventory_data['total_sold'] / inventory_data['total_purchased']
            kpis['inventory_turnover_estimate'] = round(turnover, 2)

        # Clienti attivi nel mese corrente
        cursor.execute("""
            SELECT COUNT(DISTINCT anagraphics_id) as active_customers
            FROM Invoices
            WHERE type = 'Attiva' 
              AND doc_date >= ?
              AND doc_date <= ?
        """, (start_of_month, today_str))
        active_data = cursor.fetchone()
        if active_data:
            kpis['active_customers_month'] = active_data['active_customers'] or 0

        # Nuovi clienti nel mese (prima fattura)
        cursor.execute("""
            SELECT COUNT(*) as new_customers
            FROM (
                SELECT anagraphics_id, MIN(doc_date) as first_invoice
                FROM Invoices
                WHERE type = 'Attiva'
                GROUP BY anagraphics_id
                HAVING first_invoice >= ? AND first_invoice <= ?
            )
        """, (start_of_month, today_str))
        new_data = cursor.fetchone()
        if new_data:
            kpis['new_customers_month'] = new_data['new_customers'] or 0

        logger.debug(f"KPI Dashboard calcolati: {kpis}")
        return kpis
        
    except Exception as e:
        logger.error(f"Errore calcolo KPI dashboard: {e}", exc_info=True)
        return kpis
    finally:
        if conn:
            conn.close()

def get_top_overdue_invoices(limit=10):
    """Top fatture scadute con priorità business"""
    conn = None
    today_py = date.today()
    cols_out = ['id', 'type', 'doc_number', 'due_date_fmt', 'counterparty_name', 'open_amount_fmt', 'days_overdue', 'priority']
    empty_df = pd.DataFrame(columns=cols_out)
    
    try:
        conn = get_connection()
        
        query = """
            SELECT
                i.id, i.type, i.doc_number, i.due_date,
                a.denomination AS counterparty_name,
                (i.total_amount - i.paid_amount) AS open_amount_raw,
                a.score as customer_score
            FROM Invoices i 
            JOIN Anagraphics a ON i.anagraphics_id = a.id
            WHERE i.type = 'Attiva'
              AND i.payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.')
              AND i.due_date IS NOT NULL 
              AND date(i.due_date) < date(?)
            ORDER BY date(i.due_date) ASC
            LIMIT ?;
        """
        
        df = pd.read_sql_query(query, conn, params=(today_py.isoformat(), limit * 2), parse_dates=['due_date'])

        if df is None or df.empty:
            return empty_df

        df['open_amount_dec'] = df['open_amount_raw'].apply(to_decimal).apply(quantize)
        df = df[df['open_amount_dec'].abs() > AMOUNT_TOLERANCE / 2].copy()
        
        if df.empty:
            return empty_df

        df['due_date_fmt'] = df['due_date'].dt.strftime('%d/%m/%Y')
        df['days_overdue'] = (today_py - df['due_date'].dt.date).apply(lambda x: x.days if pd.notna(x) else pd.NA)
        df['open_amount_fmt'] = df['open_amount_dec'].apply(lambda x: f"{x:,.2f}€")

        # Calcola priorità basata su importo, giorni di ritardo e score cliente
        df['priority_score'] = (
            (df['open_amount_dec'].astype(float) / 1000) * 0.4 +  # Peso importo
            (df['days_overdue'].fillna(0) / 10) * 0.4 +  # Peso giorni ritardo
            ((100 - df['customer_score'].fillna(50)) / 20) * 0.2  # Peso score cliente (inverso)
        )

        # Classifica priorità
        df['priority'] = pd.cut(
            df['priority_score'],
            bins=[0, 2, 5, 10, float('inf')],
            labels=['Bassa', 'Media', 'Alta', 'Critica']
        )

        # Ordina per priorità e giorni di ritardo
        df_sorted = df.sort_values(['priority_score'], ascending=False).head(limit)

        return df_sorted[cols_out]
        
    except Exception as e:
        logger.error(f"Errore recupero top fatture scadute: {e}", exc_info=True)
        return empty_df
    finally:
        if conn:
            conn.close()

def get_due_dates_in_month(year, month):
    """Date con scadenze nel mese per calendario"""
    conn = None
    try:
        conn = get_connection()
        start_date = date(year, month, 1)
        end_date = start_date + relativedelta(months=1) - timedelta(days=1)
        start_str, end_str = start_date.isoformat(), end_date.isoformat()
        
        query = """
            SELECT DISTINCT date(due_date) as due_day
            FROM Invoices
            WHERE payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.')
              AND due_date BETWEEN ? AND ?
            ORDER BY due_day;
        """
        
        cursor = conn.cursor()
        cursor.execute(query, (start_str, end_str))
        dates = [date.fromisoformat(row['due_day']) for row in cursor.fetchall() if row['due_day']]
        return dates
        
    except Exception as e:
        logger.error(f"Errore recupero date scadenza per {year}-{month}: {e}", exc_info=True)
        return []
    finally:
        if conn:
            conn.close()

def get_invoices_due_on_date(due_date_py):
    """Fatture in scadenza in una data specifica"""
    conn = None
    today_py = date.today()
    cols_out = ['id', 'type', 'doc_number', 'counterparty_name', 'open_amount_fmt', 'days_overdue']
    empty_df = pd.DataFrame(columns=cols_out)
    
    try:
        conn = get_connection()
        due_date_str = due_date_py.isoformat()
        
        query = """
            SELECT
                i.id, i.type, i.doc_number, a.denomination AS counterparty_name,
                (i.total_amount - i.paid_amount) AS open_amount_raw,
                i.due_date
            FROM Invoices i 
            JOIN Anagraphics a ON i.anagraphics_id = a.id
            WHERE date(i.due_date) = date(?)
              AND i.payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.')
            ORDER BY a.denomination COLLATE NOCASE, i.type;
        """
        
        df = pd.read_sql_query(query, conn, params=(due_date_str,), parse_dates=['due_date'])

        if df is None or df.empty:
            return empty_df

        df['open_amount_dec'] = df['open_amount_raw'].apply(to_decimal).apply(quantize)
        df = df[df['open_amount_dec'].abs() > AMOUNT_TOLERANCE / 2].copy()
        
        if df.empty:
            return empty_df

        df['days_overdue'] = (today_py - df['due_date'].dt.date).apply(lambda x: x.days if pd.notna(x) else pd.NA)
        df['open_amount_fmt'] = df['open_amount_dec'].apply(lambda x: f"{x:,.2f}€")

        return df[cols_out]
        
    except Exception as e:
        logger.error(f"Errore recupero fatture per scadenza {due_date_str}: {e}", exc_info=True)
        return empty_df
    finally:
        if conn:
            conn.close()

def get_anagraphic_financial_summary(anagraphic_id):
    """Sommario finanziario anagrafica con metriche avanzate"""
    summary = {
        'total_invoiced': Decimal('0.0'),
        'total_paid': Decimal('0.0'),
        'open_balance': Decimal('0.0'),
        'overdue_amount': Decimal('0.0'),
        'overdue_count': 0,
        'avg_payment_days': None,
        'avg_order_value_ytd': None,
        'avg_order_value_prev_year': None,
        'revenue_ytd': Decimal('0.0'),
        'revenue_prev_year': Decimal('0.0'),
        'revenue_yoy_change': None,
        'order_frequency': None,
        'last_order_date': None,
        'customer_segment': None,
        'preferred_products': []
    }
    
    conn = None
    today = date.today()
    today_str = today.isoformat()
    start_of_year = date(today.year, 1, 1).isoformat()
    start_of_prev_year = date(today.year - 1, 1, 1).isoformat()
    end_of_prev_year = date(today.year - 1, 12, 31).isoformat()

    if anagraphic_id is None:
        return summary

    try:
        conn = get_connection()
        cursor = conn.cursor()

        # Dati base
        cursor.execute("""
            SELECT
                SUM(CASE WHEN type = 'Attiva' THEN total_amount ELSE 0 END) as total_revenue,
                SUM(CASE WHEN type = 'Passiva' THEN total_amount ELSE 0 END) as total_cost,
                SUM(CASE WHEN type = 'Attiva' THEN paid_amount ELSE 0 END) as total_received,
                COUNT(CASE WHEN type = 'Attiva' THEN 1 END) as total_invoices,
                MAX(CASE WHEN type = 'Attiva' THEN doc_date END) as last_order_date
            FROM Invoices
            WHERE anagraphics_id = ?
        """, (anagraphic_id,))
        
        totals = cursor.fetchone()
        if totals:
            summary['total_invoiced'] = quantize(to_decimal(totals['total_revenue'])) if totals['total_revenue'] else Decimal('0.0')
            summary['total_paid'] = quantize(to_decimal(totals['total_received'])) if totals['total_received'] else Decimal('0.0')
            summary['open_balance'] = summary['total_invoiced'] - summary['total_paid']
            summary['last_order_date'] = totals['last_order_date']
            
            # Frequenza ordini (ordini per mese negli ultimi 12 mesi)
            if totals['total_invoices'] and totals['last_order_date']:
                last_order = pd.to_datetime(totals['last_order_date']).date()
                months_active = max(1, (today - last_order).days / 30)
                summary['order_frequency'] = round(totals['total_invoices'] / months_active, 2)

        # Scaduti
        cursor.execute("""
            SELECT 
                SUM(total_amount - paid_amount) as overdue_amount,
                COUNT(*) as overdue_count
            FROM Invoices
            WHERE anagraphics_id = ?
              AND type = 'Attiva'
              AND payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.')
              AND due_date < ?
        """, (anagraphic_id, today_str))
        
        overdue_data = cursor.fetchone()
        if overdue_data:
            summary['overdue_amount'] = quantize(to_decimal(overdue_data['overdue_amount'])) if overdue_data['overdue_amount'] else Decimal('0.0')
            summary['overdue_count'] = overdue_data['overdue_count'] or 0

        # YTD e anno precedente
        cursor.execute("""
            SELECT 
                SUM(CASE WHEN doc_date >= ? AND doc_date <= ? THEN total_amount ELSE 0 END) as revenue_ytd,
                COUNT(CASE WHEN doc_date >= ? AND doc_date <= ? THEN 1 END) as count_ytd,
                SUM(CASE WHEN doc_date BETWEEN ? AND ? THEN total_amount ELSE 0 END) as revenue_prev_year,
                COUNT(CASE WHEN doc_date BETWEEN ? AND ? THEN 1 END) as count_prev_year
            FROM Invoices
            WHERE anagraphics_id = ? AND type = 'Attiva'
        """, (start_of_year, today_str, start_of_year, today_str, 
              start_of_prev_year, end_of_prev_year, start_of_prev_year, end_of_prev_year, anagraphic_id))
        
        yearly_data = cursor.fetchone()
        if yearly_data:
            summary['revenue_ytd'] = quantize(to_decimal(yearly_data['revenue_ytd'])) if yearly_data['revenue_ytd'] else Decimal('0.0')
            summary['revenue_prev_year'] = quantize(to_decimal(yearly_data['revenue_prev_year'])) if yearly_data['revenue_prev_year'] else Decimal('0.0')
            
            if yearly_data['count_ytd'] and yearly_data['count_ytd'] > 0:
                summary['avg_order_value_ytd'] = quantize(summary['revenue_ytd'] / Decimal(yearly_data['count_ytd']))
            if yearly_data['count_prev_year'] and yearly_data['count_prev_year'] > 0:
                summary['avg_order_value_prev_year'] = quantize(summary['revenue_prev_year'] / Decimal(yearly_data['count_prev_year']))
            
            # YoY change
            if summary['revenue_prev_year'] > 0:
                change = ((summary['revenue_ytd'] - summary['revenue_prev_year']) / summary['revenue_prev_year']) * 100
                summary['revenue_yoy_change'] = round(float(change), 1)

        # Giorni medi di pagamento dallo score
        cursor.execute("SELECT score FROM Anagraphics WHERE id = ?", (anagraphic_id,))
        score_row = cursor.fetchone()
        if score_row and score_row['score'] is not None and score_row['score'] < 100:
            summary['avg_payment_days'] = round((100.0 - score_row['score']) / 1.5)

        # Segmentazione cliente basata su revenue YTD
        revenue_ytd_float = float(summary['revenue_ytd'])
        if revenue_ytd_float >= 100000:
            summary['customer_segment'] = 'Platinum'
        elif revenue_ytd_float >= 50000:
            summary['customer_segment'] = 'Gold'
        elif revenue_ytd_float >= 10000:
            summary['customer_segment'] = 'Silver'
        elif revenue_ytd_float > 0:
            summary['customer_segment'] = 'Bronze'
        else:
            summary['customer_segment'] = 'Inactive'

        # Top 3 prodotti preferiti (ultimi 6 mesi)
        six_months_ago = (today - relativedelta(months=6)).isoformat()
        cursor.execute("""
            SELECT 
                il.description,
                SUM(il.total_price) as total_value,
                SUM(il.quantity) as total_quantity
            FROM InvoiceLines il
            JOIN Invoices i ON il.invoice_id = i.id
            WHERE i.anagraphics_id = ?
              AND i.type = 'Attiva'
              AND i.doc_date >= ?
              AND il.description IS NOT NULL
            GROUP BY il.description
            ORDER BY total_value DESC
            LIMIT 3
        """, (anagraphic_id, six_months_ago))
        
        preferred_products = cursor.fetchall()
        summary['preferred_products'] = [
            {
                'product': row['description'],
                'value': f"{quantize(to_decimal(row['total_value'])):,.2f}€",
                'quantity': f"{quantize(to_decimal(row['total_quantity'])):,.2f}"
            }
            for row in preferred_products
        ]

    except Exception as e:
        logger.error(f"Errore sommario finanziario per ID {anagraphic_id}: {e}", exc_info=True)
    finally:
        if conn:
            conn.close()

    return summary

# ===== FUNZIONI DI SUPPORTO PER BUSINESS INTELLIGENCE =====

def get_business_insights_summary(start_date=None, end_date=None):
    """Riepilogo insights business per dashboard executive"""
    insights = {
        'seasonal_trends': {},
        'top_performing_products': [],
        'cash_flow_health': '',
        'customer_retention': 0.0,
        'inventory_alerts': [],
        'profitability_trend': '',
        'recommendations': []
    }
    
    try:
        # Analisi stagionalità
        seasonal_data = get_seasonal_product_analysis('all', 2)
        if not seasonal_data.empty:
            # Identifica mese con maggiori vendite
            monthly_sales = seasonal_data.groupby('month_num')['total_value'].sum()
            peak_month = monthly_sales.idxmax()
            insights['seasonal_trends']['peak_month'] = peak_month
            insights['seasonal_trends']['peak_value'] = f"{monthly_sales[peak_month]:,.2f}€"
        
        # Cash flow health
        cashflow_data = get_cashflow_data(start_date, end_date)
        if not cashflow_data.empty:
            avg_net_flow = cashflow_data['net_cash_flow'].mean()
            if avg_net_flow > 10000:
                insights['cash_flow_health'] = 'Eccellente'
            elif avg_net_flow > 5000:
                insights['cash_flow_health'] = 'Buono'
            elif avg_net_flow > 0:
                insights['cash_flow_health'] = 'Stabile'
            else:
                insights['cash_flow_health'] = 'Critico'
        
        # Prodotti top performance
        products_analysis = get_products_analysis('Attiva', start_date, end_date)
        if not products_analysis.empty:
            insights['top_performing_products'] = products_analysis.head(5)['Prodotto Normalizzato'].tolist()
        
        # Raccomandazioni basate sui dati
        recommendations = []
        
        # Raccomandazione su stagionalità
        if 'peak_month' in insights['seasonal_trends']:
            peak = insights['seasonal_trends']['peak_month']
            if peak in ['11', '12', '01']:  # Inverno
                recommendations.append("Incrementa stock agrumi e verdure invernali per il periodo di picco")
            elif peak in ['06', '07', '08']:  # Estate
                recommendations.append("Pianifica maggiori approvvigionamenti frutta estiva")
        
        # Raccomandazione su cash flow
        if insights['cash_flow_health'] == 'Critico':
            recommendations.append("Rivedere termini di pagamento clienti e ottimizzare incassi")
        
        insights['recommendations'] = recommendations
        
        return insights
        
    except Exception as e:
        logger.error(f"Errore generazione business insights: {e}", exc_info=True)
        return insights

def export_analysis_to_excel(analysis_type, data, filename=None):
    """Esporta analisi in formato Excel per condivisione"""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"analisi_{analysis_type}_{timestamp}.xlsx"
    
    try:
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            if isinstance(data, dict):
                for sheet_name, df in data.items():
                    if isinstance(df, pd.DataFrame) and not df.empty:
                        df.to_excel(writer, sheet_name=sheet_name[:31], index=False)  # Max 31 chars per sheet name
            elif isinstance(data, pd.DataFrame) and not data.empty:
                data.to_excel(writer, sheet_name='Analisi', index=False)
        
        logger.info(f"Analisi esportata in: {filename}")
        return filename
        
    except Exception as e:
        logger.error(f"Errore esportazione Excel: {e}", exc_info=True)
        return None

# ===== FUNZIONI ORIGINALI MANTENUTE E MIGLIORATE =====

def _get_categorized_transactions(start_date_str, end_date_str):
    """Categorizzazione transazioni migliorata per business frutta e verdura"""
    conn = None
    try:
        conn = get_connection()
        query_trans = """
            SELECT
                bt.id as transaction_id,
                bt.transaction_date,
                bt.amount,
                bt.description,
                GROUP_CONCAT(rl.id) as link_ids,
                GROUP_CONCAT(i.type) as linked_invoice_types,
                SUM(rl.reconciled_amount) as total_linked_amount
            FROM BankTransactions bt
            LEFT JOIN ReconciliationLinks rl ON bt.id = rl.transaction_id
            LEFT JOIN Invoices i ON rl.invoice_id = i.id
            WHERE bt.transaction_date BETWEEN ? AND ?
              AND bt.reconciliation_status != 'Ignorato'
            GROUP BY bt.id, bt.transaction_date, bt.amount, bt.description
            ORDER BY bt.transaction_date;
        """
        df = pd.read_sql_query(query_trans, conn, params=(start_date_str, end_date_str), parse_dates=['transaction_date'])

        if df.empty:
            logger.info("Nessuna transazione trovata nel periodo per la categorizzazione.")
            return pd.DataFrame(columns=['transaction_id', 'transaction_date', 'amount', 'description', 'category', 'amount_dec'])

        df['amount_dec'] = df['amount'].apply(lambda x: quantize(to_decimal(x)))
        df['category'] = 'Altro'

        # Pattern migliorati per business frutta e verdura
        pos_pattern = r'\b(POS|PAGOBANCOMAT|CIRRUS|MAESTRO|VISA|MASTERCARD|AMEX|WORLDLINE|ESE COMM)\b'
        cash_pattern = r'(VERSAMENTO CONTANT|CONTANTI|CASSA)'
        commission_pattern = r'^(COMMISSIONI|COMPETENZE BANC|SPESE TENUTA CONTO|IMPOSTA DI BOLLO)'
        fuel_pattern = r'(BENZINA|GASOLIO|CARBURANTE|DISTRIBUTORE|ENI|AGIP|Q8)'
        transport_pattern = r'(AUTOSTRADA|PEDAGGI|TELEPASS|TRASPORT)'
        utilities_pattern = r'(ENEL|GAS|ACQUA|TELEFON|INTERNET|TIM|VODAFONE)'
        taxes_pattern = r'(F24|TRIBUTI|INPS|INAIL|AGENZIA ENTRATE)'
        bank_fees_pattern = r'(CANONE|BOLLO|COMMISSIONI)'

        for index, row in df.iterrows():
            trans_id = row['transaction_id']
            amount = row['amount_dec']
            desc = str(row['description'] or '').upper()
            has_links = pd.notna(row['link_ids'])
            linked_types = set(str(row['linked_invoice_types'] or '').split(',')) if has_links else set()

            if has_links:
                if amount > 0 and 'Attiva' in linked_types:
                    df.loc[index, 'category'] = 'IngrossoIn'
                elif amount < 0 and 'Passiva' in linked_types:
                    df.loc[index, 'category'] = 'IngrossoOut'
                else:
                    df.loc[index, 'category'] = 'Altro'
            else:
                if amount > 0:
                    if re.search(cash_pattern, desc, re.IGNORECASE):
                        df.loc[index, 'category'] = 'DettaglioIn'
                    else:
                        df.loc[index, 'category'] = 'DettaglioIn'
                elif amount < 0:
                    if re.search(commission_pattern, desc, re.IGNORECASE):
                        df.loc[index, 'category'] = 'Commissioni'
                    elif re.search(pos_pattern, desc, re.IGNORECASE):
                        df.loc[index, 'category'] = 'DettaglioOut'
                    else:
                        df.loc[index, 'category'] = 'Altro'

        return df

    except Exception as e:
        logger.error(f"Errore durante la categorizzazione delle transazioni: {e}", exc_info=True)
        return pd.DataFrame(columns=['transaction_id', 'transaction_date', 'amount', 'description', 'category', 'amount_dec'])
    finally:
        if conn: 
            conn.close()

def get_cashflow_data_original(start_date=None, end_date=None):
    """Versione originale del cash flow data mantenuta per compatibilità"""
    cols_out = ['month', 'inflow_ingrosso', 'outflow_ingrosso', 'inflow_dettaglio',
                'outflow_dettaglio', 'outflow_commissioni', 'inflow_altro', 'outflow_altro',
                'balance_ingrosso', 'balance_dettaglio', 'balance_commissioni', 'balance_altro',
                'total_balance']
    empty_df = pd.DataFrame(columns=cols_out).astype(float)
    empty_df['month'] = pd.Series(dtype='str')

    try:
        end_date_obj = pd.to_datetime(end_date, errors='coerce').date() if end_date else date.today()
        start_date_obj = pd.to_datetime(start_date, errors='coerce').date() if start_date else (end_date_obj - relativedelta(years=1) + relativedelta(days=1))

        if pd.isna(start_date_obj) or pd.isna(end_date_obj):
            logger.error("Date inizio/fine non valide per get_cashflow_data")
            return empty_df

        start_str, end_str = start_date_obj.isoformat(), end_date_obj.isoformat()
        df_categorized = _get_categorized_transactions(start_str, end_str)

        if df_categorized.empty:
            logger.info("Nessuna transazione categorizzata trovata per il cash flow.")
            return empty_df

        df_categorized['month'] = df_categorized['transaction_date'].dt.strftime('%Y-%m')
        
        # Somma importi per mese e categoria
        df_monthly = df_categorized.groupby(['month', 'category'])['amount_dec'].sum().unstack(fill_value=Decimal('0.0')).reset_index()

        all_categories = ['IngrossoIn', 'IngrossoOut', 'DettaglioIn', 'DettaglioOut', 'Commissioni', 'Altro']
        for cat in all_categories:
            if cat not in df_monthly.columns:
                df_monthly[cat] = Decimal('0.0')

        df_out = pd.DataFrame()
        df_out['month'] = df_monthly['month']
        df_out['inflow_ingrosso'] = df_monthly['IngrossoIn'].apply(lambda x: x if x > 0 else Decimal('0.0'))
        df_out['outflow_ingrosso'] = df_monthly['IngrossoOut'].apply(lambda x: x.copy_abs() if x < 0 else Decimal('0.0'))
        df_out['inflow_dettaglio'] = df_monthly['DettaglioIn'].apply(lambda x: x if x > 0 else Decimal('0.0'))
        df_out['outflow_dettaglio'] = df_monthly['DettaglioOut'].apply(lambda x: x.copy_abs() if x < 0 else Decimal('0.0'))
        df_out['outflow_commissioni'] = df_monthly['Commissioni'].apply(lambda x: x.copy_abs() if x < 0 else Decimal('0.0'))
        df_out['inflow_altro'] = df_monthly['Altro'].apply(lambda x: x if x > 0 else Decimal('0.0'))
        df_out['outflow_altro'] = df_monthly['Altro'].apply(lambda x: x.copy_abs() if x < 0 else Decimal('0.0'))

        df_out['balance_ingrosso'] = df_out['inflow_ingrosso'] - df_out['outflow_ingrosso']
        df_out['balance_dettaglio'] = df_out['inflow_dettaglio'] - df_out['outflow_dettaglio']
        df_out['balance_commissioni'] = -df_out['outflow_commissioni']
        df_out['balance_altro'] = df_out['inflow_altro'] - df_out['outflow_altro']
        df_out['total_balance'] = df_monthly[all_categories].sum(axis=1)

        # Converti in float alla fine
        for col in cols_out[1:]:
            df_out[col] = df_out[col].astype(float)

        df_out['month'] = pd.to_datetime(df_out['month'], format='%Y-%m').dt.strftime('%Y-%m')

        return df_out[cols_out]

    except Exception as e:
        logger.error(f"Errore recupero dati cash flow categorizzato: {e}", exc_info=True)
        return empty_df

def get_cashflow_table_original(start_date=None, end_date=None):
    """Versione originale della tabella cash flow"""
    cols_out = ['Mese', 'Entrate Ingrosso', 'Uscite Ingrosso', 'Saldo Ingrosso',
                'Entrate Dettaglio', 'Uscite Dettaglio', 'Saldo Dettaglio',
                'Commissioni', 'Saldo Altro', 'Saldo Totale']
    empty_df = pd.DataFrame(columns=cols_out)
    
    try:
        df_data = get_cashflow_data_original(start_date, end_date)
        if df_data.empty: 
            return empty_df

        df_table = pd.DataFrame()
        try:
            df_table['Mese'] = pd.to_datetime(df_data['month'], format='%Y-%m').dt.strftime('%b %Y')
        except ValueError:
            df_table['Mese'] = df_data['month']

        def format_dec_curr(val):
            if pd.isna(val): 
                return 'N/A'
            try:
                d = quantize(Decimal(str(val)))
                return f"{d:,.2f}".replace(",", "X").replace(".", ",").replace("X", ".")
            except (InvalidOperation, TypeError):
                return "Errore"

        df_table['Entrate Ingrosso'] = df_data['inflow_ingrosso'].apply(format_dec_curr)
        df_table['Uscite Ingrosso'] = df_data['outflow_ingrosso'].apply(format_dec_curr)
        df_table['Saldo Ingrosso'] = df_data['balance_ingrosso'].apply(format_dec_curr)
        df_table['Entrate Dettaglio'] = df_data['inflow_dettaglio'].apply(format_dec_curr)
        df_table['Uscite Dettaglio'] = df_data['outflow_dettaglio'].apply(format_dec_curr)
        df_table['Saldo Dettaglio'] = df_data['balance_dettaglio'].apply(format_dec_curr)
        df_table['Commissioni'] = df_data['outflow_commissioni'].apply(format_dec_curr)
        df_table['Saldo Altro'] = df_data['balance_altro'].apply(format_dec_curr)
        df_table['Saldo Totale'] = df_data['total_balance'].apply(format_dec_curr)

        return df_table[cols_out]

    except Exception as e:
        logger.error(f"Errore creazione tabella cash flow categorizzato: {e}", exc_info=True)
        return empty_df

# ===== ANALISI PREDITTIVE E AVANZATE =====

def get_sales_forecast(product_name=None, months_ahead=3):
    """Previsione vendite basata su trend storici"""
    conn = None
    try:
        conn = get_connection()
        
        # Query per dati storici ultimi 24 mesi
        two_years_ago = (date.today() - relativedelta(years=2)).isoformat()
        today_str = date.today().isoformat()
        
        where_clause = ""
        params = [two_years_ago, today_str]
        
        if product_name:
            where_clause = "AND il.description LIKE ?"
            params.append(f"%{product_name}%")

        query = f"""
            SELECT 
                strftime('%Y-%m', i.doc_date) as month,
                il.description,
                SUM(il.quantity) as total_quantity,
                SUM(il.total_price) as total_value,
                AVG(il.unit_price) as avg_price
            FROM InvoiceLines il
            JOIN Invoices i ON il.invoice_id = i.id
            WHERE i.type = 'Attiva' 
              AND i.doc_date BETWEEN ? AND ?
              AND il.quantity > 0
              {where_clause}
            GROUP BY month, il.description
            ORDER BY month
        """
        
        df = pd.read_sql_query(query, conn, params=params)
        
        if df.empty:
            return pd.DataFrame()
        
        # Normalizza prodotti se necessario
        if product_name:
            df['normalized_product'] = df['description'].apply(normalize_product_name)
            df = df[df['normalized_product'].str.contains(product_name, case=False, na=False)]
        
        # Aggrega per mese
        df_monthly = df.groupby('month').agg({
            'total_quantity': 'sum',
            'total_value': 'sum',
            'avg_price': 'mean'
        }).reset_index()
        
        # Conversione a serie temporale
        df_monthly['month_date'] = pd.to_datetime(df_monthly['month'])
        df_monthly = df_monthly.sort_values('month_date')
        
        # Calcolo trend lineare semplice
        if len(df_monthly) >= 6:  # Minimo 6 mesi per forecast
            # Trend quantity
            x_vals = np.arange(len(df_monthly))
            qty_trend = np.polyfit(x_vals, df_monthly['total_quantity'], 1)
            val_trend = np.polyfit(x_vals, df_monthly['total_value'], 1)
            
            # Previsioni per i prossimi mesi
            forecasts = []
            last_month = df_monthly['month_date'].max()
            
            for i in range(1, months_ahead + 1):
                future_month = last_month + relativedelta(months=i)
                future_x = len(df_monthly) + i - 1
                
                pred_qty = qty_trend[0] * future_x + qty_trend[1]
                pred_val = val_trend[0] * future_x + val_trend[1]
                pred_price = pred_val / pred_qty if pred_qty > 0 else 0
                
                forecasts.append({
                    'month': future_month.strftime('%Y-%m'),
                    'month_name': future_month.strftime('%b %Y'),
                    'predicted_quantity': max(0, pred_qty),  # Non negativo
                    'predicted_value': max(0, pred_val),
                    'predicted_avg_price': max(0, pred_price),
                    'confidence': max(0.3, 1.0 - (i * 0.15))  # Confidenza decrescente
                })
            
            return pd.DataFrame(forecasts)
        
        return pd.DataFrame()
        
    except Exception as e:
        logger.error(f"Errore forecast vendite: {e}", exc_info=True)
        return pd.DataFrame()
    finally:
        if conn:
            conn.close()

def get_customer_churn_analysis():
    """Analisi rischio abbandono clienti"""
    conn = None
    try:
        conn = get_connection()
        today = date.today()
        
        query = """
            SELECT 
                a.id,
                a.denomination,
                MAX(i.doc_date) as last_order_date,
                COUNT(i.id) as total_orders,
                SUM(i.total_amount) as total_revenue,
                AVG(i.total_amount) as avg_order_value,
                COUNT(DISTINCT strftime('%Y-%m', i.doc_date)) as active_months
            FROM Anagraphics a
            JOIN Invoices i ON a.id = i.anagraphics_id
            WHERE a.type = 'Cliente'
              AND i.type = 'Attiva'
            GROUP BY a.id, a.denomination
            HAVING total_orders > 0
        """
        
        df = pd.read_sql_query(query, conn, parse_dates=['last_order_date'])
        
        if df.empty:
            return pd.DataFrame()
        
        # Calcola giorni dall'ultimo ordine
        df['days_since_last_order'] = (today - df['last_order_date'].dt.date).dt.days
        
        # Calcola frequenza media ordini (ordini per mese)
        df['avg_order_frequency'] = df['total_orders'] / df['active_months']
        
        # Calcola score di rischio churn
        # Fattori: giorni dall'ultimo ordine, frequenza ordini, valore cliente
        df['churn_risk_score'] = (
            (df['days_since_last_order'] / 30) * 0.5 +  # Mesi senza ordini
            (1 / (df['avg_order_frequency'] + 0.1)) * 0.3 +  # Bassa frequenza
            (1 / (df['total_revenue'] / 1000 + 0.1)) * 0.2  # Basso valore
        )
        
        # Normalizza score 0-100
        max_score = df['churn_risk_score'].max()
        df['churn_risk_score'] = (df['churn_risk_score'] / max_score * 100).round(1)
        
        # Classifica rischio
        df['risk_category'] = pd.cut(
            df['churn_risk_score'],
            bins=[0, 25, 50, 75, 100],
            labels=['Basso', 'Medio', 'Alto', 'Critico']
        )
        
        # Suggerimenti azione
        def get_action_suggestion(row):
            if row['risk_category'] == 'Critico':
                return "Contatto immediato, offerta speciale"
            elif row['risk_category'] == 'Alto':
                return "Chiamata commerciale, verifica soddisfazione"
            elif row['risk_category'] == 'Medio':
                return "Email promozionale, newsletter"
            else:
                return "Mantenimento relazione standard"
        
        df['suggested_action'] = df.apply(get_action_suggestion, axis=1)
        
        # Ordina per rischio decrescente
        return df.sort_values('churn_risk_score', ascending=False)
        
    except Exception as e:
        logger.error(f"Errore analisi churn: {e}", exc_info=True)
        return pd.DataFrame()
    finally:
        if conn:
            conn.close()

def get_payment_behavior_analysis():
    """Analisi comportamento pagamenti per migliorare cash flow"""
    conn = None
    try:
        conn = get_connection()
        
        query = """
            SELECT 
                a.id,
                a.denomination,
                i.due_date,
                i.total_amount,
                MAX(bt.transaction_date) as payment_date,
                julianday(MAX(bt.transaction_date)) - julianday(i.due_date) as payment_delay_days,
                CASE 
                    WHEN strftime('%w', i.due_date) = '0' THEN 'Domenica'
                    WHEN strftime('%w', i.due_date) = '1' THEN 'Lunedì'
                    WHEN strftime('%w', i.due_date) = '2' THEN 'Martedì'
                    WHEN strftime('%w', i.due_date) = '3' THEN 'Mercoledì'
                    WHEN strftime('%w', i.due_date) = '4' THEN 'Giovedì'
                    WHEN strftime('%w', i.due_date) = '5' THEN 'Venerdì'
                    WHEN strftime('%w', i.due_date) = '6' THEN 'Sabato'
                END as due_day_of_week,
                strftime('%m', i.due_date) as due_month
            FROM Invoices i
            JOIN Anagraphics a ON i.anagraphics_id = a.id
            JOIN ReconciliationLinks rl ON i.id = rl.invoice_id
            JOIN BankTransactions bt ON rl.transaction_id = bt.id
            WHERE i.type = 'Attiva'
              AND i.payment_status = 'Pagata Tot.'
              AND i.due_date IS NOT NULL
              AND i.doc_date >= date('now', '-2 years')
        """
        
        df = pd.read_sql_query(query, conn)
        
        if df.empty:
            return {}
        
        # Analisi per cliente
        client_behavior = df.groupby(['id', 'denomination']).agg({
            'payment_delay_days': ['mean', 'std', 'count'],
            'total_amount': 'sum'
        }).round(2)
        
        client_behavior.columns = ['avg_delay', 'delay_std', 'payment_count', 'total_paid']
        client_behavior = client_behavior.reset_index()
        
        # Classifica clienti per comportamento
        client_behavior['behavior_category'] = pd.cut(
            client_behavior['avg_delay'],
            bins=[-float('inf'), -5, 5, 15, float('inf')],
            labels=['Anticipato', 'Puntuale', 'Lieve Ritardo', 'Ritardo Cronico']
        )
        
        # Analisi per giorno della settimana
        day_analysis = df.groupby('due_day_of_week')['payment_delay_days'].agg(['mean', 'count']).round(2)
        
        # Analisi per mese
        month_analysis = df.groupby('due_month')['payment_delay_days'].agg(['mean', 'count']).round(2)
        
        # Correlazione importo vs ritardo
        amount_delay_corr = df['total_amount'].corr(df['payment_delay_days'])
        
        # Statistiche generali
        general_stats = {
            'avg_delay_all': df['payment_delay_days'].mean(),
            'median_delay': df['payment_delay_days'].median(),
            'std_delay': df['payment_delay_days'].std(),
            'on_time_percentage': (df['payment_delay_days'] <= 0).mean() * 100,
            'late_percentage': (df['payment_delay_days'] > 0).mean() * 100,
            'amount_delay_correlation': amount_delay_corr
        }
        
        return {
            'client_behavior': client_behavior,
            'day_analysis': day_analysis,
            'month_analysis': month_analysis,
            'general_stats': general_stats,
            'raw_data': df
        }
        
    except Exception as e:
        logger.error(f"Errore analisi comportamento pagamenti: {e}", exc_info=True)
        return {}
    finally:
        if conn:
            conn.close()

def get_competitive_analysis():
    """Analisi competitiva basata sui dati di acquisto/vendita"""
    conn = None
    try:
        conn = get_connection()
        
        # Analizza differenze di prezzo tra acquisto e vendita per prodotto
        query = """
            SELECT 
                il_buy.description as product,
                AVG(il_buy.unit_price) as avg_purchase_price,
                AVG(il_sell.unit_price) as avg_selling_price,
                COUNT(DISTINCT i_buy.anagraphics_id) as num_suppliers,
                COUNT(DISTINCT i_sell.anagraphics_id) as num_customers,
                SUM(il_buy.quantity) as total_purchased,
                SUM(il_sell.quantity) as total_sold
            FROM InvoiceLines il_buy
            JOIN Invoices i_buy ON il_buy.invoice_id = i_buy.id
            LEFT JOIN InvoiceLines il_sell ON il_buy.description = il_sell.description
            LEFT JOIN Invoices i_sell ON il_sell.invoice_id = i_sell.id AND i_sell.type = 'Attiva'
            WHERE i_buy.type = 'Passiva'
              AND i_buy.doc_date >= date('now', '-1 year')
              AND il_buy.unit_price > 0
              AND il_sell.unit_price > 0
            GROUP BY il_buy.description
            HAVING COUNT(DISTINCT i_buy.anagraphics_id) >= 1
            ORDER BY (avg_selling_price - avg_purchase_price) DESC
        """
        
        df = pd.read_sql_query(query, conn)
        
        if df.empty:
            return pd.DataFrame()
        
        # Calcola metriche competitive
        df['markup_amount'] = df['avg_selling_price'] - df['avg_purchase_price']
        df['markup_percentage'] = ((df['avg_selling_price'] - df['avg_purchase_price']) / df['avg_purchase_price'] * 100).round(2)
        df['supplier_diversification'] = df['num_suppliers']
        df['market_penetration'] = (df['total_sold'] / df['total_purchased'] * 100).round(2)
        
        # Normalizza nomi prodotti
        df['normalized_product'] = df['product'].apply(normalize_product_name)
        
        # Classifica competitive position
        df['competitive_position'] = pd.cut(
            df['markup_percentage'],
            bins=[0, 10, 25, 50, float('inf')],
            labels=['Basso Margine', 'Margine Normale', 'Buon Margine', 'Alto Margine']
        )
        
        # Identifica opportunità
        def identify_opportunity(row):
            if row['markup_percentage'] < 15 and row['market_penetration'] > 80:
                return "Aumentare prezzi vendita"
            elif row['supplier_diversification'] == 1:
                return "Diversificare fornitori"
            elif row['market_penetration'] < 50:
                return "Aumentare vendite"
            else:
                return "Ottimizzazione generale"
        
        df['opportunity'] = df.apply(identify_opportunity, axis=1)
        
        # Formatta prezzi per display
        df['avg_purchase_price_fmt'] = df['avg_purchase_price'].apply(lambda x: f"{x:.2f}€")
        df['avg_selling_price_fmt'] = df['avg_selling_price'].apply(lambda x: f"{x:.2f}€")
        df['markup_amount_fmt'] = df['markup_amount'].apply(lambda x: f"{x:.2f}€")
        
        return df.sort_values('markup_amount', ascending=False)
        
    except Exception as e:
        logger.error(f"Errore analisi competitiva: {e}", exc_info=True)
        return pd.DataFrame()
    finally:
        if conn:
            conn.close()
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/core/analysis.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/core/cloud_sync.py ---
# core/cloud_sync.py
"""
Modulo per la sincronizzazione del database con Google Drive.
Gestisce upload, download, risoluzione conflitti e sincronizzazione automatica.
"""

import os
import json
import sqlite3
import hashlib
import logging
import tempfile
import threading
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional, Dict, Any, Tuple
import time

try:
    from google.oauth2.credentials import Credentials
    from google_auth_oauthlib.flow import InstalledAppFlow
    from google.auth.transport.requests import Request
    from googleapiclient.discovery import build
    from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload
    from google.auth.exceptions import RefreshError
    GOOGLE_AVAILABLE = True
except ImportError:
    GOOGLE_AVAILABLE = False
    logging.warning("Google Drive sync non disponibile: installare google-api-python-client, google-auth-httplib2, google-auth-oauthlib")

from .database import get_connection, DB_PATH
from .utils import CONFIG_FILE_PATH

logger = logging.getLogger(__name__)

# Scope richiesti per Google Drive
SCOPES = ['https://www.googleapis.com/auth/drive.file']

class CloudSyncManager:
    """Gestisce la sincronizzazione del database con Google Drive"""
    
    def __init__(self, config_path: str = None):
        self.config_path = config_path or CONFIG_FILE_PATH
        self.credentials_file = os.path.join(os.path.dirname(self.config_path), 'google_credentials.json')
        self.token_file = os.path.join(os.path.dirname(self.config_path), 'google_token.json')
        self.service = None
        self.sync_enabled = False
        self.auto_sync_interval = 300  # 5 minuti default
        self.last_sync_time = None
        self.sync_thread = None
        self.stop_sync = threading.Event()
        
        # Nome file su Google Drive
        self.remote_db_name = "fattura_analyzer_db.sqlite"
        self.remote_file_id = None
        
        if GOOGLE_AVAILABLE:
            self._load_config()
            self._init_google_service()
    
    def _load_config(self):
        """Carica configurazione sync da config.ini"""
        try:
            import configparser
            config = configparser.ConfigParser()
            config.read(self.config_path)
            
            if 'CloudSync' in config:
                self.sync_enabled = config.getboolean('CloudSync', 'enabled', fallback=False)
                self.auto_sync_interval = config.getint('CloudSync', 'auto_sync_interval', fallback=300)
                self.remote_file_id = config.get('CloudSync', 'remote_file_id', fallback=None)
                
        except Exception as e:
            logger.error(f"Errore caricamento config sync: {e}")
    
    def _save_config(self):
        """Salva configurazione sync in config.ini"""
        try:
            import configparser
            config = configparser.ConfigParser()
            config.read(self.config_path)
            
            if 'CloudSync' not in config:
                config.add_section('CloudSync')
            
            config.set('CloudSync', 'enabled', str(self.sync_enabled))
            config.set('CloudSync', 'auto_sync_interval', str(self.auto_sync_interval))
            if self.remote_file_id:
                config.set('CloudSync', 'remote_file_id', self.remote_file_id)
            
            with open(self.config_path, 'w') as f:
                config.write(f)
                
        except Exception as e:
            logger.error(f"Errore salvataggio config sync: {e}")
    
    def _init_google_service(self):
        """Inizializza il servizio Google Drive"""
        if not GOOGLE_AVAILABLE:
            return False
            
        try:
            creds = None
            
            # Carica token esistente
            if os.path.exists(self.token_file):
                creds = Credentials.from_authorized_user_file(self.token_file, SCOPES)
            
            # Se non ci sono credenziali valide, richiedi autorizzazione
            if not creds or not creds.valid:
                if creds and creds.expired and creds.refresh_token:
                    try:
                        creds.refresh(Request())
                    except RefreshError:
                        logger.info("Token scaduto, richiesta nuova autorizzazione")
                        creds = None
                
                if not creds:
                    if not os.path.exists(self.credentials_file):
                        logger.error("File credentials.json non trovato. Scaricare da Google Cloud Console.")
                        return False
                    
                    flow = InstalledAppFlow.from_client_secrets_file(self.credentials_file, SCOPES)
                    creds = flow.run_local_server(port=0)
                
                # Salva token per usi futuri
                with open(self.token_file, 'w') as token:
                    token.write(creds.to_json())
            
            self.service = build('drive', 'v3', credentials=creds)
            logger.info("Servizio Google Drive inizializzato con successo")
            return True
            
        except Exception as e:
            logger.error(f"Errore inizializzazione Google Drive: {e}")
            return False
    
    def _get_file_hash(self, file_path: str) -> str:
        """Calcola hash MD5 di un file"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def _find_remote_file(self) -> Optional[str]:
        """Trova il file database su Google Drive"""
        try:
            if self.remote_file_id:
                # Verifica se il file esiste ancora
                try:
                    file = self.service.files().get(fileId=self.remote_file_id).execute()
                    return self.remote_file_id
                except:
                    self.remote_file_id = None
            
            # Cerca per nome
            results = self.service.files().list(
                q=f"name='{self.remote_db_name}' and trashed=false",
                fields="files(id, name, modifiedTime)"
            ).execute()
            
            files = results.get('files', [])
            if files:
                self.remote_file_id = files[0]['id']
                self._save_config()
                return self.remote_file_id
            
            return None
            
        except Exception as e:
            logger.error(f"Errore ricerca file remoto: {e}")
            return None
    
    def _upload_database(self, force_create: bool = False) -> bool:
        """Upload del database locale su Google Drive"""
        try:
            if not os.path.exists(DB_PATH):
                logger.error("Database locale non trovato")
                return False
            
            # Crea backup temporaneo per upload sicuro
            with tempfile.NamedTemporaryFile(suffix='.sqlite', delete=False) as temp_file:
                temp_path = temp_file.name
            
            # Copia database in temp file
            import shutil
            shutil.copy2(DB_PATH, temp_path)
            
            media = MediaFileUpload(temp_path, mimetype='application/x-sqlite3')
            
            if self.remote_file_id and not force_create:
                # Aggiorna file esistente
                file = self.service.files().update(
                    fileId=self.remote_file_id,
                    media_body=media
                ).execute()
                logger.info(f"Database aggiornato su Google Drive: {file.get('id')}")
            else:
                # Crea nuovo file
                file_metadata = {
                    'name': self.remote_db_name,
                    'description': f'FatturaAnalyzer Database - Ultimo aggiornamento: {datetime.now().isoformat()}'
                }
                file = self.service.files().create(
                    body=file_metadata,
                    media_body=media,
                    fields='id'
                ).execute()
                self.remote_file_id = file.get('id')
                self._save_config()
                logger.info(f"Nuovo database creato su Google Drive: {self.remote_file_id}")
            
            # Pulizia file temporaneo
            os.unlink(temp_path)
            return True
            
        except Exception as e:
            logger.error(f"Errore upload database: {e}")
            return False
    
    def _download_database(self) -> bool:
        """Download del database da Google Drive"""
        try:
            if not self.remote_file_id:
                logger.error("File remoto non trovato")
                return False
            
            # Backup del database locale se esistente
            if os.path.exists(DB_PATH):
                backup_path = f"{DB_PATH}.backup.{int(time.time())}"
                import shutil
                shutil.copy2(DB_PATH, backup_path)
                logger.info(f"Backup database locale creato: {backup_path}")
            
            # Download in file temporaneo
            with tempfile.NamedTemporaryFile(suffix='.sqlite', delete=False) as temp_file:
                temp_path = temp_file.name
            
            request = self.service.files().get_media(fileId=self.remote_file_id)
            with open(temp_path, 'wb') as f:
                downloader = MediaIoBaseDownload(f, request)
                done = False
                while done is False:
                    status, done = downloader.next_chunk()
            
            # Verifica integrità database scaricato
            try:
                conn = sqlite3.connect(temp_path)
                conn.execute("SELECT COUNT(*) FROM sqlite_master")
                conn.close()
            except Exception as e:
                logger.error(f"Database scaricato corrotto: {e}")
                os.unlink(temp_path)
                return False
            
            # Sostituisci database locale
            import shutil
            shutil.move(temp_path, DB_PATH)
            logger.info("Database scaricato e sostituito con successo")
            return True
            
        except Exception as e:
            logger.error(f"Errore download database: {e}")
            return False
    
    def _get_remote_modified_time(self) -> Optional[datetime]:
        """Ottiene il timestamp di modifica del file remoto"""
        try:
            if not self.remote_file_id:
                return None
            
            file = self.service.files().get(
                fileId=self.remote_file_id,
                fields='modifiedTime'
            ).execute()
            
            modified_time = file.get('modifiedTime')
            if modified_time:
                return datetime.fromisoformat(modified_time.replace('Z', '+00:00'))
            
            return None
            
        except Exception as e:
            logger.error(f"Errore recupero timestamp remoto: {e}")
            return None
    
    def _get_local_modified_time(self) -> Optional[datetime]:
        """Ottiene il timestamp di modifica del database locale"""
        try:
            if not os.path.exists(DB_PATH):
                return None
            
            timestamp = os.path.getmtime(DB_PATH)
            return datetime.fromtimestamp(timestamp, tz=timezone.utc)
            
        except Exception as e:
            logger.error(f"Errore recupero timestamp locale: {e}")
            return None
    
    def sync_database(self, force_direction: str = None) -> Dict[str, Any]:
        """
        Sincronizza il database locale con Google Drive
        
        Args:
            force_direction: 'upload', 'download' o None per auto-detect
            
        Returns:
            Dict con risultato sincronizzazione
        """
        result = {
            'success': False,
            'action': None,
            'message': '',
            'timestamp': datetime.now().isoformat()
        }
        
        if not self.service:
            result['message'] = 'Servizio Google Drive non disponibile'
            return result
        
        try:
            # Trova file remoto
            remote_file_id = self._find_remote_file()
            local_exists = os.path.exists(DB_PATH)
            
            if force_direction == 'upload':
                if not local_exists:
                    result['message'] = 'Database locale non trovato per upload'
                    return result
                
                success = self._upload_database()
                result['action'] = 'upload'
                result['success'] = success
                result['message'] = 'Upload completato' if success else 'Errore durante upload'
                
            elif force_direction == 'download':
                if not remote_file_id:
                    result['message'] = 'File remoto non trovato per download'
                    return result
                
                success = self._download_database()
                result['action'] = 'download'
                result['success'] = success
                result['message'] = 'Download completato' if success else 'Errore durante download'
                
            else:
                # Auto-detect direzione
                if not remote_file_id and not local_exists:
                    result['message'] = 'Nessun database trovato (locale o remoto)'
                    return result
                
                elif not remote_file_id and local_exists:
                    # Solo locale esiste - upload
                    success = self._upload_database(force_create=True)
                    result['action'] = 'upload'
                    result['success'] = success
                    result['message'] = 'Database locale caricato su Drive'
                    
                elif remote_file_id and not local_exists:
                    # Solo remoto esiste - download
                    success = self._download_database()
                    result['action'] = 'download'
                    result['success'] = success
                    result['message'] = 'Database scaricato da Drive'
                    
                else:
                    # Entrambi esistono - confronta timestamp
                    remote_time = self._get_remote_modified_time()
                    local_time = self._get_local_modified_time()
                    
                    if not remote_time or not local_time:
                        result['message'] = 'Impossibile confrontare timestamp'
                        return result
                    
                    if local_time > remote_time:
                        # Locale più recente - upload
                        success = self._upload_database()
                        result['action'] = 'upload'
                        result['message'] = 'Database locale più recente - sincronizzato su Drive'
                    elif remote_time > local_time:
                        # Remoto più recente - download
                        success = self._download_database()
                        result['action'] = 'download'
                        result['message'] = 'Database remoto più recente - sincronizzato localmente'
                    else:
                        # Stessi timestamp
                        result['action'] = 'none'
                        result['success'] = True
                        result['message'] = 'Database già sincronizzato'
                        return result
                    
                    result['success'] = success
            
            if result['success']:
                self.last_sync_time = datetime.now()
            
            return result
            
        except Exception as e:
            logger.error(f"Errore sincronizzazione: {e}")
            result['message'] = f'Errore sincronizzazione: {str(e)}'
            return result
    
    def start_auto_sync(self):
        """Avvia sincronizzazione automatica in background"""
        if not self.sync_enabled or not self.service:
            return
        
        if self.sync_thread and self.sync_thread.is_alive():
            return
        
        self.stop_sync.clear()
        self.sync_thread = threading.Thread(target=self._auto_sync_worker, daemon=True)
        self.sync_thread.start()
        logger.info(f"Auto-sync avviato (intervallo: {self.auto_sync_interval}s)")
    
    def stop_auto_sync(self):
        """Ferma sincronizzazione automatica"""
        if self.sync_thread:
            self.stop_sync.set()
            self.sync_thread.join(timeout=5)
        logger.info("Auto-sync fermato")
    
    def _auto_sync_worker(self):
        """Worker thread per sincronizzazione automatica"""
        while not self.stop_sync.is_set():
            try:
                result = self.sync_database()
                if result['success'] and result['action'] != 'none':
                    logger.info(f"Auto-sync: {result['message']}")
                    
            except Exception as e:
                logger.error(f"Errore auto-sync: {e}")
            
            # Attendi prossimo ciclo
            self.stop_sync.wait(self.auto_sync_interval)
    
    def is_sync_enabled(self) -> bool:
        """Verifica se la sincronizzazione è abilitata e configurata"""
        return self.sync_enabled and self.service is not None
    
    def get_sync_status(self) -> Dict[str, Any]:
        """Ottiene stato corrente della sincronizzazione"""
        return {
            'enabled': self.sync_enabled,
            'service_available': self.service is not None,
            'remote_file_id': self.remote_file_id,
            'last_sync_time': self.last_sync_time.isoformat() if self.last_sync_time else None,
            'auto_sync_running': self.sync_thread is not None and self.sync_thread.is_alive()
        }

# Singleton instance
_sync_manager = None

def get_sync_manager() -> CloudSyncManager:
    """Ottiene istanza singleton del CloudSyncManager"""
    global _sync_manager
    if _sync_manager is None:
        _sync_manager = CloudSyncManager()
    return _sync_manager
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/core/cloud_sync.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/core/database.py ---
# core/database.py

import sqlite3
import os
from datetime import datetime, date
import configparser
import logging
import pandas as pd
from decimal import Decimal, InvalidOperation
import re # Importa re per usare regex nel filtro POS avanzato

# Importa solo le funzioni necessarie da utils per evitare cicli
try:
    from .utils import to_decimal, quantize, AMOUNT_TOLERANCE
except ImportError:
    logging.warning("Import relativo .utils fallito in database.py, tento import assoluto.")
    try:
        from utils import to_decimal, quantize, AMOUNT_TOLERANCE
    except ImportError as e_abs:
        logging.critical(f"FATAL: Impossibile importare funzioni necessarie da utils ({e_abs}).")
        # Definisci fallback minimali per permettere avvio, ma con funzionalità ridotte
        AMOUNT_TOLERANCE = Decimal('0.01')
        def to_decimal(value, default='0.0'): return Decimal(value) if value else Decimal(default)
        def quantize(d): return d.quantize(Decimal('0.01')) if isinstance(d, Decimal) else Decimal('NaN')


DATABASE_NAME = 'database.db'
logger = logging.getLogger(__name__)


def get_db_path():
    try:
        config = configparser.ConfigParser()
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        config_path = os.path.join(project_root, 'config.ini')
        if not os.path.exists(config_path):
            logging.warning(f"Config file '{config_path}' non trovato. Uso default '{DATABASE_NAME}' nella root.")
            db_full_path = os.path.join(project_root, DATABASE_NAME)
        else:
            config.read(config_path)
            db_name = config.get('Paths', 'DatabaseFile', fallback=DATABASE_NAME)
            if os.path.isabs(db_name):
                db_full_path = db_name
            else:
                db_full_path = os.path.join(project_root, db_name)
        db_dir = os.path.dirname(db_full_path)
        if db_dir:
            os.makedirs(db_dir, exist_ok=True)
        logging.debug(f"Percorso DB determinato: {db_full_path}")
        return db_full_path
    except Exception as e:
        logging.error(f"Errore lettura config per percorso DB: {e}. Uso fallback.")
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        fallback_path = os.path.join(project_root, DATABASE_NAME)
        logging.warning(f"Percorso DB fallback: {fallback_path}")
        db_dir = os.path.dirname(fallback_path)
        if db_dir:
            os.makedirs(db_dir, exist_ok=True)
        return fallback_path

DB_PATH = get_db_path()

def get_connection():
    conn = None
    try:
        conn = sqlite3.connect(DB_PATH, timeout=15, detect_types=sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES)
        conn.row_factory = sqlite3.Row
        conn.execute("PRAGMA foreign_keys = ON;")
        conn.execute("PRAGMA journal_mode=WAL;")
        logging.debug(f"Connessione DB {DB_PATH} OK (WAL mode).")
        return conn
    except sqlite3.Error as e:
        logging.error(f"Errore connessione DB {DB_PATH}: {e}")
        raise

def create_tables():
    logging.info(f"Verifica/Creazione tabelle in: {DB_PATH}")
    conn = None
    try:
        conn = get_connection()
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS Anagraphics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                type TEXT NOT NULL CHECK(type IN ('Cliente', 'Fornitore')),
                piva TEXT,
                cf TEXT,
                denomination TEXT NOT NULL COLLATE NOCASE,
                address TEXT,
                cap TEXT,
                city TEXT,
                province TEXT,
                country TEXT DEFAULT 'IT',
                iban TEXT,
                email TEXT,
                phone TEXT,
                pec TEXT,
                codice_destinatario TEXT,
                score REAL DEFAULT 100.0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );""")
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS Invoices (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                anagraphics_id INTEGER NOT NULL,
                type TEXT NOT NULL CHECK(type IN ('Attiva', 'Passiva')),
                doc_type TEXT,
                doc_number TEXT NOT NULL COLLATE NOCASE,
                doc_date DATE NOT NULL,
                total_amount REAL NOT NULL,
                due_date DATE,
                payment_status TEXT DEFAULT 'Aperta' CHECK(payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.', 'Pagata Tot.', 'Insoluta', 'Riconciliata')),
                paid_amount REAL DEFAULT 0.0,
                payment_method TEXT,
                notes TEXT,
                xml_filename TEXT,
                p7m_source_file TEXT,
                unique_hash TEXT UNIQUE NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (anagraphics_id) REFERENCES Anagraphics(id) ON DELETE RESTRICT
            );""")
        try:
             cursor.execute("ALTER TABLE Invoices ADD COLUMN paid_amount REAL DEFAULT 0.0;")
             logging.info("Colonna 'paid_amount' aggiunta a Invoices.")
        except sqlite3.OperationalError: pass
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS InvoiceLines (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                invoice_id INTEGER NOT NULL,
                line_number INTEGER NOT NULL,
                description TEXT COLLATE NOCASE,
                quantity REAL,
                unit_measure TEXT,
                unit_price REAL,
                total_price REAL NOT NULL,
                vat_rate REAL NOT NULL,
                item_code TEXT COLLATE NOCASE,
                item_type TEXT,
                FOREIGN KEY (invoice_id) REFERENCES Invoices(id) ON DELETE CASCADE
            );""")
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS InvoiceVATSummary (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                invoice_id INTEGER NOT NULL,
                vat_rate REAL NOT NULL,
                taxable_amount REAL NOT NULL,
                vat_amount REAL NOT NULL,
                FOREIGN KEY (invoice_id) REFERENCES Invoices(id) ON DELETE CASCADE
            );""")
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS BankTransactions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                transaction_date DATE NOT NULL,
                value_date DATE,
                amount REAL NOT NULL,
                description TEXT COLLATE NOCASE,
                causale_abi INTEGER,
                unique_hash TEXT UNIQUE NOT NULL,
                reconciliation_status TEXT DEFAULT 'Da Riconciliare' CHECK(reconciliation_status IN ('Da Riconciliare', 'Riconciliato Parz.', 'Riconciliato Tot.', 'Riconciliato Eccesso', 'Ignorato')),
                reconciled_amount REAL DEFAULT 0.0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );""")
        try:
            cursor.execute("ALTER TABLE BankTransactions ADD COLUMN reconciled_amount REAL DEFAULT 0.0;")
            logging.info("Colonna 'reconciled_amount' aggiunta a BankTransactions.")
        except sqlite3.OperationalError: pass
        try:
            cursor.execute("ALTER TABLE BankTransactions ADD COLUMN reconciliation_status TEXT DEFAULT 'Da Riconciliare';")
            logging.info("Colonna 'reconciliation_status' aggiunta a BankTransactions.")
        except sqlite3.OperationalError: pass
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS ReconciliationLinks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                transaction_id INTEGER NOT NULL,
                invoice_id INTEGER NOT NULL,
                reconciled_amount REAL NOT NULL,
                reconciliation_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(transaction_id, invoice_id),
                FOREIGN KEY (transaction_id) REFERENCES BankTransactions(id) ON DELETE CASCADE,
                FOREIGN KEY (invoice_id) REFERENCES Invoices(id) ON DELETE CASCADE
            );""")
        try:
            cursor.execute("ALTER TABLE ReconciliationLinks ADD COLUMN reconciliation_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP;")
            logging.info("Colonna 'reconciliation_date' aggiunta a ReconciliationLinks.")
        except sqlite3.OperationalError: pass
        cursor.execute("CREATE TABLE IF NOT EXISTS Settings (key TEXT PRIMARY KEY, value TEXT);")

        logging.info("Creazione/Verifica indici...")
        indices = [
            "CREATE INDEX IF NOT EXISTS idx_anagraphics_piva ON Anagraphics(piva) WHERE piva IS NOT NULL;",
            "CREATE INDEX IF NOT EXISTS idx_anagraphics_cf ON Anagraphics(cf) WHERE cf IS NOT NULL;",
            "CREATE INDEX IF NOT EXISTS idx_anagraphics_denomination ON Anagraphics(denomination COLLATE NOCASE);",
            "CREATE INDEX IF NOT EXISTS idx_anagraphics_type ON Anagraphics(type);",
            "CREATE INDEX IF NOT EXISTS idx_anagraphics_score ON Anagraphics(score);",
            "CREATE INDEX IF NOT EXISTS idx_invoices_anagraphics_id ON Invoices(anagraphics_id);",
            "CREATE INDEX IF NOT EXISTS idx_invoices_type_status ON Invoices(type, payment_status);",
            "CREATE INDEX IF NOT EXISTS idx_invoices_doc_date ON Invoices(doc_date);",
            "CREATE INDEX IF NOT EXISTS idx_invoices_due_date ON Invoices(due_date) WHERE due_date IS NOT NULL;",
            "CREATE INDEX IF NOT EXISTS idx_invoices_doc_number ON Invoices(doc_number COLLATE NOCASE);",
            "CREATE UNIQUE INDEX IF NOT EXISTS idx_invoices_unique_hash ON Invoices(unique_hash);",
            "CREATE INDEX IF NOT EXISTS idx_invoicelines_invoice ON InvoiceLines(invoice_id);",
            "CREATE INDEX IF NOT EXISTS idx_invoicelines_item_code ON InvoiceLines(item_code COLLATE NOCASE) WHERE item_code IS NOT NULL;",
            "CREATE INDEX IF NOT EXISTS idx_invoicelines_description ON InvoiceLines(description COLLATE NOCASE) WHERE description IS NOT NULL;",
            "CREATE INDEX IF NOT EXISTS idx_transactions_date ON BankTransactions(transaction_date);",
            "CREATE INDEX IF NOT EXISTS idx_transactions_status ON BankTransactions(reconciliation_status);",
            "CREATE INDEX IF NOT EXISTS idx_transactions_amount ON BankTransactions(amount);",
            "CREATE UNIQUE INDEX IF NOT EXISTS idx_transactions_unique_hash ON BankTransactions(unique_hash);",
            "CREATE INDEX IF NOT EXISTS idx_reconlinks_invoice ON ReconciliationLinks(invoice_id);",
            "CREATE INDEX IF NOT EXISTS idx_reconlinks_transaction ON ReconciliationLinks(transaction_id);",
            "CREATE UNIQUE INDEX IF NOT EXISTS idx_reconlinks_trans_inv ON ReconciliationLinks(transaction_id, invoice_id);"
        ]
        for index_sql in indices:
            try:
                cursor.execute(index_sql)
            except sqlite3.OperationalError as e:
                if "already exists" not in str(e).lower() and "duplicate" not in str(e).lower():
                    logging.warning(f"Errore creazione indice (ignoro se esiste già): {e} SQL: {index_sql}")

        conn.commit()
        logging.info("Tabelle e indici DB pronti.")
    except sqlite3.Error as e:
        logging.error(f"Errore creazione tabelle/indici: {e}")
        if conn:
            try:
                conn.rollback()
            except Exception as rb_err:
                logging.error(f"Errore durante rollback in create_tables: {rb_err}")
        raise
    finally:
        if conn:
            try:
                conn.close()
            except Exception as close_err:
                logging.error(f"Errore chiusura connessione in create_tables: {close_err}")

def check_entity_duplicate(cursor, table, column, value):
    try:
        cursor.execute(f"SELECT 1 FROM {table} WHERE {column} = ? LIMIT 1", (value,))
        return cursor.fetchone() is not None
    except sqlite3.Error as e:
        logger.error(f"Errore check duplicato {table}.{column} = '{value}': {e}")
        return False

def add_anagraphics_if_not_exists(cursor, anag_data, anag_type):
    piva = anag_data.get('piva')
    cf = anag_data.get('cf')
    denomination = anag_data.get('denomination')

    if not denomination:
        logger.error(f"Tentativo inserimento anagrafica {anag_type} senza denominazione. Dati: {anag_data}")
        return None
    if not piva and not cf:
        logger.warning(f"Tentativo inserimento anagrafica {anag_type} '{denomination}' senza P.IVA né C.F. Annullato.")
        return None

    anag_id = None
    if piva and len(piva) > 5:
        cursor.execute("SELECT id FROM Anagraphics WHERE type = ? AND piva = ?", (anag_type, piva))
        row = cursor.fetchone()
        anag_id = row['id'] if row else None

    if not anag_id and cf and len(cf) > 10:
        cursor.execute("SELECT id FROM Anagraphics WHERE type = ? AND cf = ?", (anag_type, cf))
        row = cursor.fetchone()
        anag_id = row['id'] if row else None

    if not anag_id and cf and not piva:
         cursor.execute("SELECT id FROM Anagraphics WHERE type = ? AND piva = ?", (anag_type, cf))
         row = cursor.fetchone()
         if row:
             anag_id = row['id']
             logger.info(f"Trovata anagrafica {anag_type} '{denomination}' cercando CF ({cf}) come PIVA (ID:{anag_id}).")

    if anag_id:
        logger.debug(f"Anagrafica {anag_type} '{denomination}' già esistente (ID:{anag_id}).")
        return anag_id
    else:
        try:
            sql = """INSERT INTO Anagraphics (type, piva, cf, denomination, address, cap, city, province, country, email, phone, pec, codice_destinatario, updated_at)
                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"""
            params = (
                anag_type, piva, cf, denomination,
                anag_data.get('address'), anag_data.get('cap'), anag_data.get('city'),
                anag_data.get('province'), anag_data.get('country', 'IT'),
                anag_data.get('email'), anag_data.get('phone'), anag_data.get('pec'),
                anag_data.get('codice_destinatario'), datetime.now()
            )
            cursor.execute(sql, params)
            new_id = cursor.lastrowid
            logging.info(f"Nuova Anagrafica {anag_type} '{denomination}' (P:{piva}, CF:{cf}) inserita con ID:{new_id}.")
            return new_id
        except sqlite3.IntegrityError as ie:
            logger.warning(f"Constraint violato durante inserimento anagrafica {denomination} (P:{piva}, CF:{cf}): {ie}. Ritento ricerca.")
            if piva:
                cursor.execute("SELECT id FROM Anagraphics WHERE type = ? AND piva = ?", (anag_type, piva))
            elif cf:
                cursor.execute("SELECT id FROM Anagraphics WHERE type = ? AND cf = ?", (anag_type, cf))
            else:
                return None
            row = cursor.fetchone()
            if row:
                logger.info(f"Trovata anagrafica {anag_type} '{denomination}' al secondo tentativo (ID:{row['id']}).")
                return row['id']
            else:
                logger.error(f"Inserimento fallito per IntegrityError ma non trovata al secondo tentativo: {denomination}")
                return None
        except Exception as e:
            logger.error(f"Errore inserimento anagrafica {denomination}: {e}", exc_info=True)
            return None

def get_reconciliation_links_for_item(item_type, item_id):
    conn = None
    links = []
    try:
        conn = get_connection()
        if item_type == 'transaction':
            query = """
                SELECT rl.id, rl.invoice_id, rl.reconciled_amount, rl.reconciliation_date,
                       i.doc_number as invoice_doc_number, i.type as invoice_type
                FROM ReconciliationLinks rl
                JOIN Invoices i ON rl.invoice_id = i.id
                WHERE rl.transaction_id = ?
                ORDER BY i.doc_date DESC, i.doc_number;
            """
        elif item_type == 'invoice':
            query = """
                SELECT rl.id, rl.transaction_id, rl.reconciled_amount, rl.reconciliation_date,
                       bt.transaction_date as transaction_date, bt.amount as transaction_amount
                FROM ReconciliationLinks rl
                JOIN BankTransactions bt ON rl.transaction_id = bt.id
                WHERE rl.invoice_id = ?
                ORDER BY bt.transaction_date DESC;
            """
        else:
            logger.error(f"Tipo item non valido per get_reconciliation_links: {item_type}")
            return pd.DataFrame()

        df = pd.read_sql_query(query, conn, params=(item_id,), parse_dates=['reconciliation_date', 'transaction_date' if item_type == 'invoice' else None])
        return df if df is not None else pd.DataFrame()

    except Exception as e:
        logger.error(f"Errore recupero link per {item_type} ID {item_id}: {e}", exc_info=True)
        return pd.DataFrame()
    finally:
        if conn: conn.close()

def add_transactions(cursor, transactions_df):
    inserted_count, db_duplicate_count, batch_duplicate_count, error_count = 0, 0, 0, 0
    if transactions_df is None or transactions_df.empty:
        logging.info("Nessuna transazione da aggiungere (DataFrame vuoto).")
        return 0, 0, 0, 0

    hashes_in_db = set()
    try:
        min_date = transactions_df['DataContabile'].min()
        max_date = transactions_df['DataContabile'].max()
        MAX_DAYS_RANGE_FOR_HASH_LOAD = 180
        load_all_hashes = True

        if pd.notna(min_date) and pd.notna(max_date) and isinstance(min_date, pd.Timestamp) and isinstance(max_date, pd.Timestamp):
            date_range_days = (max_date - min_date).days
            if date_range_days <= MAX_DAYS_RANGE_FOR_HASH_LOAD:
                load_all_hashes = False
                min_date_str = min_date.strftime('%Y-%m-%d')
                max_date_str = max_date.strftime('%Y-%m-%d')
                cursor.execute("SELECT unique_hash FROM BankTransactions WHERE transaction_date BETWEEN ? AND ?", (min_date_str, max_date_str))
                hashes_in_db = {row['unique_hash'] for row in cursor.fetchall()}
                logging.debug(f"Caricati {len(hashes_in_db)} hash DB per range {min_date_str} - {max_date_str}.")
            else:
                logging.debug(f"Range date ({date_range_days} gg) troppo ampio, carico tutti gli hash.")
        else:
            logging.warning("Date min/max non valide o non Timestamp nel DataFrame, carico tutti gli hash.")

        if load_all_hashes:
            logging.debug("Caricamento di tutti gli hash delle transazioni dal DB...")
            cursor.execute("SELECT unique_hash FROM BankTransactions")
            hashes_in_db = {row['unique_hash'] for row in cursor.fetchall()}
            logging.debug(f"Caricati {len(hashes_in_db)} hash totali dal DB.")

    except sqlite3.Error as e:
        logging.error(f"Errore DB caricamento hash esistenti: {e}. Check duplicati DB non eseguito.")
        hashes_in_db = set()
    except Exception as e_gen:
        logging.error(f"Errore imprevisto caricamento hash: {e_gen}")
        hashes_in_db = set()

    data_to_insert = []
    batch_hashes = set()
    preparation_errors = 0
    skipped_db_duplicates = 0
    skipped_batch_duplicates = 0

    for index, row in transactions_df.iterrows():
        trans_hash = row.get('unique_hash')
        if trans_hash is None or not isinstance(trans_hash, str) or not trans_hash.strip():
            logging.error(f"Hash mancante o non valido per transazione riga DF {index}. Salto.")
            preparation_errors += 1
            continue

        if trans_hash in hashes_in_db:
            skipped_db_duplicates += 1
            continue
        if trans_hash in batch_hashes:
            skipped_batch_duplicates += 1
            logger.warning(f"Duplicato trovato nello stesso file CSV (Riga DF {index}, Hash: {trans_hash[:10]}...). Salto.")
            continue

        try:
            t_date_ts = row.get('DataContabile')
            if not isinstance(t_date_ts, pd.Timestamp): raise ValueError("DataContabile non valida.")
            t_date = t_date_ts.strftime('%Y-%m-%d')

            v_date_val = row.get('DataValuta')
            v_date = v_date_val.strftime('%Y-%m-%d') if pd.notna(v_date_val) and isinstance(v_date_val, pd.Timestamp) else None

            amount_val = row.get('Importo')
            if amount_val is None or pd.isna(amount_val): raise ValueError("Importo mancante o NaN")
            amount_float = float(amount_val)

            desc_val = row.get('Descrizione')
            description = str(desc_val).strip() if pd.notna(desc_val) else None

            causale_val = row.get('CausaleABI')
            causale = int(causale_val) if pd.notna(causale_val) else None

            data_to_insert.append((t_date, v_date, amount_float, description, causale, trans_hash))
            batch_hashes.add(trans_hash)
        except Exception as e_prep:
            preparation_errors += 1
            logging.error(f"Errore preparazione dati transazione (Riga DF {index}, Hash: {trans_hash[:10]}...): {e_prep}")

    db_duplicate_count = skipped_db_duplicates
    batch_duplicate_count = skipped_batch_duplicates
    error_count = preparation_errors

    if data_to_insert:
        insert_sql = """INSERT INTO BankTransactions (transaction_date, value_date, amount, description, causale_abi, unique_hash, reconciled_amount, reconciliation_status)
                        VALUES (?, ?, ?, ?, ?, ?, 0.0, 'Da Riconciliare')"""
        try:
            cursor.executemany(insert_sql, data_to_insert)
            inserted_count = len(data_to_insert)
            logging.info(f"Inserimento batch di {inserted_count} nuove transazioni completato.")
        except sqlite3.IntegrityError as e_int:
            logger.error(f"Errore UNIQUE constraint durante inserimento batch: {e_int}. Nessuna riga inserita.")
            error_count += len(data_to_insert)
            inserted_count = 0
        except sqlite3.Error as e_db:
            logger.error(f"Errore DB generico durante inserimento batch transazioni: {e_db}")
            error_count += len(data_to_insert)
            inserted_count = 0
        except Exception as e_generic_insert:
             logger.error(f"Errore imprevisto durante inserimento batch transazioni: {e_generic_insert}", exc_info=True)
             error_count += len(data_to_insert)
             inserted_count = 0

    logging.info(f"Risultato aggiunta transazioni: Nuove:{inserted_count}, Duplicati DB:{db_duplicate_count}, Duplicati File:{batch_duplicate_count}, Errori Prep./Insert:{error_count}.")
    return inserted_count, db_duplicate_count, batch_duplicate_count, error_count

def get_anagraphics(type_filter=None):
    conn = None
    try:
        conn = get_connection()
        query = """SELECT id, type, denomination, piva, cf, city, province, score,
                          email, phone, pec, iban, address, cap, codice_destinatario
                   FROM Anagraphics"""
        params = []
        if type_filter and type_filter in ['Cliente', 'Fornitore']:
            query += " WHERE type = ?"
            params.append(type_filter)
        query += " ORDER BY denomination COLLATE NOCASE"
        df = pd.read_sql_query(query, conn, params=params if params else None)
        return df if df is not None else pd.DataFrame()
    except Exception as e:
        logger.error(f"Errore recupero anagrafiche (filtro: {type_filter}): {e}", exc_info=True)
        return pd.DataFrame()
    finally:
        if conn: conn.close()

def get_invoices(type_filter=None, status_filter=None, anagraphics_id_filter=None, limit=None):
    conn = None
    cols_out_expected = [
        'id', 'type', 'doc_number', 'doc_date', 'total_amount', 'due_date',
        'payment_status', 'paid_amount', 'payment_method', 'counterparty_name',
        'anagraphics_id', 'xml_filename', 'p7m_source_file',
        'doc_date_fmt', 'due_date_fmt', 'total_amount_dec', 'paid_amount_dec',
        'open_amount_dec', 'total_amount_fmt', 'open_amount_fmt'
    ]
    empty_df_out = pd.DataFrame(columns=cols_out_expected)
    try:
        conn = get_connection()
        query = """SELECT i.id, i.type, i.doc_number, i.doc_date, i.total_amount, i.due_date,
                          i.payment_status, i.paid_amount, i.payment_method,
                          a.denomination AS counterparty_name, i.anagraphics_id,
                          i.xml_filename, i.p7m_source_file
                   FROM Invoices i JOIN Anagraphics a ON i.anagraphics_id = a.id"""
        filters, params = [], []

        if type_filter and type_filter in ['Attiva', 'Passiva']:
            filters.append("i.type = ?"); params.append(type_filter)
        if status_filter:
             valid_statuses = ['Aperta', 'Scaduta', 'Pagata Parz.', 'Pagata Tot.', 'Insoluta', 'Riconciliata']
             if isinstance(status_filter, list):
                 valid_list = [s for s in status_filter if s in valid_statuses]
                 if valid_list:
                     placeholders = ','.join('?' * len(valid_list))
                     filters.append(f"i.payment_status IN ({placeholders})")
                     params.extend(valid_list)
             elif status_filter == 'Aperta/Scaduta':
                 filters.append("i.payment_status IN (?, ?, ?)")
                 params.extend(['Aperta', 'Scaduta', 'Pagata Parz.'])
             elif status_filter == 'Aperta':
                 filters.append("i.payment_status IN (?, ?)")
                 params.extend(['Aperta', 'Pagata Parz.'])
             elif status_filter == 'Scaduta':
                 filters.append("i.payment_status IN (?, ?)")
                 params.extend(['Scaduta', 'Pagata Parz.'])
             elif status_filter in valid_statuses:
                 filters.append("i.payment_status = ?")
                 params.append(status_filter)
             else:
                 logger.warning(f"Filtro stato fattura non valido: '{status_filter}'. Mostro tutte.")

        if anagraphics_id_filter is not None:
            try:
                anag_id = int(anagraphics_id_filter)
                filters.append("i.anagraphics_id = ?"); params.append(anag_id)
            except (ValueError, TypeError):
                 logger.warning(f"Filtro anagraphics_id non valido ignorato: {anagraphics_id_filter}")

        if filters: query += " WHERE " + " AND ".join(filters)
        query += " ORDER BY i.doc_date DESC, i.id DESC"
        if limit:
            try: query += " LIMIT ?"; params.append(int(limit))
            except (ValueError, TypeError): pass

        df = pd.read_sql_query(query, conn, params=params if params else None, parse_dates=['doc_date', 'due_date'])
        if df is None: return empty_df_out.copy()

        if df.empty:
             logger.debug("get_invoices: Nessuna fattura trovata con i criteri specificati.")
             return empty_df_out.copy()

        df['total_amount_dec'] = df['total_amount'].apply(lambda x: to_decimal(x, default='NaN'))
        df['paid_amount_dec'] = df['paid_amount'].apply(lambda x: to_decimal(x, default='NaN'))

        # Calcola open_amount_dec solo se total e paid sono validi
        mask_valid = df['total_amount_dec'].apply(lambda d: isinstance(d, Decimal) and d.is_finite()) & \
                     df['paid_amount_dec'].apply(lambda d: isinstance(d, Decimal) and d.is_finite())
        df['open_amount_dec'] = pd.NA # Inizializza a NA (pandas)
        df.loc[mask_valid, 'open_amount_dec'] = (df.loc[mask_valid, 'total_amount_dec'] - df.loc[mask_valid, 'paid_amount_dec']).apply(quantize)
        # Assicura che la colonna sia di tipo Decimal o object contenente Decimal/NaN
        df['open_amount_dec'] = df['open_amount_dec'].apply(lambda x: x if isinstance(x, Decimal) else Decimal('NaN'))

        # Logica di formattazione
        df['doc_date_fmt'] = df['doc_date'].dt.strftime('%d/%m/%Y').fillna('N/D')
        df['due_date_fmt'] = df['due_date'].dt.strftime('%d/%m/%Y').fillna('N/D')
        df['total_amount_fmt'] = df['total_amount_dec'].apply(lambda x: f"{x:,.2f}".replace(",", "X").replace(".", ",").replace("X", ".") if isinstance(x, Decimal) and x.is_finite() else 'Errore')
        df['open_amount_fmt'] = df['open_amount_dec'].apply(lambda x: f"{x:,.2f}".replace(",", "X").replace(".", ",").replace("X", ".") if isinstance(x, Decimal) and x.is_finite() else 'Errore')

        # Assicura che tutte le colonne attese siano presenti
        for col in cols_out_expected:
            if col not in df.columns:
                logger.warning(f"Colonna attesa '{col}' mancante in get_invoices, aggiunta vuota.")
                df[col] = pd.NA if '_dec' in col else '' # Usa '' per stringhe, NA per decimali

        return df[cols_out_expected]

    except Exception as e:
        logger.error(f"Errore recupero fatture: {e}", exc_info=True)
        return empty_df_out.copy()
    finally:
        if conn: conn.close()

def get_transactions(start_date=None, end_date=None, status_filter=None, limit=None,
                     anagraphics_id_heuristic_filter=None,
                     hide_pos=False, hide_worldline=False, hide_cash=False, hide_commissions=False):
    conn = None
    try:
        # Import qui per evitare ciclo se chiamato da altri moduli prima di reconciliation
        try: from .reconciliation import find_anagraphics_id_from_description
        except ImportError: from reconciliation import find_anagraphics_id_from_description

        conn = get_connection()
        # Registra funzione REGEXP se non esiste (necessario per alcuni backend SQLite)
        try:
            conn.create_function("REGEXP", 2, lambda pattern, item: bool(re.search(pattern, item, re.IGNORECASE)) if item else False)
        except Exception as e_regexp:
            logger.warning(f"Impossibile registrare funzione REGEXP, alcuni filtri potrebbero non funzionare correttamente: {e_regexp}")

        query = """SELECT id, transaction_date, value_date, amount, description,
                          causale_abi, reconciliation_status, reconciled_amount, unique_hash
                   FROM BankTransactions"""
        filters, params = [], []

        if start_date:
             try:
                 start_dt = pd.to_datetime(start_date).strftime('%Y-%m-%d')
                 filters.append("transaction_date >= date(?)"); params.append(start_dt)
             except Exception: logging.warning(f"Data inizio non valida ignorata: {start_date}")
        if end_date:
             try:
                 end_dt = pd.to_datetime(end_date).strftime('%Y-%m-%d')
                 filters.append("transaction_date <= date(?)"); params.append(end_dt)
             except Exception: logging.warning(f"Data fine non valida ignorata: {end_date}")

        valid_statuses = ['Da Riconciliare', 'Riconciliato Parz.', 'Riconciliato Tot.', 'Riconciliato Eccesso', 'Ignorato']
        if status_filter:
            if isinstance(status_filter, list):
                 valid_list = [s for s in status_filter if s in valid_statuses]
                 if valid_list:
                     placeholders = ','.join('?' * len(valid_list))
                     filters.append(f"reconciliation_status IN ({placeholders})")
                     params.extend(valid_list)
            elif status_filter in valid_statuses:
                 filters.append("reconciliation_status = ?")
                 params.append(status_filter)
            else:
                 logging.warning(f"Filtro stato transazione non valido: {status_filter}. Ignorato.")
        elif anagraphics_id_heuristic_filter is None: # Default: non mostrare Ignorati se non richiesto specificamente
             filters.append("reconciliation_status != ?")
             params.append('Ignorato')

        # Applica filtri descrizione
        desc_conditions = []
        if hide_pos:
            pos_pattern = r'\b(POS|PAGOBANCOMAT|CIRRUS|MAESTRO|VISA|MASTERCARD|AMEX|AMERICAN EXPRESS|CARTA DI CREDITO|CREDIT CARD|ESE COMM)\b'
            desc_conditions.append(f"description NOT REGEXP '{pos_pattern}'")
        if hide_worldline:
            desc_conditions.append("description NOT LIKE '%WORLDLINE%'")
        if hide_cash:
            desc_conditions.append("description NOT LIKE '%VERSAMENTO CONTANTE%'")
        if hide_commissions:
            desc_conditions.append("description NOT LIKE 'COMMISSIONI%'")
            desc_conditions.append("description NOT LIKE 'COMPETENZE BANC%'")
            desc_conditions.append("description NOT LIKE 'SPESE TENUTA CONTO%'")
            desc_conditions.append("description NOT LIKE 'IMPOSTA DI BOLLO%'")

        if desc_conditions:
            filters.append(f"(description IS NULL OR ({' AND '.join(desc_conditions)}))")


        if filters: query += " WHERE " + " AND ".join(filters)
        query += " ORDER BY transaction_date DESC, id DESC"
        apply_limit_later = (anagraphics_id_heuristic_filter is not None and limit is not None)
        if limit and not apply_limit_later:
             try: query += " LIMIT ?"; params.append(int(limit))
             except (ValueError, TypeError): pass

        df = pd.read_sql_query(query, conn, params=params if params else None, parse_dates=['transaction_date', 'value_date'])
        if df is None: return pd.DataFrame()

        if anagraphics_id_heuristic_filter is not None and not df.empty:
            try:
                target_anag_id = int(anagraphics_id_heuristic_filter)
                logger.info(f"Applicazione filtro euristico transazioni per AnagID: {target_anag_id} su {len(df)} righe...")
                # Ottimizzazione: usa apply invece di iterrows
                def check_description_match(desc):
                    if pd.isna(desc): return False
                    found_id = find_anagraphics_id_from_description(desc)
                    return found_id == target_anag_id

                # Maschera booleana per filtro descrizione
                desc_mask = df['description'].apply(check_description_match)

                # Maschera booleana per link parziali (più complesso, richiede query aggiuntiva)
                partial_link_mask = pd.Series(False, index=df.index)
                partial_trans_ids = df.loc[df['reconciliation_status'] == 'Riconciliato Parz.', 'id'].tolist()
                if partial_trans_ids:
                    placeholders = ','.join('?' * len(partial_trans_ids))
                    query_links = f"""SELECT DISTINCT rl.transaction_id
                                      FROM ReconciliationLinks rl JOIN Invoices i ON rl.invoice_id = i.id
                                      WHERE rl.transaction_id IN ({placeholders}) AND i.anagraphics_id = ?"""
                    params_links = partial_trans_ids + [target_anag_id]
                    df_links = pd.read_sql_query(query_links, conn, params=params_links)
                    linked_ids_for_anag = set(df_links['transaction_id'])
                    partial_link_mask = df['id'].isin(linked_ids_for_anag)

                # Combina le maschere
                final_mask = desc_mask | partial_link_mask
                df = df[final_mask].copy()
                logger.info(f"Risultato filtro euristico: {len(df)} transazioni mantenute.")
            except (ValueError, TypeError):
                logger.error(f"ID anagrafica non valido per filtro euristico: {anagraphics_id_heuristic_filter}")
                df = pd.DataFrame()
            except Exception as e_filter:
                logger.error(f"Errore durante filtro euristico transazioni: {e_filter}", exc_info=True)
                df = pd.DataFrame()

        if apply_limit_later and not df.empty:
            try: df = df.head(int(limit))
            except (ValueError, TypeError): pass

        # Colonne calcolate/formattate (come prima, ma applicate dopo il filtro)
        if not df.empty:
            df['transaction_date_fmt'] = df['transaction_date'].dt.strftime('%d/%m/%Y')
            df['value_date_fmt'] = df['value_date'].dt.strftime('%d/%m/%Y').fillna('N/D')
            df['amount_dec'] = df['amount'].apply(lambda x: to_decimal(x, default='NaN'))
            df['reconciled_amount_dec'] = df['reconciled_amount'].apply(lambda x: to_decimal(x, default='NaN'))
            mask_valid = df['amount_dec'].apply(lambda d: isinstance(d, Decimal) and d.is_finite()) & \
                         df['reconciled_amount_dec'].apply(lambda d: isinstance(d, Decimal) and d.is_finite())
            df['remaining_amount_dec'] = pd.NA
            df.loc[mask_valid, 'remaining_amount_dec'] = (df.loc[mask_valid, 'amount_dec'] - df.loc[mask_valid, 'reconciled_amount_dec']).apply(quantize)
            df['remaining_amount_dec'] = df['remaining_amount_dec'].apply(lambda x: x if isinstance(x, Decimal) else Decimal('NaN'))

            df['amount_fmt'] = df['amount_dec'].apply(lambda x: f"{x:,.2f}".replace(",", "X").replace(".", ",").replace("X", ".") if isinstance(x, Decimal) and x.is_finite() else 'Errore')
            df['remaining_amount_fmt'] = df['remaining_amount_dec'].apply(lambda x: f"{x:,.2f}".replace(",", "X").replace(".", ",").replace("X", ".") if isinstance(x, Decimal) and x.is_finite() else 'Errore')
            df['causale_abi'] = df['causale_abi'].apply(lambda x: str(int(x)) if pd.notna(x) else '')
        else:
             # Se df è vuoto, crea colonne vuote per coerenza
            cols_to_add_fmt = ['transaction_date_fmt', 'value_date_fmt', 'amount_fmt', 'remaining_amount_fmt', 'causale_abi']
            cols_to_add_dec = ['amount_dec', 'reconciled_amount_dec', 'remaining_amount_dec']
            for col in cols_to_add_fmt: df[col] = pd.Series(dtype='object')
            for col in cols_to_add_dec: df[col] = pd.Series(dtype='object') # object può contenere Decimal o NaN


        cols_to_return = [
            'id', 'transaction_date_fmt', 'value_date_fmt', 'amount_fmt',
            'remaining_amount_fmt', 'description', 'causale_abi', 'reconciliation_status',
            'amount_dec', 'reconciled_amount_dec', 'remaining_amount_dec', 'unique_hash', 'amount'
        ]
        # Assicura che tutte le colonne esistano prima di selezionarle
        for col in cols_to_return:
             if col not in df.columns:
                 df[col] = pd.NA if '_dec' in col else ''

        return df[cols_to_return]
    except Exception as e:
        logger.error(f"Errore recupero transazioni: {e}", exc_info=True)
        cols_out = [
            'id', 'transaction_date_fmt', 'value_date_fmt', 'amount_fmt',
            'remaining_amount_fmt', 'description', 'causale_abi', 'reconciliation_status',
            'amount_dec', 'reconciled_amount_dec', 'remaining_amount_dec', 'unique_hash', 'amount'
        ]
        return pd.DataFrame(columns=cols_out)
    finally:
        if conn: conn.close()


def get_item_details(conn, item_type, item_id):
    table = None
    if item_type == 'invoice':
        table = "Invoices"
    elif item_type == 'transaction':
        table = "BankTransactions"
    else:
        logger.error(f"Tipo elemento non valido per get_item_details: '{item_type}'")
        return None
    try:
        cursor = conn.cursor()
        cursor.execute(f"SELECT * FROM {table} WHERE id = ?", (item_id,))
        row = cursor.fetchone()
        if not row:
            logging.warning(f"Nessun dettaglio trovato per {item_type} ID {item_id}.")
        return row
    except sqlite3.Error as e:
        logger.error(f"Errore DB recupero dettagli {item_type} {item_id}: {e}")
        return None

def update_invoice_reconciliation_state(conn, invoice_id, payment_status, paid_amount):
    try:
        cursor = conn.cursor()
        # Assicura che paid_amount sia Decimal prima di quantize e conversione
        paid_amount_dec = quantize(to_decimal(paid_amount, default='0.0'))
        paid_amount_float = float(paid_amount_dec)
        now_ts = datetime.now()
        cursor.execute("UPDATE Invoices SET payment_status = ?, paid_amount = ?, updated_at = ? WHERE id = ?",
                       (payment_status, paid_amount_float, now_ts, invoice_id))
        if cursor.rowcount > 0:
            logger.debug(f"Aggiornato stato/importo fattura {invoice_id}: Status='{payment_status}', Paid={paid_amount_float:.2f}")
            return True
        else:
            logging.warning(f"Nessuna fattura trovata con ID {invoice_id} per aggiornamento stato/importo.")
            return False
    except (sqlite3.Error, ValueError, InvalidOperation) as e:
        logger.error(f"Errore DB/Dati update stato fattura {invoice_id}: {e}")
        return False
    except Exception as e_gen:
        logger.error(f"Errore generico update stato fattura {invoice_id}: {e_gen}", exc_info=True)
        return False

def update_transaction_reconciliation_state(conn, transaction_id, reconciliation_status, reconciled_amount):
    try:
        cursor = conn.cursor()
        # Assicura che reconciled_amount sia Decimal prima di quantize e conversione
        reconciled_amount_dec = quantize(to_decimal(reconciled_amount, default='0.0'))
        reconciled_amount_float = float(reconciled_amount_dec)
        cursor.execute("UPDATE BankTransactions SET reconciliation_status = ?, reconciled_amount = ? WHERE id = ?",
                       (reconciliation_status, reconciled_amount_float, transaction_id))
        if cursor.rowcount > 0:
            logger.debug(f"Aggiornato stato/importo transazione {transaction_id}: Status='{reconciliation_status}', Reconciled={reconciled_amount_float:.2f}")
            return True
        else:
            logging.warning(f"Nessuna transazione trovata con ID {transaction_id} per aggiornamento stato/importo.")
            return False
    except (sqlite3.Error, ValueError, InvalidOperation) as e:
        logger.error(f"Errore DB/Dati update stato transazione {transaction_id}: {e}")
        return False
    except Exception as e_gen:
        logger.error(f"Errore generico update stato transazione {transaction_id}: {e_gen}", exc_info=True)
        return False

def add_or_update_reconciliation_link(conn, invoice_id, transaction_id, amount_to_add):
    cursor = conn.cursor()
    try:
        # Assicura che amount_to_add sia Decimal
        amount_dec = quantize(to_decimal(amount_to_add))
        if amount_dec.copy_abs() < AMOUNT_TOLERANCE / 2:
            logging.warning(f"Tentativo link importo trascurabile ({amount_dec:.4f}) tra I:{invoice_id} e T:{transaction_id}. Ignorato.")
            return True # Consideralo successo (nessuna azione necessaria)

        cursor.execute("SELECT id, reconciled_amount FROM ReconciliationLinks WHERE invoice_id = ? AND transaction_id = ?", (invoice_id, transaction_id))
        existing_link = cursor.fetchone()
        now_ts = datetime.now()

        if existing_link:
            link_id = existing_link['id']
            current_amount = to_decimal(existing_link['reconciled_amount'])
            new_amount = quantize(current_amount + amount_dec)
            cursor.execute("UPDATE ReconciliationLinks SET reconciled_amount = ?, reconciliation_date = ? WHERE id = ?",
                           (float(new_amount), now_ts, link_id))
            logging.debug(f"Aggiornato link {link_id} (I:{invoice_id}, T:{transaction_id}): Add={amount_dec:.2f}, NewTotalLink={new_amount:.2f}")
        else:
            cursor.execute("INSERT INTO ReconciliationLinks (invoice_id, transaction_id, reconciled_amount, reconciliation_date) VALUES (?, ?, ?, ?)",
                           (invoice_id, transaction_id, float(amount_dec), now_ts))
            new_link_id = cursor.lastrowid
            logging.debug(f"Inserito nuovo link ID:{new_link_id} (I:{invoice_id}, T:{transaction_id}): Importo={amount_dec:.2f}")
        return True
    except (sqlite3.Error, ValueError, InvalidOperation) as e:
        logger.error(f"Errore DB/Dati add/update link (I:{invoice_id}, T:{transaction_id}): {e}")
        return False
    except Exception as e_gen:
        logger.error(f"Errore generico add/update link (I:{invoice_id}, T:{transaction_id}): {e_gen}", exc_info=True)
        return False

def remove_reconciliation_links(conn, transaction_id=None, invoice_id=None):
    if transaction_id is None and invoice_id is None:
        logging.warning("Chiamata a remove_reconciliation_links senza specificare transaction_id né invoice_id.")
        return False, ([], [])

    cursor = conn.cursor()
    affected_invoices = set()
    affected_transactions = set()
    id_type = ""
    primary_id = None

    try:
        if transaction_id is not None:
            where_clause = "transaction_id = ?"
            primary_id = transaction_id
            id_type = "T"
            affected_transactions.add(transaction_id)
            cursor.execute("SELECT DISTINCT invoice_id FROM ReconciliationLinks WHERE transaction_id = ?", (transaction_id,))
            affected_invoices.update(row['invoice_id'] for row in cursor.fetchall())
        elif invoice_id is not None:
            where_clause = "invoice_id = ?"
            primary_id = invoice_id
            id_type = "I"
            affected_invoices.add(invoice_id)
            cursor.execute("SELECT DISTINCT transaction_id FROM ReconciliationLinks WHERE invoice_id = ?", (invoice_id,))
            affected_transactions.update(row['transaction_id'] for row in cursor.fetchall())

        if primary_id is None:
            logging.error("ID primario non valido per remove_reconciliation_links.")
            return False, ([], [])

        # Non serve controllare se ci sono link, DELETE non darà errore se non trova nulla
        delete_query = f"DELETE FROM ReconciliationLinks WHERE {where_clause}"
        cursor.execute(delete_query, (primary_id,))
        deleted_count = cursor.rowcount
        if deleted_count > 0:
            logging.info(f"Rimossi {deleted_count} link associati a {id_type}:{primary_id}.")
        else:
            # Aggiungi comunque l'ID primario alla lista degli affetti per forzare aggiornamento stato
            if id_type == "T": affected_transactions.add(primary_id)
            else: affected_invoices.add(primary_id)
            logging.debug(f"Nessun link trovato da rimuovere per {id_type}:{primary_id}, ma lo includo per aggiornamento stato.")
        return True, (list(affected_invoices), list(affected_transactions))

    except sqlite3.Error as e:
        primary_info = f"{id_type}:{primary_id}" if primary_id else "ID non specificato"
        logger.error(f"Errore DB durante rimozione link ({primary_info}): {e}")
        return False, ([], [])
    except Exception as e_gen:
        primary_info = f"{id_type}:{primary_id}" if primary_id else "ID non specificato"
        logger.error(f"Errore generico durante rimozione link ({primary_info}): {e_gen}", exc_info=True)
        return False, ([], [])
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/core/database.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/core/importer.py ---
# core/importer.py

import os
import zipfile
import logging
import tempfile
import shutil
import sqlite3
import configparser
# ### MODIFICA: Aggiunti import mancanti ###
from decimal import Decimal, InvalidOperation
import pandas as pd
from datetime import datetime
import re # Importa re per _is_own_company se serve
# ### FINE MODIFICA ###

# Import funzioni dagli altri moduli core
# ### MODIFICA: Assicurato import robusto ###
try:
    from .parser_xml import parse_fattura_xml
    from .parser_p7m import extract_xml_from_p7m, _cleanup_temp_file
    from .parser_csv import parse_bank_csv
    from .database import (get_connection, add_anagraphics_if_not_exists,
                         check_entity_duplicate, add_transactions)
    from .utils import to_decimal, quantize
except ImportError:
    logging.warning("Import relativo fallito in importer.py, tento import assoluto.")
    try:
        from parser_xml import parse_fattura_xml
        from parser_p7m import extract_xml_from_p7m, _cleanup_temp_file
        from parser_csv import parse_bank_csv
        from database import (get_connection, add_anagraphics_if_not_exists,
                              check_entity_duplicate, add_transactions)
        from utils import to_decimal, quantize
    except ImportError as e:
        logging.critical(f"Impossibile importare dipendenze in importer.py: {e}")
        raise ImportError(f"Impossibile importare dipendenze in importer.py: {e}") from e
# ### FINE MODIFICA ###


logger = logging.getLogger(__name__)

def _is_metadata_file(filename):
    """Verifica se un filename è un file di metadati comune (es. macOS)."""
    base = os.path.basename(filename)
    return base.startswith('._') or base == '.DS_Store'


# --- Funzione add_invoice_data (MODIFICATA per quantize e robustezza) ---
def add_invoice_data(cursor, invoice_data, counterparty_anagraphics_id, p7m_source=None):
    """Aggiunge dati fattura, righe e IVA al DB. Applica quantize prima di salvare."""
    general_data = invoice_data.get('body', {}).get('general_data', {})
    invoice_type = invoice_data.get('type')
    invoice_hash = invoice_data.get('unique_hash')
    doc_number = general_data.get('doc_number', 'N/A') # Usato per logging

    if not invoice_hash:
        logger.error(f"Hash mancante per fattura {doc_number}. Impossibile inserire."); return None, False
    if check_entity_duplicate(cursor, 'Invoices', 'unique_hash', invoice_hash):
        logger.warning(f"Fattura duplicata (hash): {doc_number}"); return None, True # Ritorna True per duplicato

    doc_date_str = general_data.get('doc_date');
    total_amount_dec = general_data.get('total_amount') # Dovrebbe essere Decimal

    try:
        # Validazione dati essenziali
        if not doc_number or doc_number == 'N/A' or not doc_date_str or total_amount_dec is None or not isinstance(total_amount_dec, Decimal) or not total_amount_dec.is_finite():
            raise ValueError(f"Dati generali fattura invalidi/mancanti: Num='{doc_number}', Data='{doc_date_str}', Tot={total_amount_dec}")

        doc_date = pd.to_datetime(doc_date_str, errors='coerce').strftime('%Y-%m-%d') if doc_date_str else None
        due_date_str = general_data.get('due_date')
        due_date = pd.to_datetime(due_date_str, errors='coerce').strftime('%Y-%m-%d') if due_date_str else None
        if not doc_date: raise ValueError(f"Formato Data Doc non valido: {doc_date_str}")

        # Quantizza total_amount prima di convertire a float
        total_amount_float = float(quantize(total_amount_dec))

        inv_sql = """INSERT INTO Invoices (anagraphics_id, type, doc_type, doc_number, doc_date, total_amount, due_date, payment_method, xml_filename, p7m_source_file, unique_hash, updated_at, paid_amount) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 0.0)"""
        inv_params = (counterparty_anagraphics_id, invoice_type, general_data.get('doc_type'), doc_number, doc_date, total_amount_float, due_date, general_data.get('payment_method'), os.path.basename(invoice_data.get('source_file', '')), os.path.basename(p7m_source) if p7m_source else None, invoice_hash, datetime.now())
        cursor.execute(inv_sql, inv_params); invoice_id = cursor.lastrowid
        logging.debug(f"Invoice ID {invoice_id} ('{doc_number}') inserito.")

        # Righe: Prendi i Decimal da invoice_data, applica quantize, converti a float
        lines_data = []
        for i, line in enumerate(invoice_data.get('body', {}).get('lines', [])):
            try:
                qty_dec = line.get('quantity')
                unit_p_dec = line.get('unit_price')
                tot_p_dec = line.get('total_price', Decimal('0.0')) # Usa default se manca
                vat_r_dec = line.get('vat_rate', Decimal('0.0'))   # Usa default se manca

                # Conversione sicura a float, gestendo None e applicando quantize
                qty_float = float(quantize(qty_dec)) if qty_dec is not None and isinstance(qty_dec, Decimal) and qty_dec.is_finite() else None
                unit_p_float = float(quantize(unit_p_dec)) if unit_p_dec is not None and isinstance(unit_p_dec, Decimal) and unit_p_dec.is_finite() else None
                tot_p_float = float(quantize(tot_p_dec))
                vat_r_float = float(quantize(vat_r_dec))

                lines_data.append((
                    invoice_id, line.get('line_number', i + 1), line.get('description'),
                    qty_float, line.get('unit_measure'), unit_p_float, tot_p_float, vat_r_float,
                    line.get('item_code'), line.get('item_type')
                ))
            except (TypeError, ValueError, InvalidOperation, AttributeError) as line_prep_err:
                logger.error(f"Errore preparazione dati riga {i+1} fattura ID:{invoice_id} ('{doc_number}'): {line_prep_err} - Dati riga: {line}")
            except Exception as line_generic_err:
                logger.error(f"Errore generico prep riga {i+1} fattura ID:{invoice_id} ('{doc_number}'): {line_generic_err}", exc_info=True)

        if lines_data:
            line_sql = "INSERT INTO InvoiceLines (invoice_id, line_number, description, quantity, unit_measure, unit_price, total_price, vat_rate, item_code, item_type) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"
            try:
                cursor.executemany(line_sql, lines_data); logging.debug(f"Inserite {len(lines_data)} righe per fattura ID:{invoice_id}.")
            except sqlite3.Error as line_err:
                logger.error(f"Errore DB insert righe ID:{invoice_id} ('{doc_number}'): {line_err}")

        # Riepilogo IVA
        vat_summary_data = []
        for summary in invoice_data.get('body', {}).get('vat_summary', []):
             try:
                 # Applica quantize PRIMA di convertire a float
                 vat_r_sum = float(quantize(to_decimal(summary.get('vat_rate', '0.0'))))
                 tax_a = float(quantize(to_decimal(summary.get('taxable_amount', '0.0'))))
                 vat_a = float(quantize(to_decimal(summary.get('vat_amount', '0.0'))))
                 vat_summary_data.append((invoice_id, vat_r_sum, tax_a, vat_a))
             except Exception as vat_prep_err:
                 logger.error(f"Errore prep riepilogo IVA {summary.get('vat_rate')}% ID:{invoice_id} ('{doc_number}'): {vat_prep_err}")

        if vat_summary_data:
            vat_sql = "INSERT INTO InvoiceVATSummary (invoice_id, vat_rate, taxable_amount, vat_amount) VALUES (?, ?, ?, ?)"
            try:
                cursor.executemany(vat_sql, vat_summary_data); logging.debug(f"Inseriti {len(vat_summary_data)} riepiloghi IVA per fattura ID:{invoice_id}.")
            except sqlite3.Error as vat_err:
                logger.error(f"Errore DB insert IVA ID:{invoice_id} ('{doc_number}'): {vat_err}")

        return invoice_id, False # Successo, non duplicato

    # ### MODIFICA: Rimossa seconda parte duplicata del blocco try...except ###
    # La logica era duplicata, il primo blocco try...except è sufficiente.

    except ValueError as ve:
        logging.error(f"Errore dati fattura '{doc_number}': {ve}")
        return None, False
    except sqlite3.Error as dbe:
        logging.error(f"Errore DB insert fattura '{doc_number}': {dbe}")
        return None, False
    except Exception as e:
        logging.error(f"Errore generico insert fattura '{doc_number}': {e}", exc_info=True)
        return None, False
# ### FINE MODIFICA ###


# --- Funzione process_file MODIFICATA (passa my_company_data a parser)---
def process_file(filepath, conn, my_company_data):
    """Processa un singolo file (XML, P7M, CSV) e lo importa nel DB."""
    cursor = conn.cursor()
    _, ext = os.path.splitext(filepath); ext = ext.lower()
    status = 'Error - Initializing'; temp_xml_path = None; xml_data = None
    base_name = os.path.basename(filepath)
    file_type_processed = None

    try:
        if ext == '.p7m':
            file_type_processed = 'P7M'
            logger.debug(f"Processing P7M: {base_name}")
            temp_xml_path = extract_xml_from_p7m(filepath)
            if temp_xml_path:
                logger.debug(f"P7M estratto in: {temp_xml_path}")
                # Passa my_company_data al parser XML
                xml_data = parse_fattura_xml(temp_xml_path, my_company_data) # Passa qui
                if not xml_data: status = 'Error - XML Parse Failed (None from parse_fattura_xml)'; logger.error(f"{status} for {base_name}. XML temp: {temp_xml_path}")
                elif xml_data.get('error'): status = f"Error - XML Parse: {xml_data['error']}"; logger.error(f"{status} for {base_name}. XML temp: {temp_xml_path}")
            else: status = 'Error - P7M Extraction Failed'; logger.error(f"{status} for {base_name}")

        elif ext == '.xml':
            file_type_processed = 'XML'
            logger.debug(f"Processing XML: {base_name}")
            # Passa my_company_data al parser XML
            xml_data = parse_fattura_xml(filepath, my_company_data) # Passa qui
            if not xml_data: status = 'Error - XML Parse Failed (None from parse_fattura_xml)'; logger.error(f"{status} for {base_name}")
            elif xml_data.get('error'): status = f"Error - XML Parse: {xml_data['error']}"; logger.error(f"{status} for {base_name}")

        elif ext == '.csv':
            file_type_processed = 'CSV'
            logger.debug(f"Processing CSV: {base_name}")
            transactions_df = parse_bank_csv(filepath)
            if transactions_df is not None:
                if not transactions_df.empty:
                    inserted, db_duplicates, batch_duplicates, errors = add_transactions(cursor, transactions_df)
                    duplicates = db_duplicates + batch_duplicates
                    if errors > 0: status = f'Error - {errors} DB errors/prep errors during CSV insert'; logger.error(f"{status} for {base_name}")
                    elif inserted > 0: status = f'Success ({inserted} new)'; logger.info(f"CSV {base_name}: {status}, Duplicates (DB/File): {db_duplicates}/{batch_duplicates}")
                    elif duplicates > 0: status = f'Duplicate ({duplicates} existing/in file)'; logger.warning(f"CSV {base_name}: {status} (DB: {db_duplicates}, File: {batch_duplicates})")
                    else: status = 'Success - No new data'; logger.info(f"CSV {base_name}: {status}")
                else: status = 'Success - Empty/Filtered CSV'; logger.info(f"CSV {base_name}: {status}")
            else: status = 'Error - CSV Parse Failed'; logger.error(f"{status} for {base_name}")
            return status
        else:
            status = 'Unsupported File Type'; logger.warning(f"File non supportato: {base_name} ({ext})"); return status

        # --- Inserimento DB per XML/P7M (Usa tipo da parser) ---
        if xml_data and not xml_data.get('error'):
             logger.debug(f"Parsing {file_type_processed} OK for {base_name}. Inserting into DB...")
             invoice_type = xml_data.get('type') # TIPO DETERMINATO DA PARSER

             if not invoice_type or invoice_type == 'Unknown':
                 # Errore se il parser non ha potuto determinare il tipo
                 # (probabilmente a causa di dati config.ini mancanti/errati)
                 status = 'Error - Invoice type not determined by parser'
                 logger.error(f"{status} for {base_name}. Verifica P.IVA/CF in config.ini e nel file XML.")
             elif invoice_type == 'Autofattura':
                 status = 'Skipped - Autofattura (Not Implemented)'
                 logger.warning(f"File {base_name} è un'autofattura. Import non gestito.")
             else:
                 # Identifica controparte basandosi sul tipo determinato
                 ced_data = xml_data.get('anagraphics', {}).get('cedente', {})
                 ces_data = xml_data.get('anagraphics', {}).get('cessionario', {})
                 counterparty_id = None
                 counterparty_data = None
                 anag_type_for_counterparty = None

                 if invoice_type == 'Attiva':
                     counterparty_data = ces_data; anag_type_for_counterparty = 'Cliente'
                 elif invoice_type == 'Passiva':
                     counterparty_data = ced_data; anag_type_for_counterparty = 'Fornitore'

                 if counterparty_data and anag_type_for_counterparty:
                     # Inserisci/Trova ID controparte
                     counterparty_id = add_anagraphics_if_not_exists(cursor, counterparty_data, anag_type_for_counterparty)
                     if not counterparty_id:
                         status = f'Error - Counterparty ({anag_type_for_counterparty}) Insert/Find Failed'
                         logger.error(f"{status} for {base_name}")
                 else:
                     # Non dovrebbe accadere se type è Attiva/Passiva
                     status = 'Error - Could not determine Counterparty Data/Type (XML structure issue?)'
                     logger.error(f"{status} for {base_name}")

                 # Procedi con inserimento fattura solo se abbiamo ID controparte e nessun errore
                 if counterparty_id and status == 'Error - Initializing':
                     logger.debug(f"Controparte ID: {counterparty_id}. Inserisco fattura {base_name}...")
                     p7m_original_source = filepath if file_type_processed == 'P7M' else None
                     invoice_id, is_duplicate = add_invoice_data(cursor, xml_data, counterparty_id, p7m_source=p7m_original_source)
                     if is_duplicate: status = 'Duplicate'
                     elif invoice_id is not None: status = 'Success'
                     else: status = 'Error - Invoice Insert Failed'
                 elif status == 'Error - Initializing':
                      status = 'Error - Counterparty ID Missing/Error'
                      logger.error(f"{status} for {base_name}. Impossibile inserire fattura.")

        # Gestione errori parsing/estrazione iniziali
        elif status == 'Error - Initializing':
             if xml_data and xml_data.get('error'): status = f"Error - XML Parse: {xml_data.get('error', 'Unknown XML Parse Error')}"
             elif file_type_processed == 'P7M' and not temp_xml_path: status = 'Error - P7M Extraction Failed'
             elif file_type_processed in ['XML', 'P7M'] and not xml_data: status = 'Error - XML Parse Failed (None returned)'
             else: status = 'Error - Unknown Processing Failure'; logger.error(f"{status} for {base_name}")

    except sqlite3.Error as db_err: status = f'Critical DB Error: {db_err}'; logger.error(f"Errore DB non catturato per {base_name}: {db_err}", exc_info=True)
    except ValueError as ve: status = f"Error - Validation: {ve}"; logger.error(f"Errore validazione dati per {base_name}: {ve}", exc_info=True)
    except Exception as e: status = f'Critical Error: {e}'; logger.error(f"Errore critico process_file per {base_name}: {e}", exc_info=True)
    finally:
         if temp_xml_path: _cleanup_temp_file(temp_xml_path)
    return status


# --- Funzione import_from_source (MODIFICATA per leggere config e passare my_company_data) ---
def import_from_source(source_path, progress_callback=None):
    """
    Importa dati da un file singolo (XML, P7M, CSV) o da un file ZIP/directory.
    Ritorna un dizionario con i risultati dell'importazione.
    """
    results = {'processed': 0, 'success': 0, 'duplicates': 0, 'errors': 0, 'unsupported': 0, 'files': []}
    conn = None; temp_dir = None; files_to_process = []; processed_paths = set(); skipped_meta_files_count = 0; files_passed_to_process_file = 0

    # Lettura dati azienda dal config
    my_company_data = {'piva': None, 'cf': None}
    missing_config = False
    try:
        config = configparser.ConfigParser()
        # Percorso relativo alla directory del progetto (assumendo che importer.py sia in core/)
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        config_path = os.path.join(project_root, 'config.ini')
        if os.path.exists(config_path):
            config.read(config_path)
            my_company_data['piva'] = config.get('Azienda', 'PartitaIVA', fallback=None)
            my_company_data['cf'] = config.get('Azienda', 'CodiceFiscale', fallback=None)
            logger.info(f"Dati azienda letti da config: PIVA={my_company_data['piva']}, CF={my_company_data['cf']}")
            # Verifica essenziale per fatture
            if not my_company_data['piva'] and not my_company_data['cf']:
                logger.error("ERRORE CRITICO: Né PartitaIVA né CodiceFiscale specificati in config.ini [Azienda]. Impossibile determinare tipo fattura (Attiva/Passiva). L'importazione di XML/P7M fallirà.")
                missing_config = True # Segnala come se mancasse il config per bloccare
        else:
            logger.error(f"ERRORE CRITICO: Config file '{config_path}' non trovato. Impossibile validare anagrafica aziendale e determinare tipo fattura.")
            missing_config = True # Segnala config mancante

    except Exception as e:
        logger.error(f"Errore lettura config per dati azienda: {e}")
        missing_config = True # Segnala errore lettura config

    # Blocca import se manca config o dati essenziali
    if missing_config:
         results['errors'] = 1; results['files'].append({'name': source_path, 'status': 'Error - Config.ini mancante, illeggibile o senza P.IVA/CF azienda'}); return results

    try:
        # Identifica e raccogli file da processare
        if os.path.isfile(source_path):
            if zipfile.is_zipfile(source_path):
                logger.info(f"Input ZIP: {source_path}. Estraggo..."); temp_dir = tempfile.mkdtemp(prefix="fattura_import_")
                try:
                    with zipfile.ZipFile(source_path, 'r') as zip_ref: zip_ref.extractall(temp_dir)
                    logger.info(f"ZIP estratto in: {temp_dir}. Raccolgo file...");
                    for root, _, files in os.walk(temp_dir):
                        for filename in files:
                            full_path = os.path.join(root, filename)
                            if not _is_metadata_file(filename) and full_path not in processed_paths: files_to_process.append(full_path); processed_paths.add(full_path)
                            elif _is_metadata_file(filename): skipped_meta_files_count += 1
                except zipfile.BadZipFile: raise ValueError("File ZIP non valido o corrotto.")
                except Exception as zip_err: raise IOError(f"Errore estrazione ZIP: {zip_err}") from zip_err
            else:
                if not _is_metadata_file(source_path): files_to_process = [source_path]
                else: skipped_meta_files_count += 1
        elif os.path.isdir(source_path):
            logger.info(f"Input directory: {source_path}. Raccolgo file...");
            for root, _, files in os.walk(source_path):
                 for filename in files:
                     full_path = os.path.join(root, filename)
                     if not _is_metadata_file(filename) and full_path not in processed_paths: files_to_process.append(full_path); processed_paths.add(full_path)
                     elif _is_metadata_file(filename): skipped_meta_files_count += 1
        else: raise FileNotFoundError(f"Percorso non valido: {source_path}")

        files_to_process.sort(); total_files_to_process = len(files_to_process)
        results['unsupported'] = skipped_meta_files_count
        if total_files_to_process == 0:
            logger.warning(f"Nessun file valido trovato in '{source_path}' (esclusi {skipped_meta_files_count} file metadati).")
            results['processed'] = skipped_meta_files_count
            return results

        # Processamento file
        logger.info(f"Inizio processamento di {total_files_to_process} file...")
        conn = get_connection(); conn.execute('BEGIN TRANSACTION')

        for i, fpath in enumerate(files_to_process):
             files_passed_to_process_file += 1; base_name = os.path.basename(fpath)
             rel_path_for_log = os.path.relpath(fpath, start=temp_dir if temp_dir else source_path)
             logger.info(f"Processo file {files_passed_to_process_file}/{total_files_to_process}: '{rel_path_for_log}'")
             if progress_callback:
                 try: progress_callback(files_passed_to_process_file, total_files_to_process)
                 except InterruptedError: raise # Propaga interruzione
                 except Exception as cb_err: logger.warning(f"Errore callback progresso: {cb_err}")

             # Passa dati azienda a process_file
             file_status = process_file(fpath, conn, my_company_data)
             results['files'].append({'name': base_name, 'status': file_status})

             # Aggiorna contatori
             if file_status.startswith('Success'): results['success'] += 1
             elif file_status == 'Duplicate': results['duplicates'] += 1 # Solo duplicati fattura
             elif file_status.startswith('Error'): results['errors'] += 1
             elif file_status == 'Unsupported File Type' or file_status.startswith('Skipped'): results['unsupported'] += 1
             else: logger.warning(f"Stato file '{file_status}' non riconosciuto per {base_name}. Contato come errore."); results['errors'] += 1

        conn.commit(); logger.info(f"Transazione DB completata con successo.")

    except (ValueError, IOError, FileNotFoundError, InterruptedError) as user_err:
        logger.error(f"Errore input/file o annullamento durante importazione: {user_err}")
        if conn:
            try:
                conn.rollback()
            # ### CORREZIONE: Indentazione except ### START
            except Exception as rb_err:
                logger.error(f"Errore durante il rollback (user_err): {rb_err}")
            # ### CORREZIONE: Indentazione except ### END
        # Calcola errori residui
        # Assicurati che results['errors'] non diventi negativo
        processed_count = results['success'] + results['duplicates'] + results['unsupported']
        results['errors'] = max(0, files_passed_to_process_file - processed_count) + 1 # Aggiungi 1 per l'errore corrente
        if isinstance(user_err, InterruptedError): results['files'].append({'name':source_path, 'status': 'Error - Import Annullato'})
        else: results['files'].append({'name':source_path, 'status': f'Error: {user_err}'})
        # Nota: non è necessario aggiornare results['processed'] qui, lo facciamo alla fine

    except sqlite3.Error as db_err:
        logger.error(f"Errore DB CRITICO durante importazione, rollback in corso: {db_err}", exc_info=True)
        if conn:
            try:
                conn.rollback()
            # ### CORREZIONE: Indentazione except ### START
            except Exception as rb_err:
                logger.error(f"Errore durante il rollback (db_err): {rb_err}")
            # ### CORREZIONE: Indentazione except ### END
        processed_count = results['success'] + results['duplicates'] + results['unsupported']
        results['errors'] = max(0, files_passed_to_process_file - processed_count) + 1
        results['files'].append({'name':'DATABASE ERROR', 'status': f'Critical DB Error: {db_err}'})

    except Exception as e:
        logger.error(f"Errore CRITICO imprevisto durante importazione: {e}", exc_info=True)
        if conn:
            try:
                conn.rollback()
            # ### CORREZIONE: Indentazione except ### START
            except Exception as rb_err:
                logger.error(f"Errore durante il rollback (generic_err): {rb_err}")
            # ### CORREZIONE: Indentazione except ### END
        processed_count = results['success'] + results['duplicates'] + results['unsupported']
        results['errors'] = max(0, files_passed_to_process_file - processed_count) + 1
        results['files'].append({'name':'CRITICAL PYTHON ERROR', 'status': f'Critical Error: {e}'})
    finally:
        if conn:
            try:
                conn.close()
                logger.debug("Connessione DB chiusa.")
            except Exception as close_err:
                logger.error(f"Errore chiusura connessione DB: {close_err}")
        if temp_dir and os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
                logger.info(f"Directory temporanea {temp_dir} rimossa.")
            except Exception as e_clean:
                logger.warning(f"Impossibile rimuovere directory temporanea {temp_dir}: {e_clean}")

    # Ricalcola 'processed' alla fine
    # 'duplicates' include solo duplicati fattura hash
    results['processed'] = results['success'] + results['duplicates'] + results['errors'] + results['unsupported']
    logger.debug(f"Conteggio finale ricalcolato: Processed={results['processed']}")
    logger.info(f"Importazione completata. Risultati: Success={results['success']}, Dup.Fatt={results['duplicates']}, Errors={results['errors']}, Unsupp/Skip={results['unsupported']} / Total Processed={results['processed']}")
    return results
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/core/importer.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/core/parser_csv.py ---
import pandas as pd
import io
import logging
import chardet
import re
from .utils import calculate_transaction_hash, to_decimal, quantize # Assicurati che utils sia importato

logger = logging.getLogger(__name__)

EXPECTED_COLUMNS = {
    'DataContabile': ['DATA', 'Data operazione', 'Date', 'Transaction Date'],
    'DataValuta': ['VALUTA', 'Data valuta', 'Value Date'],
    'ImportoDare': ['DARE', 'Addebiti', 'Debit', 'Uscite'],
    'ImportoAvere': ['AVERE', 'Accrediti', 'Credit', 'Entrate'],
    'Descrizione': ['DESCRIZIONE OPERAZIONE', 'Descrizione', 'Description', 'Dettagli'],
    'CausaleABI': ['CAUSALE ABI', 'Causale', 'Codice Causale', 'ABI Code']
}

# --- detect_encoding e find_column_names (INVARIATI rispetto all'ultima versione) ---
def detect_encoding(file_path_or_obj):
    # ... (codice invariato) ...
    if isinstance(file_path_or_obj, str):
        try:
            with open(file_path_or_obj, 'rb') as f:
                raw_data = f.read(10000)
        except IOError as e:
            logger.error(f"Impossibile leggere il file per rilevare l'encoding: {e}")
            return None
    elif hasattr(file_path_or_obj, 'read') and hasattr(file_path_or_obj, 'seek'):
        original_pos = file_path_or_obj.tell()
        file_path_or_obj.seek(0)
        raw_data = file_path_or_obj.read(10000)
        file_path_or_obj.seek(original_pos)
    else:
        logger.error("Input non valido per detect_encoding (né path né stream leggibile).")
        return None
    if not raw_data:
        logger.warning("Dati vuoti per rilevamento encoding.")
        return 'utf-8'
    try:
        result = chardet.detect(raw_data)
        encoding = result['encoding']
        confidence = result['confidence']
        logger.info(f"Rilevato encoding: {encoding} con confidenza: {confidence:.2f}")
        if confidence < 0.7:
             logger.warning(f"Confidenza encoding bassa ({confidence:.2f}). Potrebbe essere impreciso.")
        return encoding if encoding else 'utf-8'
    except Exception as e:
        logger.error(f"Errore durante rilevamento encoding con chardet: {e}")
        return 'utf-8'

def find_column_names(df_columns):
    # ... (codice invariato) ...
    column_mapping = {}
    found_columns = {}
    df_cols_lower = {col.lower().strip(): col for col in df_columns}
    for standard_name, possible_aliases in EXPECTED_COLUMNS.items():
        found = False
        for alias in possible_aliases:
            alias_lower = alias.lower().strip()
            if alias_lower in df_cols_lower:
                original_col_name = df_cols_lower[alias_lower]
                if original_col_name not in found_columns.values():
                    column_mapping[standard_name] = original_col_name
                    found_columns[standard_name] = original_col_name
                    logger.debug(f"Mappato '{standard_name}' -> '{original_col_name}'")
                    found = True
                    break
        if not found and standard_name not in ['CausaleABI', 'DataValuta']:
            logger.warning(f"Colonna standard '{standard_name}' non trovata nel CSV (alias provati: {possible_aliases}).")
    required = ['DataContabile', 'ImportoDare', 'ImportoAvere', 'Descrizione']
    missing = [req for req in required if req not in column_mapping]
    if missing:
         logger.error(f"Colonne CSV essenziali mancanti dopo mappatura: {missing}")
         return None
    return column_mapping

# ### FUNZIONE parse_bank_csv (INVARIATO rispetto all'ultima versione - low_memory era già stato rimosso) ###
def parse_bank_csv(csv_filepath_or_stringio):
    # ... (Tutto il codice di questa funzione rimane come nella risposta precedente,
    #      assicurandosi che 'low_memory=False' sia stato rimosso dalla chiamata a pd.read_csv)
    df = None
    used_encoding = None
    file_obj = None
    source_name = csv_filepath_or_stringio if isinstance(csv_filepath_or_stringio, str) else "StringIO"

    try:
        detected_encoding = detect_encoding(csv_filepath_or_stringio)
        encodings_to_try = [detected_encoding] if detected_encoding else []
        encodings_to_try.extend(['utf-8', 'latin-1', 'cp1252'])
        encodings_to_try = [enc.lower() for enc in list(dict.fromkeys(encodings_to_try)) if enc]

        logger.info(f"Tentativo lettura CSV '{source_name}' con encodings: {encodings_to_try}")
        common_delimiters = [';', ',', '\t']

        for enc in encodings_to_try:
            for delim in common_delimiters:
                try:
                    logger.debug(f"Provo a leggere CSV con encoding='{enc}', delimiter='{delim}'")
                    if isinstance(csv_filepath_or_stringio, str):
                        file_obj = open(csv_filepath_or_stringio, 'r', encoding=enc, errors='replace')
                    elif hasattr(csv_filepath_or_stringio, 'seek'):
                        csv_filepath_or_stringio.seek(0)
                        file_obj = csv_filepath_or_stringio
                    else:
                        raise TypeError("Input deve essere un percorso file (str) o uno stream leggibile.")

                    df = pd.read_csv(
                        file_obj,
                        delimiter=delim,
                        decimal=',',
                        thousands='.',
                        parse_dates=False,
                        skipinitialspace=True,
                        on_bad_lines='warn',
                        encoding=enc,
                        # low_memory=False,  # Assicurati sia Rimosso/Commentato
                        engine='python',
                        skip_blank_lines=True
                    )
                    used_encoding = enc
                    logger.info(f"CSV '{source_name}' letto con successo usando encoding='{enc}' e delimiter='{delim}'.")

                    if df.shape[1] < 3:
                         logger.warning(f"CSV letto con {enc}/{delim} ha solo {df.shape[1]} colonne. Provo prossima combinazione.")
                         df = None; continue
                    break
                except (UnicodeDecodeError, pd.errors.ParserError) as e:
                    logger.warning(f"Errore lettura/parsing ({enc}/{delim}): {e}")
                    df = None
                except Exception as read_e:
                    logger.error(f"Errore imprevisto lettura CSV '{source_name}' ({enc}/{delim}): {read_e}", exc_info=True)
                    df = None
                    if isinstance(read_e, FileNotFoundError): raise read_e
                finally:
                    if isinstance(csv_filepath_or_stringio, str) and 'file_obj' in locals() and file_obj and not file_obj.closed:
                        try: file_obj.close()
                        except Exception: pass
            if df is not None: break

        if df is None:
            logger.error(f"Impossibile leggere o parsare CSV '{source_name}'."); return None

        initial_rows = len(df)
        logger.info(f"CSV letto ({initial_rows} righe). Mappatura e pulizia...")

        column_mapping = find_column_names(df.columns)
        if column_mapping is None: logger.error(f"Mappatura fallita per '{source_name}'."); return None

        df.rename(columns={v: k for k, v in column_mapping.items()}, inplace=True)
        standard_cols_present = list(column_mapping.keys())
        df = df[standard_cols_present].copy()

        if 'DataContabile' in df.columns:
             df['DataContabileParsed'] = pd.to_datetime(df['DataContabile'], dayfirst=True, errors='coerce')
             original_rows_date = len(df)
             df = df[df['DataContabileParsed'].notna()].copy()
             rows_after_date_filter = len(df)
             if original_rows_date > rows_after_date_filter: logger.warning(f"Rimosse {original_rows_date - rows_after_date_filter} righe con DataContabile non valida.")
             df = df[~df['DataContabile'].astype(str).str.lower().str.fullmatch(r'(?:data|date)')] # Usa fullmatch per evitare match parziali
             df['DataContabile'] = df['DataContabileParsed']
             df.drop(columns=['DataContabileParsed'], inplace=True)
        else: logger.error("Colonna 'DataContabile' mancante."); return None

        filter_keywords = ['Saldo iniziale', 'Saldo contabile', 'Saldo liquido', 'Disponibilità al',
                           'Giroconto', 'Canone mensile', 'Imposta di bollo', 'Competenze']
        if 'Descrizione' in df.columns:
            desc_col = 'Descrizione'
            df[desc_col] = df[desc_col].astype(str).str.strip()
            # Filtra per keyword O righe con solo 'EUR' O vuote
            extended_filter_pattern = f"({'|'.join(filter_keywords)})|^\\s*EUR\\s*$|^\\s*$"
            original_rows = len(df)
            df = df[~df[desc_col].str.contains(extended_filter_pattern, na=False, case=False, regex=True)].copy()
            rows_after_filter = len(df)
            if original_rows > rows_after_filter: logger.info(f"Filtrate {original_rows - rows_after_filter} righe non operative.")
        else: logger.warning("Colonna 'Descrizione' non trovata. Impossibile filtrare.")

        if 'ImportoDare' not in df.columns: df['ImportoDare'] = 0.0
        if 'ImportoAvere' not in df.columns: df['ImportoAvere'] = 0.0
        df['ImportoDareDec'] = df['ImportoDare'].apply(lambda x: to_decimal(x, default='0.0'))
        df['ImportoAvereDec'] = df['ImportoAvere'].apply(lambda x: to_decimal(x, default='0.0'))
        df['amount_dec'] = (df['ImportoAvereDec'] - df['ImportoDareDec']).apply(quantize)
        df['Importo'] = df['amount_dec'].astype(float) # Converti in float per DB (o TEXT?)

        if 'DataValuta' in df.columns: df['DataValuta'] = pd.to_datetime(df['DataValuta'], dayfirst=True, errors='coerce')
        else: df['DataValuta'] = pd.NaT
        if 'CausaleABI' in df.columns: df['CausaleABI'] = pd.to_numeric(df['CausaleABI'].astype(str).str.strip(), errors='coerce').astype('Int64')
        else: df['CausaleABI'] = pd.NA
        if 'Descrizione' not in df.columns: df['Descrizione'] = ''

        # Rimuovi eventuali righe dove l'importo non è valido (risulta NaN dopo to_decimal)
        rows_before_nan_drop = len(df)
        df.dropna(subset=['amount_dec'], inplace=True)
        if len(df) < rows_before_nan_drop:
             logger.warning(f"Rimosse {rows_before_nan_drop - len(df)} righe con importo non valido (NaN).")


        df['unique_hash'] = df.apply(lambda row: calculate_transaction_hash(
            row['DataContabile'], row['amount_dec'], row['Descrizione']
        ), axis=1)

        final_columns = ['DataContabile', 'DataValuta', 'Importo', 'Descrizione', 'CausaleABI', 'unique_hash']
        cols_to_select = [col for col in final_columns if col in df.columns]
        df_final = df[cols_to_select].copy()

        final_rows = len(df_final)
        logger.info(f"Parsing CSV '{source_name}' completato: {initial_rows} righe -> {final_rows} movimenti.")
        if initial_rows > 0 and final_rows == 0: logger.warning(f"Nessuna riga valida trovata dopo pulizia CSV '{source_name}'.")
        elif initial_rows > 0 and final_rows < initial_rows * 0.7: logger.warning(f"Oltre 30% righe scartate ({initial_rows - final_rows}) pulizia CSV '{source_name}'.")

        return df_final

    except FileNotFoundError: logger.error(f"File CSV non trovato: {csv_filepath_or_stringio}"); return None
    except pd.errors.EmptyDataError: logger.warning(f"CSV '{source_name}' vuoto."); return pd.DataFrame()
    except KeyError as ke: logger.error(f"Colonna essenziale '{ke}' mancante in CSV '{source_name}'.", exc_info=True); return None
    except Exception as e: logger.error(f"Errore generico parsing CSV '{source_name}': {e}", exc_info=True); return None
    finally:
        if isinstance(csv_filepath_or_stringio, str) and 'file_obj' in locals() and file_obj and hasattr(file_obj, 'closed') and not file_obj.closed:
            try: file_obj.close()
            except Exception: pass
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/core/parser_csv.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/core/parser_p7m.py ---
# core/parser_p7m.py - Versione migliorata per gestire meglio i file P7M

import subprocess
import tempfile
import os
import logging
import re
import shutil
from lxml import etree
import sys

logger = logging.getLogger(__name__)

OPENSSL_CMD = 'openssl'

def find_openssl():
    """Trova l'eseguibile openssl nel sistema con ricerca più estesa."""
    # Prima prova con which/where
    cmd_path = shutil.which(OPENSSL_CMD)
    if cmd_path and os.path.isfile(cmd_path):
        logger.info(f"Trovato OpenSSL (which): '{cmd_path}'")
        return cmd_path
    
    # Percorsi comuni per sistema operativo
    common_paths = []
    if sys.platform == "win32":
        common_paths = [
            # Git Bash
            os.path.join(os.environ.get("ProgramFiles", "C:\\Program Files"), "Git\\usr\\bin\\openssl.exe"),
            os.path.join(os.environ.get("ProgramFiles(x86)", "C:\\Program Files (x86)"), "Git\\usr\\bin\\openssl.exe"),
            # OpenSSL standalone
            os.path.join(os.environ.get("ProgramFiles", "C:\\Program Files"), "OpenSSL-Win64\\bin\\openssl.exe"),
            os.path.join(os.environ.get("ProgramFiles(x86)", "C:\\Program Files (x86)"), "OpenSSL-Win32\\bin\\openssl.exe"),
            "C:\\Program Files\\OpenSSL\\bin\\openssl.exe",
            "C:\\OpenSSL-Win64\\bin\\openssl.exe",
            "C:\\OpenSSL-Win32\\bin\\openssl.exe",
            # Chocolatey
            "C:\\ProgramData\\chocolatey\\bin\\openssl.exe",
            # MSYS2
            "C:\\msys64\\usr\\bin\\openssl.exe",
            # Altri percorsi comuni
            "C:\\tools\\openssl\\openssl.exe",
        ]
    elif sys.platform == "darwin":  # macOS
        common_paths = [
            '/usr/bin/openssl',
            '/usr/local/bin/openssl',
            '/opt/homebrew/bin/openssl',  # Apple Silicon
            '/opt/local/bin/openssl',     # MacPorts
            '/sw/bin/openssl',            # Fink
        ]
    else:  # Linux e altri Unix
        common_paths = [
            '/usr/bin/openssl',
            '/usr/local/bin/openssl',
            '/bin/openssl',
            '/usr/sbin/openssl',
            '/opt/openssl/bin/openssl',
        ]
    
    # Verifica percorsi comuni
    for p in common_paths:
        if os.path.exists(p) and os.access(p, os.X_OK):
            logger.info(f"Trovato OpenSSL (common path): '{p}'")
            return p
    
    # Verifica finale nel PATH
    try:
        use_shell = sys.platform == "win32"
        result = subprocess.run(
            [OPENSSL_CMD, 'version'], 
            capture_output=True, 
            check=True, 
            text=True, 
            timeout=10, 
            shell=use_shell
        )
        if result.returncode == 0:
            logger.info(f"OpenSSL trovato nel PATH: versione {result.stdout.strip()}")
            return OPENSSL_CMD
    except Exception as e_check:
        logger.debug(f"Verifica OpenSSL PATH fallita: {e_check}")
    
    logger.critical(f"FATAL: Comando '{OPENSSL_CMD}' non trovato nel sistema.")
    return None

def _cleanup_temp_file(filepath):
    """Rimuove un file temporaneo se esiste."""
    if filepath and os.path.exists(filepath):
        try:
            os.remove(filepath)
            logger.debug(f"File temp rimosso: {filepath}")
        except OSError as e:
            logger.warning(f"Impossibile rimuovere file temp {filepath}: {e}")

def _validate_extracted_xml(xml_path, base_name):
    """
    Valida che l'XML estratto sia effettivamente una fattura elettronica valida.
    """
    try:
        if not os.path.exists(xml_path) or os.path.getsize(xml_path) == 0:
            logger.error(f"File XML estratto vuoto o inesistente: {xml_path}")
            return False
        
        # Leggi e verifica contenuto
        with open(xml_path, 'rb') as f:
            xml_bytes = f.read()
        
        if not xml_bytes:
            logger.error(f"Contenuto XML vuoto per {base_name}")
            return False
        
        # Verifica se sembra un XML di fattura elettronica
        xml_str = xml_bytes.decode('utf-8', errors='ignore').lower()
        
        # Controlli di base per fattura elettronica
        fattura_keywords = [
            'fatturaelettronica',
            'fatturaelettronicaheader',
            'fatturaelettronicabody',
            'cedenteprestatore',
            'cessionariocommittente'
        ]
        
        keyword_found = any(keyword in xml_str for keyword in fattura_keywords)
        
        if not keyword_found:
            logger.warning(f"Il contenuto estratto da {base_name} non sembra una fattura elettronica")
            # Log primi 200 caratteri per debug
            preview = xml_str[:200].replace('\n', ' ').replace('\r', ' ')
            logger.debug(f"Preview contenuto: {preview}")
            return False
        
        # Prova parsing XML base per verificare validità
        try:
            parser = etree.XMLParser(recover=True)
            etree.fromstring(xml_bytes, parser)
            logger.debug(f"XML estratto validato per {base_name}")
            return True
        except etree.XMLSyntaxError as xml_err:
            logger.error(f"XML estratto da {base_name} non valido: {xml_err}")
            return False
    
    except Exception as validate_err:
        logger.error(f"Errore validazione XML estratto da {base_name}: {validate_err}")
        return False

def extract_xml_from_p7m_smime_robust(openssl_path, p7m_filepath, base_name):
    """
    Estrazione P7M più robusta con multiple strategie di estrazione.
    """
    temp_xml_file_path = None
    logger.info(f"Tentativo estrazione XML da {base_name} con strategia robusta...")
    
    try:
        # Crea file temporaneo
        tf = tempfile.NamedTemporaryFile(
            delete=False, 
            mode='wb', 
            suffix=".xml", 
            prefix=f"{base_name}_extract_"
        )
        temp_xml_file_path = tf.name
        tf.close()

        # Strategia 1: smime -verify con -noverify (standard)
        strategies = [
            {
                'name': 'smime_noverify',
                'command': [openssl_path, 'smime', '-verify', '-noverify', '-inform', 'DER', 
                           '-in', p7m_filepath, '-out', temp_xml_file_path]
            },
            {
                'name': 'smime_noverify_nosigs',
                'command': [openssl_path, 'smime', '-verify', '-noverify', '-nosigs', '-inform', 'DER',
                           '-in', p7m_filepath, '-out', temp_xml_file_path]
            },
            {
                'name': 'pkcs7_print',
                'command': [openssl_path, 'pkcs7', '-print', '-inform', 'DER', 
                           '-in', p7m_filepath, '-out', temp_xml_file_path]
            }
        ]

        for i, strategy in enumerate(strategies):
            try:
                logger.debug(f"Strategia {i+1}/{len(strategies)} ({strategy['name']}): {' '.join(strategy['command'])}")
                
                # Esegui comando
                process = subprocess.run(
                    strategy['command'], 
                    capture_output=True, 
                    check=False,  # Non fare raise su errori
                    timeout=60,   # Timeout più lungo per file grandi
                    text=False    # Mantieni output binario
                )
                
                stderr_output = process.stderr.decode('utf-8', errors='ignore') if process.stderr else ""
                
                # Log del risultato
                logger.debug(f"Strategia {strategy['name']}: RC={process.returncode}")
                if stderr_output:
                    logger.debug(f"Stderr: {stderr_output[:200]}")
                
                # Valida l'output indipendentemente dal return code
                if _validate_extracted_xml(temp_xml_file_path, base_name):
                    logger.info(f"Estrazione riuscita con strategia {strategy['name']} per {base_name}")
                    return temp_xml_file_path
                else:
                    logger.debug(f"Strategia {strategy['name']} ha prodotto output non valido")
                    continue
                    
            except subprocess.TimeoutExpired:
                logger.warning(f"Timeout strategia {strategy['name']} per {base_name}")
                continue
            except Exception as strategy_err:
                logger.debug(f"Errore strategia {strategy['name']} per {base_name}: {strategy_err}")
                continue
        
        # Se tutte le strategie falliscono
        logger.error(f"Tutte le strategie di estrazione fallite per {base_name}")
        _cleanup_temp_file(temp_xml_file_path)
        return None

    except Exception as e:
        logger.error(f"Errore critico estrazione P7M per {base_name}: {e}", exc_info=True)
        _cleanup_temp_file(temp_xml_file_path)
        return None

def extract_xml_from_p7m_alternative_methods(p7m_filepath, base_name):
    """
    Metodi alternativi per estrarre XML da P7M quando OpenSSL fallisce.
    Usa librerie Python pure quando disponibili.
    """
    logger.info(f"Tentativo estrazione alternativa per {base_name}")
    
    try:
        # Metodo 1: Lettura diretta per cercare contenuto XML embedded
        with open(p7m_filepath, 'rb') as f:
            p7m_content = f.read()
        
        # Cerca pattern XML nel contenuto binario
        xml_patterns = [
            rb'<?xml[^>]*>\s*<.*?FatturaElettronica',
            rb'<.*?FatturaElettronica',
            rb'<?xml[^>]*>\s*<.*?fattura',
        ]
        
        for pattern in xml_patterns:
            matches = re.search(pattern, p7m_content, re.IGNORECASE | re.DOTALL)
            if matches:
                # Trova l'inizio dell'XML
                xml_start = matches.start()
                
                # Trova la fine guardando per il tag di chiusura
                xml_end_patterns = [
                    rb'</.*?FatturaElettronica[^>]*>',
                    rb'</.*?fattura[^>]*>',
                ]
                
                xml_end = len(p7m_content)  # Default alla fine del file
                for end_pattern in xml_end_patterns:
                    end_matches = list(re.finditer(end_pattern, p7m_content[xml_start:], re.IGNORECASE))
                    if end_matches:
                        last_match = end_matches[-1]
                        xml_end = xml_start + last_match.end()
                        break
                
                # Estrai XML
                xml_content = p7m_content[xml_start:xml_end]
                
                # Pulisci contenuto XML
                try:
                    xml_str = xml_content.decode('utf-8', errors='ignore')
                    
                    # Verifica che sia XML valido
                    parser = etree.XMLParser(recover=True)
                    etree.fromstring(xml_str.encode('utf-8'), parser)
                    
                    # Salva in file temporaneo
                    tf = tempfile.NamedTemporaryFile(
                        delete=False, 
                        mode='w', 
                        suffix=".xml", 
                        prefix=f"{base_name}_alt_",
                        encoding='utf-8'
                    )
                    tf.write(xml_str)
                    tf.close()
                    
                    logger.info(f"Estrazione alternativa riuscita per {base_name}: {tf.name}")
                    return tf.name
                    
                except Exception as xml_process_err:
                    logger.debug(f"Errore processamento XML estratto: {xml_process_err}")
                    continue
        
        logger.warning(f"Nessun contenuto XML trovato con metodi alternativi in {base_name}")
        return None
        
    except Exception as e:
        logger.error(f"Errore metodi alternativi per {base_name}: {e}")
        return None

def detect_p7m_structure(p7m_filepath):
    """
    Analizza la struttura del file P7M per determinare la strategia di estrazione migliore.
    """
    try:
        with open(p7m_filepath, 'rb') as f:
            header = f.read(1024)  # Leggi primi 1KB
        
        structure_info = {
            'size': os.path.getsize(p7m_filepath),
            'has_xml_header': b'<?xml' in header,
            'has_fattura_tag': b'fattura' in header.lower(),
            'encoding_hints': []
        }
        
        # Detect encoding hints
        if b'utf-8' in header.lower():
            structure_info['encoding_hints'].append('utf-8')
        if b'iso-8859' in header.lower():
            structure_info['encoding_hints'].append('iso-8859-1')
        
        # Detect PKCS#7 structure
        if header.startswith(b'\x30\x82') or header.startswith(b'\x30\x80'):
            structure_info['pkcs7_structure'] = 'DER'
        elif header.startswith(b'-----BEGIN'):
            structure_info['pkcs7_structure'] = 'PEM'
        else:
            structure_info['pkcs7_structure'] = 'Unknown'
        
        logger.debug(f"Struttura P7M rilevata: {structure_info}")
        return structure_info
        
    except Exception as e:
        logger.warning(f"Errore analisi struttura P7M: {e}")
        return {'size': 0, 'pkcs7_structure': 'Unknown'}

def extract_xml_from_p7m(p7m_filepath):
    """
    Funzione principale migliorata per estrazione XML da P7M.
    Usa multiple strategie per massimizzare il successo.
    """
    if not os.path.exists(p7m_filepath):
        logger.error(f"File P7M non trovato: {p7m_filepath}")
        return None
    
    base_name = os.path.basename(p7m_filepath)
    logger.info(f"Inizio estrazione XML da P7M: {base_name}")
    
    # Analizza struttura file
    structure_info = detect_p7m_structure(p7m_filepath)
    
    # Trova OpenSSL
    openssl_path = find_openssl()
    
    extracted_path = None
    
    # Strategia 1: OpenSSL (se disponibile)
    if openssl_path:
        logger.info(f"Tentativo estrazione con OpenSSL per {base_name}")
        extracted_path = extract_xml_from_p7m_smime_robust(openssl_path, p7m_filepath, base_name)
    else:
        logger.warning(f"OpenSSL non disponibile, salto estrazione OpenSSL per {base_name}")
    
    # Strategia 2: Metodi alternativi se OpenSSL fallisce
    if not extracted_path:
        logger.info(f"Tentativo estrazione con metodi alternativi per {base_name}")
        extracted_path = extract_xml_from_p7m_alternative_methods(p7m_filepath, base_name)
    
    # Strategia 3: Tentativo con librerie esterne se disponibili
    if not extracted_path:
        extracted_path = try_external_libraries_extraction(p7m_filepath, base_name)
    
    if extracted_path:
        logger.info(f"Estrazione XML completata con successo: {base_name} -> {extracted_path}")
        
        # Log dimensioni per verifica
        try:
            xml_size = os.path.getsize(extracted_path)
            p7m_size = structure_info.get('size', 0)
            logger.debug(f"Dimensioni: P7M={p7m_size} bytes, XML estratto={xml_size} bytes")
        except:
            pass
            
        return extracted_path
    else:
        logger.error(f"Tutte le strategie di estrazione fallite per {base_name}")
        return None

def try_external_libraries_extraction(p7m_filepath, base_name):
    """
    Tenta estrazione usando librerie Python esterne se disponibili.
    """
    try:
        # Prova con cryptography library se disponibile
        try:
            from cryptography.hazmat.primitives import serialization
            from cryptography.hazmat.primitives.serialization import pkcs7
            
            logger.debug(f"Tentativo estrazione con cryptography library per {base_name}")
            
            with open(p7m_filepath, 'rb') as f:
                p7m_data = f.read()
            
            # Prova a parsare come PKCS#7
            try:
                # Carica il certificato PKCS#7
                p7m_cert = pkcs7.load_der_pkcs7_certificates(p7m_data)
                logger.debug(f"PKCS#7 caricato, ma cryptography non supporta estrazione contenuto")
                # La libreria cryptography non supporta l'estrazione del contenuto
                # È principalmente per gestione certificati
                
            except Exception as crypto_err:
                logger.debug(f"Errore caricamento PKCS#7 con cryptography: {crypto_err}")
                
        except ImportError:
            logger.debug("Libreria cryptography non disponibile")
        
        # Prova con altre librerie se disponibili
        try:
            import M2Crypto
            logger.debug(f"Tentativo estrazione con M2Crypto per {base_name}")
            # Implementazione M2Crypto qui se necessario
            
        except ImportError:
            logger.debug("Libreria M2Crypto non disponibile")
        
        return None
        
    except Exception as e:
        logger.debug(f"Errore tentativo librerie esterne per {base_name}: {e}")
        return None

def verify_extraction_quality(extracted_xml_path, original_p7m_path):
    """
    Verifica la qualità dell'estrazione confrontando con l'originale.
    """
    try:
        base_name = os.path.basename(original_p7m_path)
        
        # Controlli di base
        if not os.path.exists(extracted_xml_path):
            logger.error(f"File XML estratto non esiste: {extracted_xml_path}")
            return False
        
        xml_size = os.path.getsize(extracted_xml_path)
        if xml_size == 0:
            logger.error(f"File XML estratto vuoto per {base_name}")
            return False
        
        # Verifica contenuto XML
        try:
            with open(extracted_xml_path, 'r', encoding='utf-8', errors='ignore') as f:
                xml_content = f.read(1000)  # Primi 1000 caratteri
            
            # Controlli qualità contenuto
            quality_checks = {
                'has_xml_declaration': xml_content.strip().startswith('<?xml'),
                'has_fattura_tag': 'FatturaElettronica' in xml_content or 'fattura' in xml_content.lower(),
                'has_proper_structure': any(tag in xml_content for tag in ['CedentePrestatore', 'CessionarioCommittente']),
                'no_binary_garbage': not any(ord(c) < 32 and c not in '\n\r\t' for c in xml_content[:500])
            }
            
            passed_checks = sum(quality_checks.values())
            total_checks = len(quality_checks)
            
            logger.debug(f"Controlli qualità per {base_name}: {passed_checks}/{total_checks} passati")
            logger.debug(f"Dettagli: {quality_checks}")
            
            # Almeno 3/4 controlli devono passare
            if passed_checks >= 3:
                logger.info(f"Estrazione di buona qualità per {base_name}")
                return True
            else:
                logger.warning(f"Estrazione di qualità dubbia per {base_name}: {passed_checks}/{total_checks} controlli passati")
                return False
                
        except Exception as content_err:
            logger.error(f"Errore verifica contenuto XML per {base_name}: {content_err}")
            return False
            
    except Exception as e:
        logger.error(f"Errore verifica qualità estrazione: {e}")
        return False

# Funzione di utilità per testing e debug
def test_p7m_extraction(p7m_filepath, verbose=True):
    """
    Funzione di test per verificare l'estrazione P7M.
    Utile per debugging di file problematici.
    """
    if verbose:
        print(f"\n=== TEST ESTRAZIONE P7M ===")
        print(f"File: {p7m_filepath}")
        print(f"Esiste: {os.path.exists(p7m_filepath)}")
        
        if os.path.exists(p7m_filepath):
            print(f"Dimensione: {os.path.getsize(p7m_filepath)} bytes")
    
    # Test OpenSSL availability
    openssl_path = find_openssl()
    if verbose:
        print(f"OpenSSL disponibile: {openssl_path is not None}")
        if openssl_path:
            print(f"Percorso OpenSSL: {openssl_path}")
    
    # Analizza struttura
    structure = detect_p7m_structure(p7m_filepath)
    if verbose:
        print(f"Struttura rilevata: {structure}")
    
    # Tenta estrazione
    extracted_path = extract_xml_from_p7m(p7m_filepath)
    
    if verbose:
        print(f"Estrazione riuscita: {extracted_path is not None}")
        if extracted_path:
            print(f"File estratto: {extracted_path}")
            print(f"Dimensione estratto: {os.path.getsize(extracted_path)} bytes")
            
            # Verifica qualità
            quality_ok = verify_extraction_quality(extracted_path, p7m_filepath)
            print(f"Qualità estrazione: {'OK' if quality_ok else 'PROBLEMATICA'}")
        
        print("=" * 40)
    
    return extracted_path
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/core/parser_p7m.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/core/parser_xml.py ---
# core/parser_xml.py - Versione migliorata per gestire meglio le fatture passive

from lxml import etree
import logging
import os
import re
from decimal import Decimal
from datetime import datetime

try:
    from .utils import calculate_invoice_hash, to_decimal, quantize, _is_own_company
except ImportError:
    logging.warning("Import relativo .utils fallito in parser_xml.py, tento import assoluto.")
    from utils import calculate_invoice_hash, to_decimal, quantize, _is_own_company

logger = logging.getLogger(__name__)

# === FUNZIONI HELPER XPATH MIGLIORATE ===
def xpath_get_text_robust(element, queries, default="", normalize_space=True, log_attempts=False):
    """
    Versione robusta che prova multiple query XPath per lo stesso dato.
    Utile quando i fornitori usano strutture XML diverse.
    """
    if element is None:
        return default
    
    # Se queries è una stringa, convertila in lista
    if isinstance(queries, str):
        queries = [queries]
    
    for i, query in enumerate(queries):
        try:
            result = element.xpath(query)
            if result:
                text_content = None
                if hasattr(result[0], 'text') and result[0].text is not None:
                    text_content = result[0].text
                elif isinstance(result[0], str):
                    text_content = result[0]
                
                if text_content is not None and text_content.strip():
                    final_text = ' '.join(text_content.split()) if normalize_space else text_content.strip()
                    if log_attempts:
                        logger.debug(f"XPath query {i+1}/{len(queries)} successo: '{query}' -> '{final_text}'")
                    return final_text
        except Exception as e:
            if log_attempts:
                logger.debug(f"XPath query {i+1}/{len(queries)} fallita: '{query}' -> {e}")
            continue
    
    if log_attempts and len(queries) > 1:
        logger.warning(f"Tutte le {len(queries)} query XPath fallite. Uso default: '{default}'")
    
    return default

def xpath_get_text(element, query, default="", normalize_space=True):
    """Versione originale mantenuta per compatibilità"""
    return xpath_get_text_robust(element, [query], default, normalize_space)

def xpath_find_first(element, query):
    if element is None: 
        return None
    try: 
        result = element.xpath(query)
        return result[0] if result else None
    except Exception as e: 
        logger.debug(f"Err xpath '{query}': {e}")
        return None

def xpath_find_all(element, query):
    if element is None: 
        return []
    try: 
        return element.xpath(query)
    except Exception as e: 
        logger.debug(f"Err xpath '{query}': {e}")
        return []

def xpath_get_decimal_robust(element, queries, default_str='0.0', log_attempts=False):
    """
    Versione robusta per estrazione valori decimali con multiple query.
    Gestisce meglio i formati numerici diversi dei vari fornitori.
    """
    text = xpath_get_text_robust(element, queries, default="", log_attempts=log_attempts)
    cleaned_text = text.strip()
    
    if not cleaned_text:
        return to_decimal(default_str)
    
    # Controllo pattern data per evitare conversioni errate
    date_pattern = r'^(\d{4}[-/.]\d{1,2}[-/.]\d{1,2}|\d{1,2}[-/.]\d{1,2}[-/.]\d{2,4}|\d{8})$'
    if re.match(date_pattern, cleaned_text):
        if re.search(r'[-/]', cleaned_text) or ('.' in cleaned_text and not cleaned_text.replace('.','').isdigit()):
            logger.warning(f"Campo numerico contiene pattern data: '{text}'. Uso default '{default_str}'.")
            return to_decimal(default_str)
    
    # Pulizia speciale per importi con formati diversi
    cleaned_for_decimal = clean_amount_string(cleaned_text)
    result = to_decimal(cleaned_for_decimal, default=default_str)
    
    if log_attempts:
        logger.debug(f"Conversione decimale: '{text}' -> '{cleaned_for_decimal}' -> {result}")
    
    return result

def clean_amount_string(amount_str):
    """
    Pulisce stringhe di importo gestendo diversi formati europei/italiani.
    Es: "1.234,56" -> "1234.56", "1,234.56" -> "1234.56", "1 234,56" -> "1234.56"
    """
    if not amount_str:
        return "0.0"
    
    # Rimuovi spazi e caratteri non numerici eccetto . , + -
    cleaned = re.sub(r'[^\d.,-]', '', amount_str.strip())
    
    # Gestisci formato italiano (1.234,56) vs americano (1,234.56)
    if ',' in cleaned and '.' in cleaned:
        # Se abbiamo sia virgola che punto, determina quale è il separatore decimale
        comma_pos = cleaned.rfind(',')
        dot_pos = cleaned.rfind('.')
        
        if comma_pos > dot_pos:
            # Formato italiano: punto=migliaia, virgola=decimali
            cleaned = cleaned.replace('.', '').replace(',', '.')
        else:
            # Formato americano: virgola=migliaia, punto=decimali
            cleaned = cleaned.replace(',', '')
    elif ',' in cleaned:
        # Solo virgola - potrebbe essere decimale italiano o migliaia americane
        comma_parts = cleaned.split(',')
        if len(comma_parts) == 2 and len(comma_parts[1]) <= 2:
            # Probabilmente decimale italiano
            cleaned = cleaned.replace(',', '.')
        else:
            # Probabilmente migliaia, rimuovi virgole
            cleaned = cleaned.replace(',', '')
    
    return cleaned if cleaned else "0.0"

# === FUNZIONI DI ESTRAZIONE ANAGRAFICHE MIGLIORATE ===
def extract_anagraphics_robust(node, entity_type):
    """
    Estrazione anagrafica più robusta che gestisce variazioni nella struttura XML
    di diversi fornitori.
    """
    anag_data = {}
    if node is None: 
        return anag_data

    # Prova diversi percorsi per DatiAnagrafici
    dati_anag_queries = [
        ".//*[local-name()='DatiAnagrafici']",
        "./*[local-name()='DatiAnagrafici']",
        ".//*[contains(local-name(), 'DatiAnag')]"
    ]
    
    dati_anag_node = None
    for query in dati_anag_queries:
        dati_anag_node = xpath_find_first(node, query)
        if dati_anag_node is not None:
            break
    
    if dati_anag_node is not None:
        # Estrazione P.IVA con percorsi multipli
        piva_queries = [
            ".//*[local-name()='IdFiscaleIVA']/*[local-name()='IdCodice']/text()",
            "./*[local-name()='IdFiscaleIVA']/*[local-name()='IdCodice']/text()",
            ".//*[local-name()='IdCodice']/text()",
            "./*[local-name()='IdCodice']/text()"
        ]
        anag_data['piva'] = xpath_get_text_robust(dati_anag_node, piva_queries, log_attempts=True)
        
        # Paese P.IVA
        paese_queries = [
            ".//*[local-name()='IdFiscaleIVA']/*[local-name()='IdPaese']/text()",
            "./*[local-name()='IdFiscaleIVA']/*[local-name()='IdPaese']/text()",
            ".//*[local-name()='IdPaese']/text()",
            "./*[local-name()='IdPaese']/text()"
        ]
        anag_data['piva_paese'] = xpath_get_text_robust(dati_anag_node, paese_queries, default='IT')

        # Codice Fiscale
        cf_queries = [
            ".//*[local-name()='CodiceFiscale']/text()",
            "./*[local-name()='CodiceFiscale']/text()",
            ".//*[contains(local-name(), 'CodiceFisc')]/text()"
        ]
        anag_data['cf'] = xpath_get_text_robust(dati_anag_node, cf_queries, log_attempts=True)

        # Logica di fallback P.IVA/CF migliorata
        piva_val = anag_data.get('piva', '').strip()
        cf_val = anag_data.get('cf', '').strip()
        
        if not piva_val and cf_val: 
            anag_data['piva'] = cf_val
        elif piva_val and not cf_val and len(piva_val) == 16: 
            anag_data['cf'] = piva_val

        # Denominazione con percorsi multipli
        denominazione_queries = [
            ".//*[local-name()='Anagrafica']/*[local-name()='Denominazione']/text()",
            "./*[local-name()='Anagrafica']/*[local-name()='Denominazione']/text()",
            ".//*[local-name()='Denominazione']/text()",
            "./*[local-name()='Denominazione']/text()"
        ]
        anag_data['denomination'] = xpath_get_text_robust(dati_anag_node, denominazione_queries, log_attempts=True)
        
        # Se denominazione vuota, prova con Nome/Cognome
        if not anag_data.get('denomination'):
            nome_queries = [
                ".//*[local-name()='Anagrafica']/*[local-name()='Nome']/text()",
                "./*[local-name()='Anagrafica']/*[local-name()='Nome']/text()",
                ".//*[local-name()='Nome']/text()"
            ]
            cognome_queries = [
                ".//*[local-name()='Anagrafica']/*[local-name()='Cognome']/text()",
                "./*[local-name()='Anagrafica']/*[local-name()='Cognome']/text()",
                ".//*[local-name()='Cognome']/text()"
            ]
            
            nome = xpath_get_text_robust(dati_anag_node, nome_queries)
            cognome = xpath_get_text_robust(dati_anag_node, cognome_queries)
            
            if nome or cognome:
                anag_data['denomination'] = f"{cognome} {nome}".strip()

        if not anag_data.get('denomination'): 
            anag_data['denomination'] = "N/D"

        # Regime fiscale per cedente
        if entity_type == 'Cedente':
            regime_queries = [
                ".//*[local-name()='RegimeFiscale']/text()",
                "./*[local-name()='RegimeFiscale']/text()",
                ".//*[contains(local-name(), 'Regime')]/text()"
            ]
            anag_data['regime_fiscale'] = xpath_get_text_robust(dati_anag_node, regime_queries)

    # Estrazione indirizzo con percorsi multipli
    sede_queries = [
        ".//*[local-name()='Sede']",
        "./*[local-name()='Sede']",
        ".//*[contains(local-name(), 'Sede')]"
    ]
    
    sede_node = None
    for query in sede_queries:
        sede_node = xpath_find_first(node, query)
        if sede_node is not None:
            break
    
    if sede_node is not None:
        address_queries = [
            ".//*[local-name()='Indirizzo']/text()",
            "./*[local-name()='Indirizzo']/text()"
        ]
        anag_data['address'] = xpath_get_text_robust(sede_node, address_queries)
        
        cap_queries = [
            ".//*[local-name()='CAP']/text()",
            "./*[local-name()='CAP']/text()"
        ]
        anag_data['cap'] = xpath_get_text_robust(sede_node, cap_queries)
        
        city_queries = [
            ".//*[local-name()='Comune']/text()",
            "./*[local-name()='Comune']/text()"
        ]
        anag_data['city'] = xpath_get_text_robust(sede_node, city_queries)
        
        province_queries = [
            ".//*[local-name()='Provincia']/text()",
            "./*[local-name()='Provincia']/text()"
        ]
        anag_data['province'] = xpath_get_text_robust(sede_node, province_queries)
        
        country_queries = [
            ".//*[local-name()='Nazione']/text()",
            "./*[local-name()='Nazione']/text()"
        ]
        anag_data['country'] = xpath_get_text_robust(sede_node, country_queries, default='IT')

    # Contatti per cedente
    if entity_type == 'Cedente':
        contatti_queries = [
            ".//*[local-name()='Contatti']",
            "./*[local-name()='Contatti']"
        ]
        
        contatti_node = None
        for query in contatti_queries:
            contatti_node = xpath_find_first(node, query)
            if contatti_node is not None:
                break
        
        if contatti_node is not None:
            email_queries = [
                ".//*[local-name()='Email']/text()",
                "./*[local-name()='Email']/text()"
            ]
            anag_data['email'] = xpath_get_text_robust(contatti_node, email_queries)
            
            phone_queries = [
                ".//*[local-name()='Telefono']/text()",
                "./*[local-name()='Telefono']/text()"
            ]
            anag_data['phone'] = xpath_get_text_robust(contatti_node, phone_queries)
    
    return anag_data

# === ESTRAZIONE DATI GENERALI DOCUMENTO MIGLIORATA ===
def extract_general_document_data_robust(dati_generali_doc_node, base_filename):
    """
    Estrazione dati generali documento più robusta per gestire
    variazioni strutturali dei diversi fornitori.
    """
    if dati_generali_doc_node is None:
        raise ValueError("Blocco DatiGeneraliDocumento non trovato.")
    
    # Tipo documento con percorsi multipli
    doc_type_queries = [
        ".//*[local-name()='TipoDocumento']/text()",
        "./*[local-name()='TipoDocumento']/text()",
        ".//*[contains(local-name(), 'TipoDoc')]/text()"
    ]
    doc_type = xpath_get_text_robust(dati_generali_doc_node, doc_type_queries, log_attempts=True)
    
    # Divisa
    currency_queries = [
        ".//*[local-name()='Divisa']/text()",
        "./*[local-name()='Divisa']/text()"
    ]
    doc_currency = xpath_get_text_robust(dati_generali_doc_node, currency_queries, default='EUR')
    
    # Data documento
    date_queries = [
        ".//*[local-name()='Data']/text()",
        "./*[local-name()='Data']/text()",
        ".//*[contains(local-name(), 'DataDoc')]/text()"
    ]
    doc_date_str = xpath_get_text_robust(dati_generali_doc_node, date_queries, log_attempts=True)
    
    # Numero documento
    number_queries = [
        ".//*[local-name()='Numero']/text()",
        "./*[local-name()='Numero']/text()",
        ".//*[contains(local-name(), 'NumDoc')]/text()",
        ".//*[contains(local-name(), 'NumeroDoc')]/text()"
    ]
    doc_number = xpath_get_text_robust(dati_generali_doc_node, number_queries, log_attempts=True)
    
    # Importo totale documento con percorsi multipli
    total_amount_queries = [
        ".//*[local-name()='ImportoTotaleDocumento']/text()",
        "./*[local-name()='ImportoTotaleDocumento']/text()",
        ".//*[contains(local-name(), 'ImportoTotale')]/text()",
        ".//*[contains(local-name(), 'TotaleDoc')]/text()"
    ]
    doc_total_amount = xpath_get_decimal_robust(
        dati_generali_doc_node, 
        total_amount_queries, 
        default_str='0.0',
        log_attempts=True
    )
    
    # Validazione dati essenziali
    if not doc_date_str or not doc_number:
        raise ValueError(f"Data ('{doc_date_str}') o Numero ('{doc_number}') Documento mancanti in {base_filename}.")
    
    # Validazione formato data
    try:
        datetime.strptime(doc_date_str, '%Y-%m-%d')
    except ValueError:
        try:
            # Prova altri formati comuni
            for fmt in ['%d/%m/%Y', '%d-%m-%Y', '%Y/%m/%d']:
                datetime.strptime(doc_date_str, fmt)
                break
        except ValueError:
            logger.warning(f"Formato data documento non standard: '{doc_date_str}' in {base_filename}")
    
    return {
        'doc_type': doc_type,
        'currency': doc_currency,
        'doc_date': doc_date_str,
        'doc_number': doc_number,
        'total_amount': doc_total_amount
    }

# === ESTRAZIONE RIGHE DOCUMENTO MIGLIORATA ===
def extract_document_lines_robust(dati_beni_servizi_node, base_filename):
    """
    Estrazione righe documento più robusta per gestire
    variazioni strutturali nei DettaglioLinee.
    """
    lines_data = []
    
    if dati_beni_servizi_node is None:
        logger.warning(f"Blocco DatiBeniServizi non trovato in {base_filename}")
        return lines_data
    
    # Cerca DettaglioLinee con percorsi multipli
    line_queries = [
        ".//*[local-name()='DettaglioLinee']",
        "./*[local-name()='DettaglioLinee']",
        ".//*[contains(local-name(), 'DettaglioLinee')]",
        ".//*[contains(local-name(), 'Linea')]"
    ]
    
    line_nodes = []
    for query in line_queries:
        line_nodes = xpath_find_all(dati_beni_servizi_node, query)
        if line_nodes:
            break
    
    logger.debug(f"Trovate {len(line_nodes)} righe in {base_filename}")
    
    for idx, line_node in enumerate(line_nodes):
        try:
            # Numero linea
            line_num_queries = [
                ".//*[local-name()='NumeroLinea']/text()",
                "./*[local-name()='NumeroLinea']/text()",
                ".//*[contains(local-name(), 'NumLinea')]/text()"
            ]
            line_num_text = xpath_get_text_robust(line_node, line_num_queries, default=str(idx + 1))
            
            try:
                line_num = int(line_num_text)
            except ValueError:
                line_num = idx + 1

            # Codice articolo
            item_code_queries = [
                ".//*[local-name()='CodiceArticolo']/*[local-name()='CodiceValore']/text()",
                "./*[local-name()='CodiceArticolo']/*[local-name()='CodiceValore']/text()",
                ".//*[local-name()='CodiceValore']/text()"
            ]
            item_code = xpath_get_text_robust(line_node, item_code_queries)
            
            # Tipo codice articolo
            item_type_queries = [
                ".//*[local-name()='CodiceArticolo']/*[local-name()='CodiceTipo']/text()",
                "./*[local-name()='CodiceArticolo']/*[local-name()='CodiceTipo']/text()",
                ".//*[local-name()='CodiceTipo']/text()"
            ]
            item_type = xpath_get_text_robust(line_node, item_type_queries)
            
            # Descrizione
            description_queries = [
                ".//*[local-name()='Descrizione']/text()",
                "./*[local-name()='Descrizione']/text()",
                ".//*[contains(local-name(), 'Desc')]/text()"
            ]
            description = xpath_get_text_robust(line_node, description_queries, log_attempts=True)
            
            # Unità di misura
            unit_measure_queries = [
                ".//*[local-name()='UnitaMisura']/text()",
                "./*[local-name()='UnitaMisura']/text()",
                ".//*[contains(local-name(), 'Unita')]/text()"
            ]
            unit_measure = xpath_get_text_robust(line_node, unit_measure_queries)
            
            # Quantità
            quantity_queries = [
                ".//*[local-name()='Quantita']/text()",
                "./*[local-name()='Quantita']/text()",
                ".//*[contains(local-name(), 'Qta')]/text()",
                ".//*[contains(local-name(), 'Qty')]/text()"
            ]
            quantity = xpath_get_decimal_robust(line_node, quantity_queries, default_str='NaN', log_attempts=True)
            
            # Prezzo unitario
            unit_price_queries = [
                ".//*[local-name()='PrezzoUnitario']/text()",
                "./*[local-name()='PrezzoUnitario']/text()",
                ".//*[contains(local-name(), 'PrezzoUnit')]/text()",
                ".//*[contains(local-name(), 'Prezzo')]/text()"
            ]
            unit_price = xpath_get_decimal_robust(line_node, unit_price_queries, default_str='0.0', log_attempts=True)
            
            # Prezzo totale riga
            total_price_queries = [
                ".//*[local-name()='PrezzoTotale']/text()",
                "./*[local-name()='PrezzoTotale']/text()",
                ".//*[contains(local-name(), 'PrezzoTot')]/text()",
                ".//*[contains(local-name(), 'ImportoLinea')]/text()",
                ".//*[contains(local-name(), 'TotaleLinea')]/text()"
            ]
            total_price = xpath_get_decimal_robust(line_node, total_price_queries, default_str='0.0', log_attempts=True)
            
            # Aliquota IVA
            vat_rate_queries = [
                ".//*[local-name()='AliquotaIVA']/text()",
                "./*[local-name()='AliquotaIVA']/text()",
                ".//*[contains(local-name(), 'Aliquota')]/text()",
                ".//*[contains(local-name(), 'IVA')]/text()"
            ]
            vat_rate = xpath_get_decimal_robust(line_node, vat_rate_queries, default_str='0.0', log_attempts=True)
            
            # Validazione quantità
            if not quantity.is_finite():
                quantity = None
            
            # Calcolo automatico prezzo totale se mancante ma abbiamo quantità e prezzo unitario
            if total_price == Decimal('0.0') and quantity and unit_price and quantity.is_finite():
                calculated_total = quantize(quantity * unit_price)
                logger.info(f"Calcolato prezzo totale riga {line_num}: {quantity} x {unit_price} = {calculated_total}")
                total_price = calculated_total
            
            line_data = {
                'line_number': line_num,
                'item_code': item_code,
                'item_type': item_type,
                'description': description,
                'unit_measure': unit_measure,
                'quantity': quantity,
                'unit_price': unit_price,
                'total_price': total_price,
                'vat_rate': vat_rate
            }
            
            lines_data.append(line_data)
            
        except Exception as line_err:
            logger.error(f"Errore estrazione riga {idx+1} in {base_filename}: {line_err}")
            continue
    
    return lines_data

# === ESTRAZIONE RIEPILOGO IVA MIGLIORATA ===
def extract_vat_summary_robust(dati_beni_servizi_node, base_filename):
    """
    Estrazione riepilogo IVA più robusta.
    """
    vat_summary_data = []
    
    if dati_beni_servizi_node is None:
        return vat_summary_data
    
    # Cerca DatiRiepilogo con percorsi multipli
    riepilogo_queries = [
        ".//*[local-name()='DatiRiepilogo']",
        "./*[local-name()='DatiRiepilogo']",
        ".//*[contains(local-name(), 'Riepilogo')]"
    ]
    
    riepilogo_nodes = []
    for query in riepilogo_queries:
        riepilogo_nodes = xpath_find_all(dati_beni_servizi_node, query)
        if riepilogo_nodes:
            break
    
    logger.debug(f"Trovati {len(riepilogo_nodes)} blocchi riepilogo IVA in {base_filename}")
    
    for riep_node in riepilogo_nodes:
        try:
            # Aliquota IVA
            vat_rate_queries = [
                ".//*[local-name()='AliquotaIVA']/text()",
                "./*[local-name()='AliquotaIVA']/text()",
                ".//*[contains(local-name(), 'Aliquota')]/text()"
            ]
            vat_rate = xpath_get_decimal_robust(riep_node, vat_rate_queries, '0.0', log_attempts=True)
            
            # Imponibile
            taxable_queries = [
                ".//*[local-name()='ImponibileImporto']/text()",
                "./*[local-name()='ImponibileImporto']/text()",
                ".//*[contains(local-name(), 'Imponibile')]/text()",
                ".//*[contains(local-name(), 'TaxableAmount')]/text()"
            ]
            taxable_amount = xpath_get_decimal_robust(riep_node, taxable_queries, '0.0', log_attempts=True)
            
            # Imposta
            vat_amount_queries = [
                ".//*[local-name()='Imposta']/text()",
                "./*[local-name()='Imposta']/text()",
                ".//*[contains(local-name(), 'ImportoIVA')]/text()",
                ".//*[contains(local-name(), 'TaxAmount')]/text()"
            ]
            vat_amount = xpath_get_decimal_robust(riep_node, vat_amount_queries, '0.0', log_attempts=True)
            
            # Esigibilità IVA
            esigibilita_queries = [
                ".//*[local-name()='EsigibilitaIVA']/text()",
                "./*[local-name()='EsigibilitaIVA']/text()",
                ".//*[contains(local-name(), 'Esigibilita')]/text()"
            ]
            esigibilita = xpath_get_text_robust(riep_node, esigibilita_queries)
            
            vat_summary = {
                'vat_rate': vat_rate,
                'taxable_amount': taxable_amount,
                'vat_amount': vat_amount,
                'esigibilita': esigibilita
            }
            
            vat_summary_data.append(vat_summary)
            
        except Exception as vat_err:
            logger.error(f"Errore estrazione riepilogo IVA in {base_filename}: {vat_err}")
            continue
    
    return vat_summary_data

# === FUNZIONE PRINCIPALE PARSE_FATTURA_XML MIGLIORATA ===
def parse_fattura_xml(xml_filepath, my_company_data=None):
    """
    Parser XML migliorato per gestire meglio le variazioni strutturali
    delle fatture passive di diversi fornitori.
    """
    base_filename = os.path.basename(xml_filepath)
    logger.info(f"Parsing XML migliorato: {base_filename}")
    
    if my_company_data is None:
        logger.error(f"Dati azienda mancanti per parse_fattura_xml ({base_filename}).")
        return {'error': "Dati azienda mancanti", 'source_file': xml_filepath}

    try:
        # Parser XML più permissivo per gestire XML malformati
        parser = etree.XMLParser(
            recover=True,           # Recupera da errori XML
            remove_blank_text=True,
            remove_comments=True,
            remove_pis=True,
            no_network=True,
            huge_tree=True,
            encoding='utf-8'        # Forza encoding
        )
        
        tree = etree.parse(xml_filepath, parser)
        root = tree.getroot()
        
        if root is None:
            raise ValueError("Root element non trovato.")

        # Trova i nodi principali con ricerca più robusta
        header_queries = [
            "//*[local-name()='FatturaElettronicaHeader']",
            "//*[contains(local-name(), 'Header')]",
            "//*[contains(local-name(), 'Intestazione')]"
        ]
        
        body_queries = [
            "//*[local-name()='FatturaElettronicaBody']",
            "//*[contains(local-name(), 'Body')]",
            "//*[contains(local-name(), 'Corpo')]"
        ]
        
        header_node = None
        for query in header_queries:
            header_node = xpath_find_first(root, query)
            if header_node is not None:
                break
        
        body_node = None
        for query in body_queries:
            body_node = xpath_find_first(root, query)
            if body_node is not None:
                break
        
        if header_node is None:
            raise ValueError("Blocco FatturaElettronicaHeader non trovato.")
        if body_node is None:
            raise ValueError("Blocco FatturaElettronicaBody non trovato.")

        # Struttura dati di ritorno
        data = {
            'header': {}, 
            'body': {
                'general_data': {}, 
                'lines': [], 
                'vat_summary': [], 
                'payment_data': []
            },
            'source_file': xml_filepath, 
            'anagraphics': {'cedente': {}, 'cessionario': {}},
            'type': 'Unknown', 
            'unique_hash': None, 
            'error': None
        }

        # === ESTRAZIONE ANAGRAFICHE MIGLIORATE ===
        
        # Cedente con percorsi multipli
        cedente_queries = [
            ".//*[local-name()='CedentePrestatore']",
            "./*[local-name()='CedentePrestatore']",
            ".//*[contains(local-name(), 'Cedente')]",
            ".//*[contains(local-name(), 'Prestatore')]"
        ]
        
        cedente_node = None
        for query in cedente_queries:
            cedente_node = xpath_find_first(header_node, query)
            if cedente_node is not None:
                break
        
        # Cessionario con percorsi multipli
        cessionario_queries = [
            ".//*[local-name()='CessionarioCommittente']",
            "./*[local-name()='CessionarioCommittente']",
            ".//*[contains(local-name(), 'Cessionario')]",
            ".//*[contains(local-name(), 'Committente')]"
        ]
        
        cessionario_node = None
        for query in cessionario_queries:
            cessionario_node = xpath_find_first(header_node, query)
            if cessionario_node is not None:
                break

        if cedente_node is None:
            raise ValueError("Blocco CedentePrestatore non trovato.")
        if cessionario_node is None:
            raise ValueError("Blocco CessionarioCommittente non trovato.")

        # Estrai anagrafiche con funzione robusta
        data['anagraphics']['cedente'] = extract_anagraphics_robust(cedente_node, 'Cedente')
        data['anagraphics']['cessionario'] = extract_anagraphics_robust(cessionario_node, 'Cessionario')

        # Dati trasmissione
        dati_trasmissione_queries = [
            ".//*[local-name()='DatiTrasmissione']",
            "./*[local-name()='DatiTrasmissione']",
            ".//*[contains(local-name(), 'Trasmissione')]"
        ]
        
        dati_trasmissione = None
        for query in dati_trasmissione_queries:
            dati_trasmissione = xpath_find_first(header_node, query)
            if dati_trasmissione is not None:
                break
        
        if dati_trasmissione:
            # Codice destinatario
            cod_dest_queries = [
                ".//*[local-name()='CodiceDestinatario']/text()",
                "./*[local-name()='CodiceDestinatario']/text()",
                ".//*[contains(local-name(), 'CodDest')]/text()"
            ]
            data['anagraphics']['cessionario']['codice_destinatario'] = xpath_get_text_robust(
                dati_trasmissione, cod_dest_queries
            )
            
            # PEC destinatario
            pec_dest_queries = [
                ".//*[local-name()='PECDestinatario']/text()",
                "./*[local-name()='PECDestinatario']/text()",
                ".//*[contains(local-name(), 'PEC')]/text()"
            ]
            data['anagraphics']['cessionario']['pec'] = xpath_get_text_robust(
                dati_trasmissione, pec_dest_queries
            )

        # === DETERMINAZIONE TIPO FATTURA ===
        ced_anag = data['anagraphics']['cedente']
        ces_anag = data['anagraphics']['cessionario']
        
        is_cedente_us = _is_own_company(ced_anag, my_company_data)
        is_cessionario_us = _is_own_company(ces_anag, my_company_data)

        if is_cedente_us and is_cessionario_us:
            data['type'] = 'Autofattura'
        elif is_cedente_us:
            data['type'] = 'Attiva'
        elif is_cessionario_us:
            data['type'] = 'Passiva'
        else:
            logger.error(f"Tipo fattura non determinato per {base_filename}. Dati azienda config non corrispondono.")
            data['type'] = 'Unknown'

        logger.info(f"Tipo fattura determinato: {data['type']}")

        # === ESTRAZIONE DATI GENERALI DOCUMENTO ===
        dati_generali_queries = [
            ".//*[local-name()='DatiGenerali']",
            "./*[local-name()='DatiGenerali']",
            ".//*[contains(local-name(), 'DatiGen')]"
        ]
        
        dati_generali_node = None
        for query in dati_generali_queries:
            dati_generali_node = xpath_find_first(body_node, query)
            if dati_generali_node is not None:
                break
        
        if dati_generali_node is None:
            raise ValueError("Blocco DatiGenerali non trovato.")
        
        dati_generali_doc_queries = [
            ".//*[local-name()='DatiGeneraliDocumento']",
            "./*[local-name()='DatiGeneraliDocumento']",
            ".//*[contains(local-name(), 'DatiGeneraliDoc')]"
        ]
        
        dati_generali_doc_node = None
        for query in dati_generali_doc_queries:
            dati_generali_doc_node = xpath_find_first(dati_generali_node, query)
            if dati_generali_doc_node is not None:
                break
        
        # Estrai dati generali con funzione robusta
        general_data = extract_general_document_data_robust(dati_generali_doc_node, base_filename)
        data['body']['general_data'] = general_data

        # === CALCOLO HASH UNIVOCO ===
        hash_cedente_id = ced_anag.get('piva') or ced_anag.get('cf') or ""
        hash_cessionario_id = ces_anag.get('piva') or ces_anag.get('cf') or ""
        
        if not hash_cedente_id or not hash_cessionario_id:
            logger.warning(f"P.IVA/CF mancante per hash fattura '{general_data['doc_number']}'.")
        
        data['unique_hash'] = calculate_invoice_hash(
            hash_cedente_id, 
            hash_cessionario_id, 
            general_data['doc_type'], 
            general_data['doc_number'], 
            general_data['doc_date']
        )
        logger.debug(f"Hash calcolato: {data['unique_hash']}")

        # === ESTRAZIONE DATI BENI SERVIZI (RIGHE E IVA) ===
        dati_beni_servizi_queries = [
            ".//*[local-name()='DatiBeniServizi']",
            "./*[local-name()='DatiBeniServizi']",
            ".//*[contains(local-name(), 'BeniServizi')]"
        ]
        
        dati_beni_servizi_node = None
        for query in dati_beni_servizi_queries:
            dati_beni_servizi_node = xpath_find_first(body_node, query)
            if dati_beni_servizi_node is not None:
                break
        
        if dati_beni_servizi_node:
            # Estrai righe con funzione robusta
            data['body']['lines'] = extract_document_lines_robust(dati_beni_servizi_node, base_filename)
            
            # Estrai riepilogo IVA con funzione robusta
            data['body']['vat_summary'] = extract_vat_summary_robust(dati_beni_servizi_node, base_filename)
            
            # Validazione e ricalcolo totale da riepilogo IVA
            calculated_total_from_vat = Decimal('0.0')
            for summary in data['body']['vat_summary']:
                calculated_total_from_vat += summary['taxable_amount'] + summary['vat_amount']
            
            calculated_total_from_vat = quantize(calculated_total_from_vat)
            original_total = data['body']['general_data']['total_amount']
            
            if original_total.is_zero() and calculated_total_from_vat > Decimal('0.0'):
                logger.warning(f"Totale documento originale 0.0 per '{general_data['doc_number']}'. "
                             f"Aggiornato con totale IVA: {calculated_total_from_vat:.2f}")
                data['body']['general_data']['total_amount'] = calculated_total_from_vat
            elif abs(original_total - calculated_total_from_vat) > Decimal('0.01'):
                logger.warning(f"Discrepanza Totale Doc ({original_total:.2f}) vs "
                             f"Riepilogo IVA ({calculated_total_from_vat:.2f}) per '{general_data['doc_number']}'.")
                
                # Se la discrepanza è significativa, usa il totale calcolato dall'IVA
                if abs(original_total - calculated_total_from_vat) > Decimal('1.0'):
                    logger.info(f"Discrepanza > 1€, uso totale calcolato da IVA: {calculated_total_from_vat:.2f}")
                    data['body']['general_data']['total_amount'] = calculated_total_from_vat

        # === ESTRAZIONE DATI PAGAMENTO ===
        payment_blocks_queries = [
            ".//*[local-name()='DatiPagamento']",
            "./*[local-name()='DatiPagamento']",
            ".//*[contains(local-name(), 'Pagamento')]"
        ]
        
        payment_blocks = []
        for query in payment_blocks_queries:
            payment_blocks = xpath_find_all(body_node, query)
            if payment_blocks:
                break
        
        logger.debug(f"Trovati {len(payment_blocks)} blocchi DatiPagamento.")
        
        payment_total_check = Decimal('0.0')
        last_due_date = None
        main_payment_method = None
        conditions = None
        
        if payment_blocks:
            # Condizioni pagamento dal primo blocco
            conditions_queries = [
                ".//*[local-name()='CondizioniPagamento']/text()",
                "./*[local-name()='CondizioniPagamento']/text()",
                ".//*[contains(local-name(), 'Condizioni')]/text()"
            ]
            conditions = xpath_get_text_robust(payment_blocks[0], conditions_queries)
            
            for pay_node in payment_blocks:
                block_conditions = xpath_get_text_robust(pay_node, conditions_queries) or conditions
                
                # Dettagli pagamento
                details_queries = [
                    ".//*[local-name()='DettaglioPagamento']",
                    "./*[local-name()='DettaglioPagamento']",
                    ".//*[contains(local-name(), 'Dettaglio')]"
                ]
                
                details_nodes = []
                for query in details_queries:
                    details_nodes = xpath_find_all(pay_node, query)
                    if details_nodes:
                        break
                
                for det_node in details_nodes:
                    # Modalità pagamento
                    payment_method_queries = [
                        ".//*[local-name()='ModalitaPagamento']/text()",
                        "./*[local-name()='ModalitaPagamento']/text()",
                        ".//*[contains(local-name(), 'Modalita')]/text()"
                    ]
                    payment_method = xpath_get_text_robust(det_node, payment_method_queries)
                    
                    # Data scadenza
                    due_date_queries = [
                        ".//*[local-name()='DataScadenzaPagamento']/text()",
                        "./*[local-name()='DataScadenzaPagamento']/text()",
                        ".//*[contains(local-name(), 'DataScadenza')]/text()",
                        ".//*[contains(local-name(), 'Scadenza')]/text()"
                    ]
                    due_date = xpath_get_text_robust(det_node, due_date_queries)
                    
                    # Importo pagamento
                    amount_queries = [
                        ".//*[local-name()='ImportoPagamento']/text()",
                        "./*[local-name()='ImportoPagamento']/text()",
                        ".//*[contains(local-name(), 'ImportoPag')]/text()"
                    ]
                    amount = xpath_get_decimal_robust(det_node, amount_queries, '0.0')
                    
                    pay_detail = {
                        'conditions': block_conditions,
                        'payment_method': payment_method,
                        'due_date': due_date,
                        'amount': amount
                    }
                    
                    data['body']['payment_data'].append(pay_detail)
                    payment_total_check += pay_detail['amount']
                    
                    # Prendi l'ultima data di scadenza valida trovata
                    if pay_detail.get('due_date'):
                        last_due_date = pay_detail['due_date']
                    
                    # Prendi il primo metodo di pagamento valido trovato
                    if not main_payment_method and pay_detail.get('payment_method'):
                        main_payment_method = pay_detail['payment_method']

            # Validazione totale pagamenti
            payment_total_check = quantize(payment_total_check)
            final_total_doc = data['body']['general_data']['total_amount']
            
            if payment_total_check > Decimal('0.0') and abs(final_total_doc - payment_total_check) > Decimal('0.01'):
                logger.warning(f"Discrepanza Totale Doc ({final_total_doc:.2f}) vs "
                             f"Somma Pagamenti ({payment_total_check:.2f}) per '{general_data['doc_number']}'.")

            # Imposta data scadenza a data doc se pagamento immediato e nessuna scadenza trovata
            if last_due_date is None and conditions in ['TP01', 'TP02']:  # TP01=Contanti, TP02=Completo
                last_due_date = general_data['doc_date']
                logger.info(f"Pagamento immediato ('{conditions}') per '{general_data['doc_number']}', "
                           f"scadenza impostata a data doc: {general_data['doc_date']}")

            data['body']['general_data']['due_date'] = last_due_date
            data['body']['general_data']['payment_method'] = main_payment_method

        logger.info(f"Parsing XML completato: {base_filename} (Tipo: {data['type']})")
        return data

    except etree.XMLSyntaxError as e:
        logger.error(f"Errore sintassi XML {base_filename}: {e}", exc_info=True)
        return {'error': f"Errore Sintassi XML: {e}", 'source_file': xml_filepath}
    except ValueError as ve:
        logger.error(f"Errore dati/struttura XML {base_filename}: {ve}", exc_info=True)
        return {'error': f"Errore Dati/Struttura XML: {ve}", 'source_file': xml_filepath}
    except TypeError as te:
        logger.error(f"Errore tipo dati (TypeError) durante parsing XML {base_filename}: {te}", exc_info=True)
        return {'error': f"Errore Tipo Dati: {te}", 'source_file': xml_filepath}
    except AttributeError as ae:
        logger.error(f"Errore attributo (AttributeError) durante parsing XML {base_filename}: {ae}", exc_info=True)
        return {'error': f"Errore Attributo (NoneType?): {ae}", 'source_file': xml_filepath}
    except Exception as e:
        logger.error(f"Errore generico parsing XML {base_filename}: {e}", exc_info=True)
        return {'error': f"Errore Generico Parsing: {e}", 'source_file': xml_filepath}

# === FUNZIONI DI UTILITÀ PER DEBUGGING ===
def debug_xml_structure(xml_filepath, max_depth=3):
    """
    Funzione di debug per analizzare la struttura di un XML problematico.
    Utile per capire come adattare i parser per fornitori specifici.
    """
    try:
        parser = etree.XMLParser(recover=True)
        tree = etree.parse(xml_filepath, parser)
        root = tree.getroot()
        
        def print_element(elem, depth=0, max_depth=max_depth):
            if depth > max_depth:
                return
            
            indent = "  " * depth
            tag_name = etree.QName(elem).localname if elem.tag else "Unknown"
            
            # Mostra attributi se presenti
            attrs = elem.attrib
            attr_str = ""
            if attrs:
                attr_str = " " + " ".join([f'{k}="{v}"' for k, v in attrs.items()])
            
            # Mostra testo se presente e non ha figli
            text_content = ""
            if elem.text and elem.text.strip() and len(list(elem)) == 0:
                text_content = f" = '{elem.text.strip()[:50]}...'" if len(elem.text.strip()) > 50 else f" = '{elem.text.strip()}'"
            
            print(f"{indent}<{tag_name}{attr_str}>{text_content}")
            
            # Stampa solo alcuni figli per evitare output troppo lungo
            children = list(elem)
            for i, child in enumerate(children[:10]):  # Limita a 10 figli per livello
                print_element(child, depth + 1, max_depth)
            
            if len(children) > 10:
                print(f"{indent}  ... e altri {len(children) - 10} elementi")
        
        print(f"\n=== STRUTTURA XML: {os.path.basename(xml_filepath)} ===")
        print_element(root)
        print("=" * 50)
        
    except Exception as e:
        logger.error(f"Errore analisi struttura XML {xml_filepath}: {e}")

def validate_xml_against_schema(xml_filepath, schema_path=None):
    """
    Valida un XML contro lo schema XSD delle fatture elettroniche italiane.
    Utile per verificare se il problema è nell'XML o nel parser.
    """
    try:
        # Se non è fornito uno schema, usa una validazione base
        if schema_path and os.path.exists(schema_path):
            with open(schema_path, 'r') as schema_file:
                schema_doc = etree.parse(schema_file)
                schema = etree.XMLSchema(schema_doc)
            
            xml_doc = etree.parse(xml_filepath)
            is_valid = schema.validate(xml_doc)
            
            if not is_valid:
                logger.error(f"XML non valido secondo schema: {xml_filepath}")
                for error in schema.error_log:
                    logger.error(f"Errore schema: {error}")
            else:
                logger.info(f"XML valido secondo schema: {xml_filepath}")
                
            return is_valid
        else:
            # Validazione base solo parsing
            parser = etree.XMLParser()
            etree.parse(xml_filepath, parser)
            logger.info(f"XML ben formato: {xml_filepath}")
            return True
            
    except etree.XMLSyntaxError as e:
        logger.error(f"XML malformato {xml_filepath}: {e}")
        return False
    except Exception as e:
        logger.error(f"Errore validazione XML {xml_filepath}: {e}")
        return False
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/core/parser_xml.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/core/reconciliation.py ---
# core/reconciliation.py - Versione completa ottimizzata per uso professionale

import logging
import sqlite3
from itertools import combinations
import time
import pandas as pd
from decimal import Decimal, ROUND_HALF_UP
from datetime import datetime, date, timedelta
import re
from dateutil.relativedelta import relativedelta
from typing import List, Dict, Set, Tuple, Optional, Any
import concurrent.futures
from dataclasses import dataclass
from collections import defaultdict
import numpy as np

try:
    from .database import (get_connection, get_item_details,
                          update_invoice_reconciliation_state,
                          update_transaction_reconciliation_state,
                          add_or_update_reconciliation_link,
                          remove_reconciliation_links)
    from .utils import to_decimal, quantize, extract_invoice_number, AMOUNT_TOLERANCE
    from .smart_client_reconciliation import (suggest_client_based_reconciliation,
                                            enhance_cumulative_matches_with_client_patterns)
except ImportError:
    logging.warning("Import relativo '.database'/'.utils' fallito in reconciliation.py, tento import assoluto.")
    try:
        from database import (get_connection, get_item_details,
                              update_invoice_reconciliation_state,
                              update_transaction_reconciliation_state,
                              add_or_update_reconciliation_link,
                              remove_reconciliation_links)
        from utils import to_decimal, quantize, extract_invoice_number, AMOUNT_TOLERANCE
        try:
            from smart_client_reconciliation import (suggest_client_based_reconciliation,
                                                   enhance_cumulative_matches_with_client_patterns)
        except ImportError:
            logging.warning("Smart client reconciliation non disponibile")
            suggest_client_based_reconciliation = None
            enhance_cumulative_matches_with_client_patterns = None
    except ImportError as e:
        logging.critical(f"Impossibile importare dipendenze (database/utils) in reconciliation.py: {e}")
        raise ImportError(f"Impossibile importare dipendenze (database/utils) in reconciliation.py: {e}") from e

logger = logging.getLogger(__name__)

# Cache per anagrafiche per evitare query ripetute
_anagraphics_cache = {}
_cache_timestamp = None
CACHE_EXPIRY_MINUTES = 15

@dataclass
class InvoiceSequence:
    """Rappresenta una sequenza di fatture consecutive"""
    invoice_ids: List[int]
    start_number: int
    end_number: int
    total_amount: Decimal
    date_range_days: int
    avg_amount: Decimal
    sequence_score: float
    
    def __post_init__(self):
        if len(self.invoice_ids) > 1:
            self.avg_amount = self.total_amount / len(self.invoice_ids)
        else:
            self.avg_amount = self.total_amount

@dataclass
class ReconciliationCandidate:
    """Candidato per riconciliazione N:M migliorato"""
    invoice_ids: List[int]
    total_amount: Decimal
    confidence_score: float
    sequence_score: float
    date_coherence_score: float
    amount_coherence_score: float
    doc_numbers: List[str]
    date_range_days: int
    explanation: str

def _refresh_anagraphics_cache():
    """Aggiorna la cache delle anagrafiche"""
    global _anagraphics_cache, _cache_timestamp
    
    current_time = datetime.now()
    if (_cache_timestamp is None or 
        (current_time - _cache_timestamp).total_seconds() > CACHE_EXPIRY_MINUTES * 60):
        
        logger.debug("Aggiornamento cache anagrafiche...")
        conn = None
        try:
            conn = get_connection()
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT id, denomination, piva, cf 
                FROM Anagraphics 
                WHERE LENGTH(TRIM(COALESCE(denomination, ''))) >= 3
            """)
            
            new_cache = {}
            for row in cursor.fetchall():
                anag_id = row['id']
                denomination = (row['denomination'] or '').strip()
                piva = (row['piva'] or '').strip().upper()
                cf = (row['cf'] or '').strip().upper()
                
                # Preprocessa i termini per il matching
                denom_words = set(re.findall(r'\b\w{3,}\b', denomination.lower()))
                # Rimuovi stop words comuni
                stop_words = {'spa', 'srl', 'snc', 'sas', 'coop', 'societa', 'group', 'holding', 'soc'}
                denom_words = denom_words - stop_words
                
                new_cache[anag_id] = {
                    'denomination': denomination,
                    'piva': piva,
                    'cf': cf,
                    'search_words': denom_words,
                    'full_text': denomination.lower()
                }
            
            _anagraphics_cache = new_cache
            _cache_timestamp = current_time
            logger.debug(f"Cache anagrafiche aggiornata: {len(_anagraphics_cache)} record")
            
        except Exception as e:
            logger.error(f"Errore aggiornamento cache anagrafiche: {e}")
        finally:
            if conn:
                conn.close()

def find_anagraphics_id_from_description(description: str) -> Optional[int]:
    """Versione ottimizzata della ricerca anagrafica con cache"""
    if not description:
        return None
    
    _refresh_anagraphics_cache()
    
    log_prefix = "find_anag_id (cached)"
    logger.debug(f"{log_prefix}: Descrizione='{description[:100]}...'")
    
    # Estrai codici PIVA/CF dalla descrizione
    piva_cf_pattern = r'\b(\d{11})\b|\b([A-Z]{6}\d{2}[A-Z]\d{2}[A-Z]\d{3}[A-Z])\b'
    potential_codes = re.findall(piva_cf_pattern, description, re.IGNORECASE)
    
    # Cerca match esatti per PIVA/CF
    if potential_codes:
        codes_found = set(code.upper() for piva, cf in potential_codes if (code := piva or cf))
        for anag_id, anag_data in _anagraphics_cache.items():
            if anag_data['piva'] in codes_found or anag_data['cf'] in codes_found:
                logger.info(f"{log_prefix}: Match PIVA/CF -> ID:{anag_id}")
                return anag_id
    
    # Fallback al matching per nome
    desc_lower = description.lower()
    desc_words = set(re.findall(r'\b\w{3,}\b', desc_lower))
    
    best_score = 0.0
    best_match_id = None
    
    for anag_id, anag_data in _anagraphics_cache.items():
        # Match nome completo
        if anag_data['full_text'] in desc_lower:
            score = 0.8 + (len(anag_data['full_text']) / len(desc_lower)) * 0.2
            if score > best_score:
                best_score = score
                best_match_id = anag_id
                continue
        
        # Match per parole
        common_words = desc_words.intersection(anag_data['search_words'])
        if common_words:
            jaccard = len(common_words) / len(desc_words.union(anag_data['search_words']))
            word_coverage = len(common_words) / len(anag_data['search_words']) if anag_data['search_words'] else 0
            score = (jaccard * 0.4) + (word_coverage * 0.6)
            
            if score > best_score:
                best_score = score
                best_match_id = anag_id
    
    if best_match_id and best_score >= 0.3:
        logger.info(f"{log_prefix}: Match nome -> ID:{best_match_id}, Score: {best_score:.3f}")
        return best_match_id
    
    return None

def suggest_cumulative_matches(transaction_id, anagraphics_id_filter=None,
                               max_combination_size=5, max_search_time_ms=30000,
                               exclude_invoice_ids=None, start_date=None, end_date=None):
    """
    Versione ottimizzata N:M con supporto per algoritmi intelligenti basati su cliente
    """
    if exclude_invoice_ids is None:
        exclude_invoice_ids = set()
    
    # Se non c'è filtro anagrafica, non possiamo fare riconciliazione intelligente
    if anagraphics_id_filter is None:
        logger.info(f"Sugg.N:M T:{transaction_id}: Saltato (nessun filtro anagrafica)")
        return []
    
    conn = None
    start_time = time.monotonic()
    log_prefix = f"Sugg.N:M Enhanced T:{transaction_id}"
    
    try:
        conn = get_connection()
        cursor = conn.cursor()
        
        # Prima prova con l'algoritmo intelligente basato su cliente (se disponibile)
        if suggest_client_based_reconciliation is not None:
            logger.info(f"{log_prefix}: Tentativo riconciliazione intelligente basata su pattern cliente")
            client_suggestions = suggest_client_based_reconciliation(transaction_id, anagraphics_id_filter)
            
            if client_suggestions:
                logger.info(f"{log_prefix}: Trovati {len(client_suggestions)} suggerimenti client-based")
                # Se troviamo buoni match client-based, limitiamo la ricerca standard
                max_combination_size = min(max_combination_size, 3)
                max_search_time_ms = min(max_search_time_ms, 10000)
        else:
            client_suggestions = []
        
        # Recupera dettagli transazione
        cursor.execute("""
            SELECT amount, reconciled_amount, description
            FROM BankTransactions 
            WHERE id = ?
        """, (transaction_id,))
        
        trans_row = cursor.fetchone()
        if not trans_row:
            return client_suggestions
        
        target_amount_signed = quantize(
            to_decimal(trans_row['amount']) - to_decimal(trans_row['reconciled_amount'])
        )
        
        if target_amount_signed.copy_abs() <= AMOUNT_TOLERANCE / 2:
            return client_suggestions
        
        target_amount_abs = target_amount_signed.copy_abs()
        target_invoice_type = 'Attiva' if target_amount_signed > 0 else 'Passiva'
        
        # Query ottimizzata per candidati
        query_params = [
            target_invoice_type,
            anagraphics_id_filter,
            float(AMOUNT_TOLERANCE / 2),
            float(target_amount_abs * 1.2)  # Margine del 20%
        ]
        
        date_filter = ""
        if start_date and end_date:
            date_filter = "AND i.doc_date BETWEEN ? AND ?"
            query_params.extend([start_date, end_date])
        
        query = f"""
            SELECT 
                i.id, i.doc_number, i.doc_date, i.total_amount, i.paid_amount,
                CAST(i.total_amount - i.paid_amount AS REAL) as open_amount
            FROM Invoices i
            WHERE i.type = ?
              AND i.anagraphics_id = ?
              AND i.payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.')
              AND (i.total_amount - i.paid_amount) > ?
              AND (i.total_amount - i.paid_amount) <= ?
              {date_filter}
            ORDER BY i.doc_date DESC, i.doc_number
            LIMIT 50
        """
        
        cursor.execute(query, query_params)
        rows = cursor.fetchall()
        
        if not rows:
            return client_suggestions
        
        # Prepara dati per analisi
        candidate_invoices = []
        for row in rows:
            if row['id'] in exclude_invoice_ids:
                continue
            
            open_amount = quantize(to_decimal(row['open_amount']))
            if open_amount <= AMOUNT_TOLERANCE / 2:
                continue
            
            candidate_invoices.append({
                'id': row['id'],
                'doc_number': row['doc_number'],
                'doc_date': row['doc_date'],
                'amount': float(open_amount),
                'amount_decimal': open_amount
            })
        
        if len(candidate_invoices) < 2:
            return client_suggestions
        
        logger.info(f"{log_prefix}: Analisi {len(candidate_invoices)} fatture candidate (standard)")
        
        # Trova combinazioni standard
        standard_suggestions = _find_standard_combinations(
            candidate_invoices, target_amount_abs, max_combination_size, 
            start_time, max_search_time_ms, exclude_ids=exclude_invoice_ids
        )
        
        # Combina suggerimenti client-based e standard
        if enhance_cumulative_matches_with_client_patterns is not None and client_suggestions:
            all_suggestions = enhance_cumulative_matches_with_client_patterns(
                transaction_id, anagraphics_id_filter, standard_suggestions
            )
        else:
            all_suggestions = client_suggestions + standard_suggestions
        
        # Rimuovi duplicati e ordina
        unique_suggestions = _deduplicate_suggestions(all_suggestions)
        
        logger.info(f"{log_prefix}: Ricerca completata. {len(unique_suggestions)} suggerimenti totali")
        return unique_suggestions[:15]  # Top 15
        
    except Exception as e:
        logger.error(f"{log_prefix}: Errore: {e}", exc_info=True)
        return []
    finally:
        if conn:
            conn.close()

def _find_standard_combinations(candidate_invoices, target_amount, max_combination_size, 
                               start_time, max_search_time_ms, exclude_ids):
    """Trova combinazioni standard con algoritmi ottimizzati"""
    suggestions = []
    timeout_ms = max_search_time_ms * 0.6  # Riserva tempo
    
    # Ordina per importo (facilita pruning)
    candidates = sorted(candidate_invoices, key=lambda x: x['amount'])
    
    for size in range(2, min(len(candidates) + 1, max_combination_size + 1)):
        elapsed_ms = (time.monotonic() - start_time) * 1000
        if elapsed_ms > timeout_ms:
            break
        
        # Algoritmo con pruning intelligente
        for combination in _generate_smart_combinations(candidates, size, target_amount):
            elapsed_ms = (time.monotonic() - start_time) * 1000
            if elapsed_ms > timeout_ms:
                break
            
            total_amount = sum(inv['amount_decimal'] for inv in combination)
            
            if abs(total_amount - target_amount) <= AMOUNT_TOLERANCE:
                confidence = _calculate_standard_confidence(combination, target_amount)
                
                suggestions.append({
                    'invoice_ids': [inv['id'] for inv in combination],
                    'total_amount': total_amount,
                    'num_invoices': len(combination),
                    'counterparty': 'N/A',  # Sarà riempito dopo
                    'anagraphics_id': None,
                    'doc_numbers': [inv['doc_number'] for inv in combination],
                    'confidence': 'Standard',
                    'sequence_score': _calculate_sequence_score(combination),
                    'date_range_days': _calculate_date_range(combination),
                    'explanation': f"Combinazione standard {len(combination)} fatture"
                })
        
        if len(suggestions) >= 10:
            break
    
    return suggestions

def _generate_smart_combinations(candidates, size, target_amount):
    """Generatore di combinazioni con pruning intelligente"""
    
    def _can_reach_target(remaining_items, current_sum, needed):
        if needed == 0:
            return abs(current_sum - target_amount) <= AMOUNT_TOLERANCE
        
        if len(remaining_items) < needed:
            return False
        
        # Quick bounds check
        min_possible = current_sum + sum(item['amount_decimal'] for item in remaining_items[:needed])
        max_possible = current_sum + sum(item['amount_decimal'] for item in remaining_items[-needed:])
        
        return min_possible <= target_amount + AMOUNT_TOLERANCE and max_possible >= target_amount - AMOUNT_TOLERANCE
    
    def _recursive_combinations(start_idx, current_combination, current_sum, needed):
        if needed == 0:
            if abs(current_sum - target_amount) <= AMOUNT_TOLERANCE:
                yield current_combination.copy()
            return
        
        for i in range(start_idx, len(candidates) - needed + 1):
            item = candidates[i]
            new_sum = current_sum + item['amount_decimal']
            
            # Pruning: se già sopra il target di troppo, salta
            if new_sum > target_amount + AMOUNT_TOLERANCE * needed:
                continue
            
            # Pruning: verifica se può raggiungere il target
            remaining = candidates[i + 1:]
            if not _can_reach_target(remaining, new_sum, needed - 1):
                continue
            
            current_combination.append(item)
            yield from _recursive_combinations(i + 1, current_combination, new_sum, needed - 1)
            current_combination.pop()
    
    yield from _recursive_combinations(0, [], Decimal('0'), size)

def _calculate_standard_confidence(combination, target_amount):
    """Calcola confidenza per combinazioni standard"""
    base_confidence = 0.6
    
    # Bonus per corrispondenza esatta
    total = sum(inv['amount_decimal'] for inv in combination)
    amount_precision = 1.0 - (abs(total - target_amount) / target_amount)
    
    # Bonus per coerenza temporale
    dates = [inv['doc_date'] for inv in combination if inv['doc_date']]
    date_bonus = 0.0
    if len(dates) >= 2:
        try:
            date_objects = [pd.to_datetime(d).date() for d in dates]
            date_range = (max(date_objects) - min(date_objects)).days
            date_bonus = max(0.0, 1.0 - date_range / 45)
        except:
            date_bonus = 0.0
    
    return min(1.0, base_confidence + amount_precision * 0.3 + date_bonus * 0.1)

def _calculate_sequence_score(combination):
    """Calcola score di sequenza"""
    if len(combination) < 2:
        return 0.5
    
    # Analizza numeri documento per consecutività
    numbers = []
    for inv in combination:
        doc_num = inv.get('doc_number', '')
        # Estrai numeri dal documento
        nums = re.findall(r'\d+', doc_num)
        if nums:
            try:
                numbers.append(int(nums[-1]))  # Prendi l'ultimo numero
            except ValueError:
                continue
    
    if len(numbers) < 2:
        return 0.5
    
    numbers.sort()
    total_range = numbers[-1] - numbers[0] + 1
    actual_count = len(numbers)
    
    return actual_count / total_range

def _calculate_date_range(combination):
    """Calcola range di date in giorni"""
    dates = [inv['doc_date'] for inv in combination if inv['doc_date']]
    
    if len(dates) < 2:
        return 0
    
    try:
        date_objects = [pd.to_datetime(d).date() for d in dates]
        return (max(date_objects) - min(date_objects)).days
    except:
        return 999

def _deduplicate_suggestions(suggestions):
    """Rimuove suggerimenti duplicati"""
    seen_combinations = set()
    unique_suggestions = []
    
    for sugg in suggestions:
        invoice_set = tuple(sorted(sugg['invoice_ids']))
        if invoice_set not in seen_combinations:
            seen_combinations.add(invoice_set)
            unique_suggestions.append(sugg)
    
    # Ordina per qualità
    def sort_key(s):
        is_client_pattern = 'Pattern Cliente' in s.get('confidence', '')
        sequence_score = s.get('sequence_score', 0)
        confidence_score = s.get('confidence_score', 0)
        return (is_client_pattern, confidence_score, sequence_score)
    
    unique_suggestions.sort(key=sort_key, reverse=True)
    return unique_suggestions

def calculate_and_update_item_status(conn, item_type, item_id):
    """Versione ottimizzata del calcolo stato con transazioni batch"""
    item = get_item_details(conn, item_type, item_id)
    if not item:
        logger.warning(f"Impossibile ricalcolare stato: {item_type} ID {item_id} non trovato.")
        return False

    cursor = conn.cursor()
    linked_amount_sum = Decimal('0.0')
    new_status = ''
    original_status = ''
    
    try:
        current_reconciled_db_raw = item['paid_amount'] if item_type == 'invoice' else item['reconciled_amount']
        current_reconciled_db = to_decimal(current_reconciled_db_raw, default='0.0')
    except KeyError as ke:
        logger.error(f"Colonna {ke} mancante nei dettagli {item_type} ID {item_id}. Assumo 0.0.")
        current_reconciled_db = Decimal('0.0')
    except Exception as e_get:
        logger.error(f"Errore accesso importo riconciliato per {item_type} ID {item_id}: {e_get}. Assumo 0.0.")
        current_reconciled_db = Decimal('0.0')

    try:
        if item_type == 'invoice':
            original_status = item['payment_status']
            item_total_db = item['total_amount']
            due_date_str = item['due_date']

            cursor.execute("SELECT SUM(reconciled_amount) FROM ReconciliationLinks WHERE invoice_id = ?", (item_id,))
            result = cursor.fetchone()
            if result and result[0] is not None:
                linked_amount_sum = quantize(to_decimal(result[0]))

            item_total_dec = quantize(to_decimal(item_total_db))
            item_total_to_match = item_total_dec.copy_abs()
            open_tolerance = AMOUNT_TOLERANCE * 10

            base_unpaid_status = 'Aperta'
            due_date_obj = None
            if due_date_str:
                try:
                    if isinstance(due_date_str, str):
                        due_date_obj = date.fromisoformat(due_date_str)
                    elif isinstance(due_date_str, (date, datetime)):
                        due_date_obj = due_date_str if isinstance(due_date_str, date) else due_date_str.date()
                    else:
                        pd_date = pd.to_datetime(due_date_str, errors='coerce')
                        if pd.notna(pd_date):
                            due_date_obj = pd_date.date()
                        else:
                            raise ValueError("Formato data non riconosciuto")

                    if due_date_obj < date.today():
                        base_unpaid_status = 'Scaduta'
                except (TypeError, ValueError) as date_err:
                    logger.warning(f"Formato/tipo data scadenza non valido per I:{item_id}: '{due_date_str}' ({type(due_date_str)}) - {date_err}")
                except Exception as date_generic_err:
                    logger.error(f"Errore generico conversione data scadenza I:{item_id}: '{due_date_str}' - {date_generic_err}", exc_info=True)

            if linked_amount_sum <= open_tolerance / 2:
                new_status = base_unpaid_status
            elif abs(linked_amount_sum - item_total_to_match) <= AMOUNT_TOLERANCE:
                new_status = 'Pagata Tot.'
            elif linked_amount_sum < item_total_to_match:
                new_status = 'Pagata Parz.'
            else:
                logger.warning(f"Fattura {item_id} pagata in eccesso (Link: {linked_amount_sum:.2f} vs Tot: {item_total_to_match:.2f}). Stato: Pagata Tot.")
                new_status = 'Pagata Tot.'

        elif item_type == 'transaction':
            original_status = item['reconciliation_status']
            item_total_db = item['amount']
            ignored_status = 'Ignorato'

            if original_status == ignored_status:
                cursor.execute("SELECT COUNT(id) FROM ReconciliationLinks WHERE transaction_id = ?", (item_id,))
                link_count = cursor.fetchone()[0]
                if link_count > 0:
                    logger.error(f"INCOERENZA: T:{item_id} '{ignored_status}' con {link_count} link attivi. Rimuovo link...")
                    success_remove, _ = remove_reconciliation_links(conn, transaction_id=item_id)
                    if not success_remove:
                        logger.error(f"Fallita rimozione link per T:{item_id} ignorata!")
                    current_reconciled_db = Decimal('0.0')
                    linked_amount_sum = Decimal('0.0')
                else:
                    linked_amount_sum = Decimal('0.0')

                if quantize(current_reconciled_db) != Decimal('0.0'):
                    logger.warning(f"Forzo reconciled_amount a 0 per T:{item_id} '{ignored_status}' (era {current_reconciled_db:.2f}).")
                    if not update_transaction_reconciliation_state(conn, item_id, ignored_status, Decimal('0.0')):
                        logger.error(f"Fallito azzeramento reconciled_amount per T:{item_id} ignorata!")
                        return False
                    else:
                        current_reconciled_db = Decimal('0.0')

                logger.debug(f"T:{item_id} '{ignored_status}' confermata. Nessun cambiamento di stato.")
                return True

            cursor.execute("SELECT SUM(reconciled_amount) FROM ReconciliationLinks WHERE transaction_id = ?", (item_id,))
            result = cursor.fetchone()
            if result and result[0] is not None:
                linked_amount_sum = quantize(to_decimal(result[0]))

            item_total_dec = quantize(to_decimal(item_total_db))
            item_total_abs = item_total_dec.copy_abs()
            open_tolerance = AMOUNT_TOLERANCE * 10

            if linked_amount_sum.copy_abs() <= open_tolerance / 2:
                new_status = 'Da Riconciliare'
            elif abs(linked_amount_sum - item_total_abs) <= AMOUNT_TOLERANCE:
                new_status = 'Riconciliato Tot.'
            elif linked_amount_sum < item_total_abs:
                new_status = 'Riconciliato Parz.'
            else:
                logger.warning(f"T:{item_id} eccedente (Link: {linked_amount_sum:.2f} vs Tot: {item_total_abs:.2f}). Stato: Riconciliato Eccesso.")
                new_status = 'Riconciliato Eccesso'
        else:
            logger.error(f"Tipo elemento non valido: {item_type}")
            return False

        current_reconciled_dec = quantize(current_reconciled_db)
        status_changed = new_status != original_status
        amount_to_update = linked_amount_sum
        amount_changed = amount_to_update != current_reconciled_dec

        if status_changed or amount_changed:
            log_msg = f"Aggiornamento {item_type} ID {item_id}: Stato '{original_status}'->'{new_status}' (C:{status_changed}), Importo {current_reconciled_dec:.2f}->{amount_to_update:.2f} (C:{amount_changed})"
            logger.info(log_msg)
            if item_type == 'invoice':
                if not update_invoice_reconciliation_state(conn, item_id, new_status, amount_to_update):
                    logger.error(f"Fallito aggiornamento DB per Invoice ID {item_id}")
                    return False
            elif item_type == 'transaction':
                if not update_transaction_reconciliation_state(conn, item_id, new_status, amount_to_update):
                    logger.error(f"Fallito aggiornamento DB per Transaction ID {item_id}")
                    return False
        else:
            logger.debug(f"Stato/importo {item_type} {item_id} non cambiati.")

        return True
    except sqlite3.Error as e:
        logger.error(f"Errore DB ricalcolo stato {item_type} ID {item_id}: {e}", exc_info=True)
        return False
    except Exception as e:
        logger.error(f"Errore generico ricalcolo stato {item_type} ID {item_id}: {e}", exc_info=True)
        return False

def suggest_reconciliation_matches_enhanced(invoice_id=None, transaction_id=None, anagraphics_id_ui_filter=None):
    """Versione ottimizzata dei suggerimenti 1:1 con cache e algoritmi migliorati"""
    
    _refresh_anagraphics_cache()
    
    conn = None
    suggestions = []
    processed_suggestions = set()
    search_direction = None
    
    if transaction_id is not None:
        search_direction = 'transaction_to_invoice'
    elif invoice_id is not None:
        search_direction = 'invoice_to_transaction'
    else:
        return suggestions

    log_prefix = f"Sugg.1:1 Enhanced {'T:'+str(transaction_id) if search_direction == 'transaction_to_invoice' else 'I:'+str(invoice_id)}"
    logger.info(f"{log_prefix}: Avvio ricerca suggerimenti 1:1 ottimizzata...")

    try:
        conn = get_connection()
        cursor = conn.cursor()

        if search_direction == 'transaction_to_invoice':
            # Ricerca ottimizzata da transazione a fattura
            cursor.execute("""
                SELECT t.id, t.amount, t.description, t.reconciliation_status, t.reconciled_amount, t.transaction_date
                FROM BankTransactions t WHERE t.id = ?
            """, (transaction_id,))
            
            trans = cursor.fetchone()
            if not trans:
                logger.warning(f"{log_prefix}: Transazione T:{transaction_id} non trovata.")
                return suggestions
                
            if trans['reconciliation_status'] not in ('Da Riconciliare', 'Riconciliato Parz.'):
                logger.debug(f"{log_prefix}: Transazione T:{transaction_id} non idonea.")
                return suggestions

            trans_remaining_signed = quantize(to_decimal(trans['amount']) - to_decimal(trans['reconciled_amount']))
            if trans_remaining_signed.copy_abs() <= AMOUNT_TOLERANCE / 2:
                logger.debug(f"{log_prefix}: Residuo transazione T:{transaction_id} trascurabile.")
                return suggestions

            target_amount_abs = trans_remaining_signed.copy_abs()
            description = trans['description'] or ""
            target_invoice_type = 'Attiva' if trans_remaining_signed > 0 else 'Passiva'
            
            # Estrazione numeri fattura ottimizzata
            invoice_numbers_in_desc = extract_invoice_number(description)
            
            # Identificazione anagrafica ottimizzata
            effective_anag_id_filter = anagraphics_id_ui_filter
            if effective_anag_id_filter is None:
                logger.debug(f"{log_prefix}: Identificazione anagrafica da descrizione...")
                effective_anag_id_filter = find_anagraphics_id_from_description(description)
                logger.info(f"{log_prefix}: ID Anagrafica auto-rilevato: {effective_anag_id_filter}")
            else:
                logger.info(f"{log_prefix}: Utilizzo filtro Anagrafica da UI: {effective_anag_id_filter}")

            # Query ottimizzata con indici
            base_query = """
                SELECT i.id, i.doc_number, i.total_amount, i.paid_amount,
                       ABS(i.total_amount - i.paid_amount) as open_amount_abs_calc,
                       a.denomination, i.anagraphics_id, i.doc_date
                FROM Invoices i 
                JOIN Anagraphics a ON i.anagraphics_id = a.id
                WHERE i.type = ?
                  AND i.payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.')
                  AND ABS(i.total_amount - i.paid_amount) > ?
            """
            
            params = [target_invoice_type, float(AMOUNT_TOLERANCE / 2)]
            
            # Applica filtro anagrafica se disponibile
            if effective_anag_id_filter is not None:
                base_query += " AND i.anagraphics_id = ?"
                params.append(effective_anag_id_filter)
            
            # Ordina per rilevanza (importo simile prima)
            base_query += " ORDER BY ABS(ABS(i.total_amount - i.paid_amount) - ?) ASC LIMIT 50"
            params.append(float(target_amount_abs))
            
            cursor.execute(base_query, params)
            open_invoices = cursor.fetchall()
            
            logger.info(f"{log_prefix}: Trovate {len(open_invoices)} fatture candidate per analisi 1:1")
            
            # Analisi ottimizzata dei match
            for inv in open_invoices:
                inv_id = inv['id']
                inv_number = inv['doc_number'] or ""
                inv_denomination = inv['denomination'] or ""
                
                invoice_remaining = quantize(to_decimal(inv['total_amount']) - to_decimal(inv['paid_amount']))
                invoice_remaining_abs = invoice_remaining.copy_abs()

                if invoice_remaining_abs <= AMOUNT_TOLERANCE / 2:
                    continue
                    
                sugg_key = f"T{transaction_id}-I{inv_id}"
                if sugg_key in processed_suggestions:
                    continue

                # Sistema di scoring migliorato
                confidence_score = 0.0
                reasons = []
                match_details = {
                    'amount': False,
                    'number': False,
                    'name': False,
                    'date': False
                }

                # Match importo (peso maggiore)
                amount_diff = abs(invoice_remaining_abs - target_amount_abs)
                if amount_diff <= AMOUNT_TOLERANCE:
                    confidence_score += 0.6
                    reasons.append("Importo Esatto")
                    match_details['amount'] = True
                elif amount_diff <= target_amount_abs * 0.02:  # Entro 2%
                    confidence_score += 0.4
                    reasons.append("Importo Simile")
                
                # Match numero fattura (peso alto)
                if inv_number and invoice_numbers_in_desc:
                    cleaned_inv_number = re.sub(r'\s+', '', inv_number).upper()
                    for desc_number in invoice_numbers_in_desc:
                        cleaned_desc_number = re.sub(r'\s+', '', desc_number).upper()
                        if cleaned_inv_number == cleaned_desc_number:
                            confidence_score += 0.3
                            reasons.append(f"Num.Fatt:'{inv_number}'")
                            match_details['number'] = True
                            break
                
                # Match denominazione (peso medio)
                if description and inv_denomination and len(inv_denomination) >= 4:
                    desc_lower = description.lower()
                    denom_lower = inv_denomination.lower()
                    
                    # Match esatto denominazione
                    if denom_lower in desc_lower:
                        name_weight = 0.15 + (len(inv_denomination) / len(description)) * 0.1
                        confidence_score += name_weight
                        reasons.append(f"Nome:'{inv_denomination}'")
                        match_details['name'] = True
                    else:
                        # Match parziale parole
                        denom_words = set(re.findall(r'\b\w{4,}\b', denom_lower))
                        desc_words = set(re.findall(r'\b\w{4,}\b', desc_lower))
                        common_words = denom_words.intersection(desc_words)
                        
                        if common_words and len(common_words) >= min(2, len(denom_words)):
                            word_score = len(common_words) / len(denom_words) * 0.1
                            confidence_score += word_score
                            reasons.append(f"Parole:'{','.join(list(common_words)[:2])}'")

                # Bonus coerenza temporale
                try:
                    inv_date = pd.to_datetime(inv['doc_date']).date()
                    trans_date = pd.to_datetime(trans['transaction_date']).date()
                    if inv_date and trans_date:
                        days_diff = abs((trans_date - inv_date).days)
                        if days_diff <= 60:  # Entro 2 mesi
                            temporal_bonus = max(0, 0.05 * (1 - days_diff / 60))
                            confidence_score += temporal_bonus
                            if days_diff <= 30:
                                match_details['date'] = True
                except:
                    pass
                
                # Determina confidenza finale
                if confidence_score >= 0.6:
                    confidence = 'Alta'
                elif confidence_score >= 0.3:
                    confidence = 'Media'
                elif confidence_score >= 0.15:
                    confidence = 'Bassa'
                else:
                    continue  # Scarta se troppo bassa

                if reasons:
                    logger.info(f"{log_prefix}: +++ Sugg. T:{transaction_id}-I:{inv_id} ({confidence}, Score: {confidence_score:.3f}), Motivi: {reasons}")
                    suggestions.append({
                        'type': confidence,
                        'confidence': confidence,
                        'confidence_score': confidence_score,
                        'invoice_ids': [inv_id],
                        'transaction_ids': [transaction_id],
                        'description': f"Fatt {inv_number}: {invoice_remaining_abs:,.2f} ({', '.join(reasons)})",
                        'match_details': match_details,
                        'reasons': reasons
                    })
                    processed_suggestions.add(sugg_key)

        elif search_direction == 'invoice_to_transaction':
            # Implementazione per invoice_to_transaction
            cursor.execute("""
                SELECT i.id, i.type, i.doc_number, i.total_amount, i.paid_amount,
                       i.payment_status, a.denomination, i.anagraphics_id, i.doc_date
                FROM Invoices i JOIN Anagraphics a ON i.anagraphics_id = a.id
                WHERE i.id = ?
            """, (invoice_id,))
            
            inv = cursor.fetchone()
            if not inv:
                logger.warning(f"{log_prefix}: Fattura I:{invoice_id} non trovata.")
                return suggestions
                
            if inv['payment_status'] not in ('Aperta', 'Scaduta', 'Pagata Parz.'):
                logger.debug(f"{log_prefix}: Fattura I:{invoice_id} non idonea.")
                return suggestions

            invoice_remaining = quantize(to_decimal(inv['total_amount']) - to_decimal(inv['paid_amount']))
            if invoice_remaining.copy_abs() <= AMOUNT_TOLERANCE / 2:
                logger.debug(f"{log_prefix}: Residuo fattura I:{invoice_id} trascurabile.")
                return suggestions

            invoice_remaining_abs = invoice_remaining.copy_abs()
            invoice_type = inv['type']
            invoice_number = inv['doc_number'] or ""
            inv_denomination = inv['denomination'] or ""
            inv_anag_id = inv['anagraphics_id']
            
            target_trans_sign = Decimal('1.0') if invoice_type == 'Attiva' else Decimal('-1.0')
            target_trans_amount_signed = invoice_remaining_abs * target_trans_sign
            
            # Query ottimizzata per transazioni
            query_trans = """
                SELECT t.id, t.amount, t.description, t.reconciled_amount, t.transaction_date,
                       (t.amount - t.reconciled_amount) as trans_remaining_signed_calc
                FROM BankTransactions t
                WHERE t.reconciliation_status IN ('Da Riconciliare', 'Riconciliato Parz.')
                  AND SIGN(t.amount - t.reconciled_amount) = ?
                  AND ABS(t.amount - t.reconciled_amount) > ?
                ORDER BY ABS((t.amount - t.reconciled_amount) - ?) ASC
                LIMIT 50
            """
            
            params = [
                int(target_trans_sign), 
                float(AMOUNT_TOLERANCE / 2),
                float(target_trans_amount_signed)
            ]
            
            cursor.execute(query_trans, params)
            open_transactions = cursor.fetchall()
            
            logger.info(f"{log_prefix}: Trovate {len(open_transactions)} transazioni candidate")
            
            # Analisi simile alla direzione opposta
            for trans in open_transactions:
                trans_id = trans['id']
                description = trans['description'] or ""
                
                trans_remaining_signed = quantize(to_decimal(trans['amount']) - to_decimal(trans['reconciled_amount']))
                if trans_remaining_signed.copy_abs() <= AMOUNT_TOLERANCE / 2:
                    continue
                    
                sugg_key = f"T{trans_id}-I{invoice_id}"
                if sugg_key in processed_suggestions:
                    continue

                confidence_score = 0.0
                reasons = []
                
                # Match importo
                amount_diff = abs(trans_remaining_signed - target_trans_amount_signed)
                if amount_diff <= AMOUNT_TOLERANCE:
                    confidence_score += 0.6
                    reasons.append("Importo Esatto")
                
                # Match numero fattura
                invoice_numbers_in_desc = extract_invoice_number(description)
                if invoice_number and invoice_numbers_in_desc:
                    cleaned_inv_number = re.sub(r'\s+', '', invoice_number).upper()
                    for desc_number in invoice_numbers_in_desc:
                        if re.sub(r'\s+', '', desc_number).upper() == cleaned_inv_number:
                            confidence_score += 0.3
                            reasons.append(f"Num.Fatt:'{invoice_number}'")
                            break
                
                # Match denominazione
                if description and inv_denomination and len(inv_denomination) >= 4:
                    if inv_denomination.lower() in description.lower():
                        # Verifica coerenza anagrafica
                        trans_anag_id_check = find_anagraphics_id_from_description(description)
                        name_weight = 0.2 if trans_anag_id_check == inv_anag_id else 0.1
                        confidence_score += name_weight
                        reasons.append(f"Nome:'{inv_denomination}'")

                # Determina confidenza
                if confidence_score >= 0.6:
                    confidence = 'Alta'
                elif confidence_score >= 0.3:
                    confidence = 'Media'
                elif confidence_score >= 0.15:
                    confidence = 'Bassa'
                else:
                    continue

                if reasons:
                    logger.info(f"{log_prefix}: +++ Sugg. T:{trans_id}-I:{invoice_id} ({confidence}, Score: {confidence_score:.3f}), Motivi: {reasons}")
                    suggestions.append({
                        'type': confidence,
                        'confidence': confidence,
                        'confidence_score': confidence_score,
                        'invoice_ids': [invoice_id],
                        'transaction_ids': [trans_id],
                        'description': f"Mov {trans_id}: {trans_remaining_signed:,.2f} ({', '.join(reasons)})",
                        'reasons': reasons
                    })
                    processed_suggestions.add(sugg_key)

        # Ordina per score di confidenza
        suggestions.sort(key=lambda x: (
            {'Alta': 3, 'Media': 2, 'Bassa': 1}.get(x.get('confidence', 'Bassa'), 0),
            x.get('confidence_score', 0)
        ), reverse=True)

    except sqlite3.Error as db_e:
        logger.error(f"{log_prefix}: Errore DB: {db_e}", exc_info=True)
    except Exception as e:
        logger.error(f"{log_prefix}: Errore generico: {e}", exc_info=True)
    finally:
        if conn:
            conn.close()

    logger.info(f"{log_prefix}: Ricerca 1:1 completata. Generati {len(suggestions)} suggerimenti.")
    return suggestions

def update_items_statuses_batch(conn, invoice_ids=None, transaction_ids=None):
    """Versione ottimizzata per aggiornamenti batch di stati"""
    updated_items_count = 0
    processed_invoices = set()
    processed_transactions = set()
    
    # Prepara statement per batch updates
    cursor = conn.cursor()
    
    try:
        # Batch update per fatture
        if invoice_ids:
            invoice_updates = []
            for inv_id in invoice_ids:
                if inv_id not in processed_invoices:
                    # Calcola nuovo stato senza aggiornare subito
                    cursor.execute("""
                        SELECT SUM(reconciled_amount) as total_linked,
                               i.total_amount, i.paid_amount, i.due_date, i.payment_status
                        FROM Invoices i
                        LEFT JOIN ReconciliationLinks rl ON i.id = rl.invoice_id
                        WHERE i.id = ?
                        GROUP BY i.id, i.total_amount, i.paid_amount, i.due_date, i.payment_status
                    """, (inv_id,))
                    
                    result = cursor.fetchone()
                    if result:
                        linked_sum = quantize(to_decimal(result['total_linked'] or 0))
                        total_amount = quantize(to_decimal(result['total_amount']))
                        
                        # Determina nuovo stato
                        if linked_sum <= AMOUNT_TOLERANCE / 2:
                            # Verifica se scaduta
                            new_status = 'Aperta'
                            if result['due_date']:
                                try:
                                    due_date = pd.to_datetime(result['due_date']).date()
                                    if due_date < date.today():
                                        new_status = 'Scaduta'
                                except:
                                    pass
                        elif abs(linked_sum - total_amount) <= AMOUNT_TOLERANCE:
                            new_status = 'Pagata Tot.'
                        else:
                            new_status = 'Pagata Parz.'
                        
                        if new_status != result['payment_status']:
                            invoice_updates.append((new_status, float(linked_sum), inv_id))
                    
                    processed_invoices.add(inv_id)
            
            # Esegui batch update
            if invoice_updates:
                cursor.executemany("""
                    UPDATE Invoices 
                    SET payment_status = ?, paid_amount = ?, updated_at = ?
                    WHERE id = ?
                """, [(status, amount, datetime.now(), inv_id) for status, amount, inv_id in invoice_updates])
                updated_items_count += len(invoice_updates)
        
        # Batch update per transazioni
        if transaction_ids:
            transaction_updates = []
            for trans_id in transaction_ids:
                if trans_id not in processed_transactions:
                    cursor.execute("""
                        SELECT SUM(reconciled_amount) as total_linked,
                               t.amount, t.reconciliation_status
                        FROM BankTransactions t
                        LEFT JOIN ReconciliationLinks rl ON t.id = rl.transaction_id
                        WHERE t.id = ?
                        GROUP BY t.id, t.amount, t.reconciliation_status
                    """, (trans_id,))
                    
                    result = cursor.fetchone()
                    if result:
                        linked_sum = quantize(to_decimal(result['total_linked'] or 0))
                        total_amount = quantize(to_decimal(result['amount']))
                        
                        if result['reconciliation_status'] == 'Ignorato':
                            continue  # Non modificare transazioni ignorate
                        
                        # Determina nuovo stato
                        if linked_sum.copy_abs() <= AMOUNT_TOLERANCE / 2:
                            new_status = 'Da Riconciliare'
                        elif abs(linked_sum - total_amount.copy_abs()) <= AMOUNT_TOLERANCE:
                            new_status = 'Riconciliato Tot.'
                        elif linked_sum < total_amount.copy_abs():
                            new_status = 'Riconciliato Parz.'
                        else:
                            new_status = 'Riconciliato Eccesso'
                        
                        if new_status != result['reconciliation_status']:
                            transaction_updates.append((new_status, float(linked_sum), trans_id))
                    
                    processed_transactions.add(trans_id)
            
            # Esegui batch update
            if transaction_updates:
                cursor.executemany("""
                    UPDATE BankTransactions 
                    SET reconciliation_status = ?, reconciled_amount = ?, updated_at = ?
                    WHERE id = ?
                """, [(status, amount, datetime.now(), trans_id) for status, amount, trans_id in transaction_updates])
                updated_items_count += len(transaction_updates)
        
        logger.debug(f"Aggiornamento batch completato per {updated_items_count} elementi.")
        return True
        
    except Exception as e:
        logger.error(f"Errore aggiornamento batch stati: {e}", exc_info=True)
        return False

def apply_manual_match_optimized(invoice_id, transaction_id, amount_to_match_input):
    """Versione ottimizzata dell'abbinamento manuale"""
    conn = None
    try:
        amount_to_match = quantize(to_decimal(amount_to_match_input))
        if amount_to_match <= Decimal('0.0'):
            return False, "Importo deve essere positivo."

        conn = get_connection()
        conn.execute('BEGIN IMMEDIATE')
        
        # Query ottimizzata con join
        cursor = conn.cursor()
        cursor.execute("""
            SELECT 
                i.id as inv_id, i.total_amount, i.paid_amount, i.payment_status, i.doc_number,
                t.id as trans_id, t.amount, t.reconciled_amount, t.reconciliation_status, t.transaction_date
            FROM Invoices i, BankTransactions t
            WHERE i.id = ? AND t.id = ?
        """, (invoice_id, transaction_id))
        
        result = cursor.fetchone()
        if not result:
            raise ValueError("Fattura o transazione non trovata.")
        
        # Validazioni
        inv_status = result['payment_status']
        tr_status = result['reconciliation_status']
        
        if inv_status in ('Pagata Tot.', 'Riconciliata', 'Insoluta'):
            raise ValueError(f"Fattura {result['doc_number']} già '{inv_status}'.")
        if tr_status in ('Riconciliato Tot.', 'Riconciliato Eccesso', 'Ignorato'):
            raise ValueError(f"Transazione già '{tr_status}'.")

        invoice_remaining = quantize(to_decimal(result['total_amount']) - to_decimal(result['paid_amount']))
        transaction_remaining = quantize(to_decimal(result['amount']) - to_decimal(result['reconciled_amount']))

        if amount_to_match > invoice_remaining.copy_abs() + AMOUNT_TOLERANCE:
            raise ValueError(f"Importo ({amount_to_match:.2f}€) supera residuo fattura ({invoice_remaining.copy_abs():.2f}€).")
        if amount_to_match > transaction_remaining.copy_abs() + AMOUNT_TOLERANCE:
            raise ValueError(f"Importo ({amount_to_match:.2f}€) supera residuo transazione ({transaction_remaining.copy_abs():.2f}€).")

        # Applica link
        if not add_or_update_reconciliation_link(conn, invoice_id, transaction_id, amount_to_match):
            raise sqlite3.Error("Errore creazione/aggiornamento link.")

        # Aggiorna stati in batch
        success = update_items_statuses_batch(conn, invoice_ids=[invoice_id], transaction_ids=[transaction_id])
        if not success:
            raise sqlite3.Error("Errore aggiornamento stati.")

        conn.commit()
        logger.info(f"Abb. manuale ottimizzato I:{invoice_id} <-> T:{transaction_id} per {amount_to_match:.2f}€ OK.")
        return True, "Abbinamento manuale applicato."

    except (sqlite3.Error, ValueError, Exception) as e:
        logger.error(f"Errore abb. manuale ottimizzato (I:{invoice_id}, T:{transaction_id}): {e}", exc_info=True)
        if conn:
            conn.rollback()
        return False, f"Errore: {e}"
    finally:
        if conn:
            conn.close()

def attempt_auto_reconciliation_optimized(transaction_ids, invoice_ids):
    """Versione ottimizzata della riconciliazione automatica N:M"""
    conn = None
    if not transaction_ids or not invoice_ids:
        return False, "Selezionare almeno una transazione e una fattura."

    try:
        conn = get_connection()
        conn.execute('BEGIN IMMEDIATE')
        cursor = conn.cursor()

        # Query batch per validazione
        trans_placeholders = ','.join('?' * len(transaction_ids))
        inv_placeholders = ','.join('?' * len(invoice_ids))
        
        # Recupera tutti i dati necessari in una query
        cursor.execute(f"""
            SELECT 
                'transaction' as type, id, amount, reconciled_amount, reconciliation_status
            FROM BankTransactions 
            WHERE id IN ({trans_placeholders})
            UNION ALL
            SELECT 
                'invoice' as type, id, total_amount as amount, paid_amount as reconciled_amount, payment_status as reconciliation_status
            FROM Invoices 
            WHERE id IN ({inv_placeholders})
        """, transaction_ids + invoice_ids)
        
        results = cursor.fetchall()
        
        # Organizza i risultati
        transactions_data = {}
        invoices_data = {}
        
        for row in results:
            item_id = row['id']
            remaining = quantize(to_decimal(row['amount']) - to_decimal(row['reconciled_amount']))
            
            if row['type'] == 'transaction':
                if row['reconciliation_status'] not in ('Da Riconciliare', 'Riconciliato Parz.'):
                    raise ValueError(f"T:{item_id} (Stato: {row['reconciliation_status']}) non utilizzabile.")
                if remaining.copy_abs() > AMOUNT_TOLERANCE / 2:
                    transactions_data[item_id] = {'remaining': remaining}
            else:  # invoice
                if row['reconciliation_status'] not in ('Aperta', 'Scaduta', 'Pagata Parz.'):
                    raise ValueError(f"I:{item_id} (Stato: {row['reconciliation_status']}) non utilizzabile.")
                if remaining.copy_abs() > AMOUNT_TOLERANCE / 2:
                    invoices_data[item_id] = {'remaining': remaining.copy_abs()}

        actual_transaction_ids = list(transactions_data.keys())
        actual_invoice_ids = list(invoices_data.keys())
        
        if not actual_transaction_ids or not actual_invoice_ids:
            raise ValueError("Nessun elemento valido rimasto dopo filtro residui.")

        # Verifica bilanciamento
        total_trans_remaining = sum(abs(data['remaining']) for data in transactions_data.values())
        total_inv_remaining = sum(data['remaining'] for data in invoices_data.values())

        if abs(total_trans_remaining - total_inv_remaining) > AMOUNT_TOLERANCE:
            mismatch_msg = (f"Somme non bilanciate.\n"
                           f"Mov. (Residuo Assoluto): {total_trans_remaining:,.2f} € | "
                           f"Fatt. (Residuo Assoluto): {total_inv_remaining:,.2f} €\n"
                           f"Differenza Assoluta: {abs(total_trans_remaining - total_inv_remaining):,.2f} €")
            raise ValueError(mismatch_msg)

        # Distribuzione ottimizzata
        links_to_create = []
        
        # Ordina per importo per distribuzione efficiente
        sorted_trans = sorted(actual_transaction_ids, key=lambda tid: abs(transactions_data[tid]['remaining']))
        sorted_inv = sorted(actual_invoice_ids, key=lambda iid: invoices_data[iid]['remaining'])
        
        current_trans_rem = {tid: data['remaining'] for tid, data in transactions_data.items()}
        current_inv_rem = {iid: data['remaining'] for iid, data in invoices_data.items()}
        
        logger.info(f"Inizio distribuzione N:M ottimizzata T:{actual_transaction_ids} <-> I:{actual_invoice_ids}")
        
        for t_id in sorted_trans:
            trans_avail_signed = current_trans_rem[t_id]
            trans_avail_abs = abs(trans_avail_signed)
            
            if trans_avail_abs <= AMOUNT_TOLERANCE / 2:
                continue
            
            for i_id in sorted_inv:
                inv_needed = current_inv_rem[i_id]
                if inv_needed <= AMOUNT_TOLERANCE / 2:
                    continue

                amount_this_link = min(trans_avail_abs, inv_needed)

                if amount_this_link > AMOUNT_TOLERANCE / 2:
                    links_to_create.append((i_id, t_id, amount_this_link))
                    
                    # Aggiorna residui
                    sign_factor = 1 if trans_avail_signed > 0 else -1
                    current_trans_rem[t_id] -= amount_this_link * sign_factor
                    current_inv_rem[i_id] -= amount_this_link
                    
                    current_trans_rem[t_id] = quantize(current_trans_rem[t_id])
                    current_inv_rem[i_id] = quantize(current_inv_rem[i_id])

                    trans_avail_abs = abs(current_trans_rem[t_id])
                    if trans_avail_abs <= AMOUNT_TOLERANCE / 2:
                        break

        # Applica tutti i link in batch
        if links_to_create:
            for invoice_id, transaction_id, amount in links_to_create:
                if not add_or_update_reconciliation_link(conn, invoice_id, transaction_id, amount):
                    raise sqlite3.Error(f"Errore DB link T:{transaction_id}<->I:{invoice_id}")

        # Aggiorna stati in batch
        logger.info(f"Distribuzione completata ({len(links_to_create)} link). Aggiorno stati...")
        success = update_items_statuses_batch(conn, 
                                            invoice_ids=actual_invoice_ids, 
                                            transaction_ids=actual_transaction_ids)
        if not success:
            raise sqlite3.Error("Errore aggiornamento stati batch")

        conn.commit()
        logger.info("Riconciliazione automatica N:M ottimizzata completata con successo.")
        return True, "Riconciliazione automatica completata."

    except ValueError as ve:
        logger.warning(f"Validazione fallita auto-rec ottimizzata: {ve}")
        if conn:
            conn.rollback()
        return False, str(ve)
    except sqlite3.Error as e_db:
        logger.error(f"Errore DB auto-rec ottimizzata: {e_db}", exc_info=True)
        if conn:
            conn.rollback()
        return False, f"Errore database: {e_db}"
    except Exception as e_generic:
        logger.error(f"Errore critico auto-rec ottimizzata: {e_generic}", exc_info=True)
        if conn:
            conn.rollback()
        return False, f"Errore imprevisto: {e_generic}"
    finally:
        if conn:
            conn.close()

def find_automatic_matches_optimized(confidence_level='Exact'):
    """Versione ottimizzata per trovare match automatici"""
    conn = None
    candidate_matches = []
    log_prefix = "[AutoMatchFinder Optimized]"
    logger.info(f"{log_prefix} Avvio ricerca match automatici ottimizzata...")

    try:
        conn = get_connection()
        cursor = conn.cursor()

        # Query ottimizzata con join per ridurre il numero di query
        query_candidates = """
            SELECT 
                t.id as transaction_id,
                t.amount,
                t.reconciled_amount,
                t.description,
                t.transaction_date,
                (t.amount - t.reconciled_amount) as trans_remaining
            FROM BankTransactions t
            WHERE t.reconciliation_status IN ('Da Riconciliare', 'Riconciliato Parz.')
              AND ABS(t.amount - t.reconciled_amount) > ?
            ORDER BY ABS(t.amount - t.reconciled_amount) DESC
            LIMIT 200
        """
        
        cursor.execute(query_candidates, (float(AMOUNT_TOLERANCE / 2),))
        eligible_transactions = cursor.fetchall()
        
        logger.info(f"{log_prefix} Analisi {len(eligible_transactions)} transazioni candidate")

        if not eligible_transactions:
            return []

        # Processa in batch per efficienza
        batch_size = 50
        processed_count = 0
        
        for i in range(0, len(eligible_transactions), batch_size):
            batch = eligible_transactions[i:i + batch_size]
            
            for trans_row in batch:
                trans_id = trans_row['transaction_id']
                trans_remaining = quantize(to_decimal(trans_row['trans_remaining']))
                
                if trans_remaining.copy_abs() <= AMOUNT_TOLERANCE / 2:
                    continue
                
                # Usa la versione ottimizzata per i suggerimenti
                trans_suggestions = suggest_reconciliation_matches_enhanced(
                    transaction_id=trans_id, 
                    anagraphics_id_ui_filter=None
                )

                # Filtra solo match ad alta confidenza con importo esatto
                exact_matches = []
                for sugg in trans_suggestions:
                    if (sugg.get('confidence') == 'Alta' and 
                        'Importo Esatto' in sugg.get('description', '') and
                        sugg.get('confidence_score', 0) >= 0.6):
                        exact_matches.append(sugg)

                # Solo se c'è esattamente un match ottimo
                if len(exact_matches) == 1:
                    match = exact_matches[0]
                    inv_id = match['invoice_ids'][0]
                    match_amount = trans_remaining.copy_abs()

                    # Recupera dettagli fattura
                    cursor.execute("""
                        SELECT doc_number, doc_date 
                        FROM Invoices 
                        WHERE id = ?
                    """, (inv_id,))
                    
                    inv_details_row = cursor.fetchone()
                    inv_details = dict(inv_details_row) if inv_details_row else {}

                    # Formatta date
                    trans_date = 'N/D'
                    if trans_row.get('transaction_date'):
                        try:
                            trans_date = pd.to_datetime(trans_row['transaction_date']).strftime('%d/%m/%Y')
                        except:
                            pass
                    
                    inv_date = 'N/D'
                    if inv_details.get('doc_date'):
                        try:
                            inv_date = pd.to_datetime(inv_details['doc_date']).strftime('%d/%m/%Y')
                        except:
                            pass

                    candidate_matches.append({
                        'transaction_id': trans_id,
                        'invoice_id': inv_id,
                        'amount': match_amount,
                        'trans_date': trans_date,
                        'trans_amount_orig': quantize(to_decimal(trans_row['amount'])),
                        'inv_number': inv_details.get('doc_number', 'N/D'),
                        'inv_date': inv_date,
                        'confidence_score': match.get('confidence_score', 0),
                        'match_reasons': match.get('reasons', [])
                    })
                    
                    logger.info(f"{log_prefix} -> Candidato ottimo: T:{trans_id} <-> I:{inv_id} per {match_amount:.2f} (Score: {match.get('confidence_score', 0):.3f})")
                
                processed_count += 1
                
                # Log progresso ogni 25 elementi
                if processed_count % 25 == 0:
                    logger.debug(f"{log_prefix} Processate {processed_count}/{len(eligible_transactions)} transazioni")

        # Ordina per qualità del match
        candidate_matches.sort(key=lambda x: x.get('confidence_score', 0), reverse=True)
        
        logger.info(f"{log_prefix} Ricerca completata. Trovati {len(candidate_matches)} candidati ottimi per auto-riconciliazione.")
        return candidate_matches[:50]  # Limita a 50 migliori

    except sqlite3.Error as db_e:
        logger.error(f"{log_prefix} Errore DB: {db_e}", exc_info=True)
        return []
    except Exception as e:
        logger.error(f"{log_prefix} Errore generico: {e}", exc_info=True)
        return []
    finally:
        if conn:
            conn.close()

def ignore_transaction(transaction_id):
    """Mantieni la versione originale per ignore_transaction"""
    conn = None
    affected_invoices = []
    log_prefix = f"[Ignore T:{transaction_id}]"
    logger.info(f"{log_prefix} Avvio operazione.")
    try:
        conn = get_connection()
        conn.execute("BEGIN TRANSACTION")

        logger.debug(f"{log_prefix} Rimozione link esistenti...")
        success_remove, (affected_invoices, _) = remove_reconciliation_links(conn, transaction_id=transaction_id)
        if not success_remove:
            logger.error(f"{log_prefix} Fallita rimozione link dal DB.")
            raise sqlite3.Error(f"Errore database rimozione link T:{transaction_id}.")
        logger.info(f"{log_prefix} Rimossi link associati. Fatture affette: {affected_invoices}")

        logger.debug(f"{log_prefix} Aggiornamento stato transazione a 'Ignorato'...")
        success_update = update_transaction_reconciliation_state(conn, transaction_id, 'Ignorato', Decimal('0.0'))
        if not success_update:
            logger.error(f"{log_prefix} Fallito aggiornamento stato transazione nel DB.")
            raise sqlite3.Error(f"Impossibile aggiornare stato T:{transaction_id}.")
        logger.info(f"{log_prefix} Stato transazione aggiornato.")

        if affected_invoices:
            logger.debug(f"{log_prefix} Aggiornamento stato per {len(affected_invoices)} fatture affette...")
            update_items_statuses_batch(conn, invoice_ids=affected_invoices)
            logger.info(f"{log_prefix} Stato fatture affette aggiornato.")

        conn.commit()
        logger.info(f"{log_prefix} Operazione completata con successo.")
        return True, "Movimento bancario marcato ignorato.", affected_invoices

    except (sqlite3.Error, Exception) as e:
        if conn:
            logger.error(f"{log_prefix} Errore durante operazione, rollback...")
            try:
                conn.rollback()
            except Exception as rb_err:
                logger.error(f"{log_prefix} Errore durante rollback: {rb_err}")
        logger.error(f"{log_prefix} Errore: {e}", exc_info=True)
        return False, f"Errore interno ignore_transaction: {e}", []
    finally:
        if conn:
            conn.close()
        logger.debug(f"{log_prefix} Connessione DB chiusa.")

# Mantieni le funzioni originali per compatibilità, ma direzionale verso le versioni ottimizzate
def update_items_statuses(conn, invoice_ids=None, transaction_ids=None):
    """Wrapper per compatibilità che usa la versione batch ottimizzata"""
    return update_items_statuses_batch(conn, invoice_ids, transaction_ids)

def apply_manual_match(invoice_id, transaction_id, amount_to_match_input):
    """Wrapper per compatibilità che usa la versione ottimizzata"""
    return apply_manual_match_optimized(invoice_id, transaction_id, amount_to_match_input)

def attempt_auto_reconciliation(transaction_ids, invoice_ids):
    """Wrapper per compatibilità che usa la versione ottimizzata"""
    return attempt_auto_reconciliation_optimized(transaction_ids, invoice_ids)

def find_automatic_matches(confidence_level='Exact'):
    """Wrapper per compatibilità che usa la versione ottimizzata"""
    return find_automatic_matches_optimized(confidence_level)

# Compatibilità con vecchie funzioni
def suggest_matches_1_1(invoice_id=None, transaction_id=None, anagraphics_id_filter=None):
    """Wrapper per compatibilità"""
    return suggest_reconciliation_matches_enhanced(invoice_id, transaction_id, anagraphics_id_filter)

def suggest_matches_n_m(transaction_id, anagraphics_id_filter=None, 
                       max_combination_size=5, max_search_time_ms=30000,
                       exclude_invoice_ids=None, start_date=None, end_date=None):
    """Wrapper per compatibilità"""
    return suggest_cumulative_matches(transaction_id, anagraphics_id_filter,
                                    max_combination_size, max_search_time_ms,
                                    exclude_invoice_ids, start_date, end_date)
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/core/reconciliation.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/core/smart_client_reconciliation.py ---
# core/smart_client_reconciliation.py
"""
Modulo per riconciliazione intelligente basata sui pattern dei clienti.
Implementa algoritmi avanzati per suggerire abbinamenti tra transazioni e fatture
analizzando i comportamenti storici di pagamento dei clienti.
"""

import logging
import sqlite3
from collections import defaultdict, Counter
from datetime import datetime, timedelta
from decimal import Decimal
from typing import List, Dict, Set, Tuple, Optional, Any
import re
import pandas as pd

try:
    from .database import get_connection
    from .utils import to_decimal, quantize, AMOUNT_TOLERANCE
except ImportError:
    logging.warning("Import relativo fallito in smart_client_reconciliation.py, tento import assoluto.")
    try:
        from database import get_connection
        from utils import to_decimal, quantize, AMOUNT_TOLERANCE
    except ImportError as e:
        logging.critical(f"Impossibile importare dipendenze in smart_client_reconciliation.py: {e}")
        raise ImportError(f"Impossibile importare dipendenze in smart_client_reconciliation.py: {e}") from e

logger = logging.getLogger(__name__)

class ClientPaymentPattern:
    """Rappresenta i pattern di pagamento di un cliente"""
    
    def __init__(self, anagraphics_id: int):
        self.anagraphics_id = anagraphics_id
        self.payment_intervals = []  # giorni tra fattura e pagamento
        self.typical_amounts = []    # importi tipici
        self.payment_methods = Counter()  # metodi di pagamento usati
        self.description_patterns = set()  # pattern nelle descrizioni
        self.monthly_volume = defaultdict(list)  # volume mensile
        self.invoice_sequences = []  # sequenze di fatture pagate insieme
        self.confidence_score = 0.0
        
    def add_payment_record(self, invoice_date, payment_date, amount, description, doc_numbers):
        """Aggiunge un record di pagamento al pattern"""
        if invoice_date and payment_date:
            try:
                inv_dt = pd.to_datetime(invoice_date).date()
                pay_dt = pd.to_datetime(payment_date).date()
                interval = (pay_dt - inv_dt).days
                self.payment_intervals.append(interval)
            except:
                pass
        
        if amount:
            self.typical_amounts.append(amount)
        
        if description:
            # Estrai pattern dalla descrizione
            self._extract_description_patterns(description)
        
        if doc_numbers:
            self.invoice_sequences.append(doc_numbers)
    
    def _extract_description_patterns(self, description: str):
        """Estrae pattern significativi dalla descrizione"""
        if not description:
            return
        
        desc_clean = description.upper().strip()
        
        # Pattern comuni
        patterns = [
            r'\b(FATT|FATTURA|FT|DOC|DOCUMENTO)\s*\.?\s*(\w+)',
            r'\b(PAGAMENTO|PAG|SALDO)\s+(\w+)',
            r'\b(RIF|RIFERIMENTO|REF)\s*\.?\s*(\w+)',
            r'\b([A-Z]{2,})\s+([A-Z]{2,})',  # Aziende con più parole
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, desc_clean)
            for match in matches:
                if isinstance(match, tuple):
                    self.description_patterns.update(match)
                else:
                    self.description_patterns.add(match)
    
    def calculate_confidence(self):
        """Calcola il punteggio di confidenza del pattern"""
        score = 0.0
        
        # Più pagamenti = maggiore confidenza
        payment_count = len(self.payment_intervals)
        if payment_count >= 5:
            score += 0.3
        elif payment_count >= 3:
            score += 0.2
        elif payment_count >= 1:
            score += 0.1
        
        # Consistenza negli intervalli di pagamento
        if self.payment_intervals:
            intervals_std = pd.Series(self.payment_intervals).std()
            if intervals_std < 5:  # Molto consistente
                score += 0.2
            elif intervals_std < 15:  # Abbastanza consistente
                score += 0.1
        
        # Pattern descrizioni ricorrenti
        if len(self.description_patterns) >= 2:
            score += 0.2
        
        # Importi simili
        if len(self.typical_amounts) >= 2:
            amounts_std = pd.Series(self.typical_amounts).std()
            avg_amount = pd.Series(self.typical_amounts).mean()
            if avg_amount > 0:
                cv = amounts_std / avg_amount  # Coefficiente di variazione
                if cv < 0.2:  # Importi molto simili
                    score += 0.2
                elif cv < 0.5:  # Importi abbastanza simili
                    score += 0.1
        
        self.confidence_score = min(1.0, score)
        return self.confidence_score
    
    def get_typical_payment_interval(self) -> Tuple[int, int]:
        """Restituisce l'intervallo tipico di pagamento (min, max giorni)"""
        if not self.payment_intervals:
            return (0, 60)  # Default
        
        intervals = pd.Series(self.payment_intervals)
        median = intervals.median()
        std = intervals.std()
        
        # Usa median +/- std come range tipico
        min_interval = max(0, int(median - std))
        max_interval = min(180, int(median + std))  # Max 6 mesi
        
        return (min_interval, max_interval)
    
    def matches_description_pattern(self, description: str) -> float:
        """Verifica se una descrizione corrisponde ai pattern del cliente"""
        if not description or not self.description_patterns:
            return 0.0
        
        desc_upper = description.upper()
        matches = 0
        
        for pattern in self.description_patterns:
            if pattern in desc_upper:
                matches += 1
        
        return min(1.0, matches / len(self.description_patterns))

class SmartClientReconciliation:
    """Gestore principale per la riconciliazione intelligente"""
    
    def __init__(self):
        self.client_patterns = {}  # anagraphics_id -> ClientPaymentPattern
        self.patterns_cache_time = None
        self.cache_validity_hours = 2
    
    def _refresh_patterns_cache(self, anagraphics_id: int = None):
        """Aggiorna la cache dei pattern per un cliente specifico o tutti"""
        current_time = datetime.now()
        
        # Verifica se cache è ancora valida
        if (self.patterns_cache_time and 
            (current_time - self.patterns_cache_time).total_seconds() < self.cache_validity_hours * 3600 and
            anagraphics_id in self.client_patterns):
            return
        
        conn = None
        try:
            conn = get_connection()
            cursor = conn.cursor()
            
            # Query per recuperare storico pagamenti
            query = """
                SELECT 
                    i.anagraphics_id,
                    i.doc_date,
                    i.doc_number,
                    rl.reconciliation_date,
                    rl.reconciled_amount,
                    bt.description,
                    bt.transaction_date,
                    GROUP_CONCAT(i2.doc_number) as related_invoices
                FROM ReconciliationLinks rl
                JOIN Invoices i ON rl.invoice_id = i.id
                JOIN BankTransactions bt ON rl.transaction_id = bt.id
                LEFT JOIN ReconciliationLinks rl2 ON rl.transaction_id = rl2.transaction_id AND rl2.invoice_id != i.id
                LEFT JOIN Invoices i2 ON rl2.invoice_id = i2.id
                WHERE i.payment_status IN ('Pagata Tot.', 'Pagata Parz.')
                  AND rl.reconciliation_date >= date('now', '-2 years')
            """
            
            params = []
            if anagraphics_id:
                query += " AND i.anagraphics_id = ?"
                params.append(anagraphics_id)
            
            query += """
                GROUP BY rl.id
                ORDER BY i.anagraphics_id, rl.reconciliation_date DESC
            """
            
            cursor.execute(query, params)
            rows = cursor.fetchall()
            
            # Raggruppa per cliente
            client_records = defaultdict(list)
            for row in rows:
                client_records[row['anagraphics_id']].append(row)
            
            # Costruisci pattern per ogni cliente
            for anag_id, records in client_records.items():
                if len(records) < 2:  # Serve almeno qualche record
                    continue
                
                pattern = ClientPaymentPattern(anag_id)
                
                for record in records:
                    doc_numbers = [record['doc_number']]
                    if record['related_invoices']:
                        doc_numbers.extend(record['related_invoices'].split(','))
                    
                    pattern.add_payment_record(
                        record['doc_date'],
                        record['reconciliation_date'] or record['transaction_date'],
                        record['reconciled_amount'],
                        record['description'],
                        doc_numbers
                    )
                
                confidence = pattern.calculate_confidence()
                if confidence >= 0.3:  # Solo pattern abbastanza affidabili
                    self.client_patterns[anag_id] = pattern
                    logger.debug(f"Pattern cliente {anag_id} aggiornato (confidenza: {confidence:.2f})")
            
            self.patterns_cache_time = current_time
            logger.info(f"Cache pattern aggiornata per {len(self.client_patterns)} clienti")
            
        except Exception as e:
            logger.error(f"Errore aggiornamento pattern clienti: {e}", exc_info=True)
        finally:
            if conn:
                conn.close()
    
    def get_client_pattern(self, anagraphics_id: int) -> Optional[ClientPaymentPattern]:
        """Ottiene il pattern di pagamento per un cliente"""
        self._refresh_patterns_cache(anagraphics_id)
        return self.client_patterns.get(anagraphics_id)
    
    def suggest_smart_combinations(self, transaction_id: int, anagraphics_id: int) -> List[Dict]:
        """Suggerisce combinazioni intelligenti basate sui pattern del cliente"""
        pattern = self.get_client_pattern(anagraphics_id)
        if not pattern:
            return []
        
        conn = None
        suggestions = []
        
        try:
            conn = get_connection()
            cursor = conn.cursor()
            
            # Recupera dettagli transazione
            cursor.execute("""
                SELECT amount, reconciled_amount, description, transaction_date
                FROM BankTransactions 
                WHERE id = ?
            """, (transaction_id,))
            
            trans_row = cursor.fetchone()
            if not trans_row:
                return []
            
            target_amount = quantize(
                to_decimal(trans_row['amount']) - to_decimal(trans_row['reconciled_amount'])
            )
            
            if target_amount.copy_abs() <= AMOUNT_TOLERANCE / 2:
                return []
            
            # Analizza pattern descrizione
            desc_match_score = pattern.matches_description_pattern(trans_row['description'])
            
            # Ottieni intervallo tipico pagamento
            min_interval, max_interval = pattern.get_typical_payment_interval()
            
            # Cerca fatture candidate basate sui pattern
            trans_date = pd.to_datetime(trans_row['transaction_date']).date()
            min_doc_date = trans_date - timedelta(days=max_interval)
            max_doc_date = trans_date - timedelta(days=min_interval)
            
            query = """
                SELECT 
                    i.id, i.doc_number, i.doc_date, i.total_amount, i.paid_amount,
                    (i.total_amount - i.paid_amount) as open_amount
                FROM Invoices i
                WHERE i.anagraphics_id = ?
                  AND i.payment_status IN ('Aperta', 'Scaduta', 'Pagata Parz.')
                  AND i.doc_date BETWEEN ? AND ?
                  AND (i.total_amount - i.paid_amount) > ?
                ORDER BY i.doc_date DESC
                LIMIT 20
            """
            
            cursor.execute(query, (
                anagraphics_id,
                min_doc_date.strftime('%Y-%m-%d'),
                max_doc_date.strftime('%Y-%m-%d'),
                float(AMOUNT_TOLERANCE / 2)
            ))
            
            candidate_invoices = cursor.fetchall()
            
            if not candidate_invoices:
                return []
            
            # Analizza combinazioni intelligenti
            suggestions.extend(self._analyze_sequence_patterns(
                candidate_invoices, target_amount, pattern, desc_match_score
            ))
            
            suggestions.extend(self._analyze_amount_patterns(
                candidate_invoices, target_amount, pattern, desc_match_score
            ))
            
            # Ordina per score di confidenza
            suggestions.sort(key=lambda x: x.get('confidence_score', 0), reverse=True)
            
            return suggestions[:10]  # Top 10
            
        except Exception as e:
            logger.error(f"Errore suggerimenti intelligenti T:{transaction_id}, Anag:{anagraphics_id}: {e}", exc_info=True)
            return []
        finally:
            if conn:
                conn.close()
    
    def _analyze_sequence_patterns(self, candidates, target_amount, pattern, desc_score):
        """Analizza pattern di sequenze consecutive"""
        suggestions = []
        
        # Cerca sequenze consecutive nei numeri documento
        candidates_by_number = []
        for inv in candidates:
            # Estrai numero da documento
            doc_num = inv['doc_number']
            numbers = re.findall(r'\d+', doc_num)
            if numbers:
                try:
                    num_val = int(numbers[-1])  # Ultimo numero trovato
                    candidates_by_number.append((num_val, inv))
                except:
                    pass
        
        if len(candidates_by_number) < 2:
            return suggestions
        
        # Ordina per numero
        candidates_by_number.sort(key=lambda x: x[0])
        
        # Cerca sequenze consecutive
        for i in range(len(candidates_by_number)):
            current_total = Decimal('0')
            current_invoices = []
            
            for j in range(i, min(i + 5, len(candidates_by_number))):  # Max 5 fatture
                _, invoice = candidates_by_number[j]
                open_amount = quantize(to_decimal(invoice['open_amount']))
                current_total += open_amount
                current_invoices.append(invoice)
                
                # Verifica se il totale corrisponde
                if abs(current_total - target_amount.copy_abs()) <= AMOUNT_TOLERANCE:
                    # Calcola score basato su pattern
                    sequence_score = self._calculate_sequence_score(current_invoices, pattern)
                    confidence_score = (sequence_score * 0.6) + (desc_score * 0.4)
                    
                    if confidence_score >= 0.4:
                        suggestions.append({
                            'invoice_ids': [inv['id'] for inv in current_invoices],
                            'total_amount': current_total,
                            'confidence_score': confidence_score,
                            'confidence': 'Pattern Cliente' if confidence_score >= 0.7 else 'Cliente',
                            'doc_numbers': [inv['doc_number'] for inv in current_invoices],
                            'explanation': f"Sequenza consecutiva {len(current_invoices)} fatture (Pattern: {sequence_score:.2f})"
                        })
        
        return suggestions
    
    def _analyze_amount_patterns(self, candidates, target_amount, pattern, desc_score):
        """Analizza pattern basati sugli importi tipici"""
        suggestions = []
        
        if not pattern.typical_amounts:
            return suggestions
        
        # Calcola importi tipici del cliente
        amounts_series = pd.Series(pattern.typical_amounts)
        typical_ranges = [
            (amounts_series.quantile(0.25), amounts_series.quantile(0.75)),  # IQR
            (amounts_series.min(), amounts_series.max())  # Range completo
        ]
        
        for min_amt, max_amt in typical_ranges:
            for candidate in candidates:
                open_amount = quantize(to_decimal(candidate['open_amount']))
                
                # Verifica se l'importo rientra nel range tipico
                if min_amt <= float(open_amount) <= max_amt:
                    # Verifica corrispondenza importo
                    if abs(open_amount - target_amount.copy_abs()) <= AMOUNT_TOLERANCE:
                        amount_score = 0.8  # Alta corrispondenza importo
                        confidence_score = (amount_score * 0.5) + (desc_score * 0.3) + (pattern.confidence_score * 0.2)
                        
                        if confidence_score >= 0.5:
                            suggestions.append({
                                'invoice_ids': [candidate['id']],
                                'total_amount': open_amount,
                                'confidence_score': confidence_score,
                                'confidence': 'Pattern Cliente',
                                'doc_numbers': [candidate['doc_number']],
                                'explanation': f"Importo tipico cliente (€{open_amount:.2f})"
                            })
        
        return suggestions
    
    def _calculate_sequence_score(self, invoices, pattern):
        """Calcola score per una sequenza di fatture"""
        if len(invoices) <= 1:
            return 0.5
        
        score = 0.0
        
        # Bonus per sequenze simili a quelle storiche
        current_sequence = [inv['doc_number'] for inv in invoices]
        for historical_seq in pattern.invoice_sequences:
            if len(historical_seq) == len(current_sequence):
                score += 0.3
                break
        
        # Bonus per consecutività numerica
        numbers = []
        for inv in invoices:
            nums = re.findall(r'\d+', inv['doc_number'])
            if nums:
                try:
                    numbers.append(int(nums[-1]))
                except:
                    pass
        
        if len(numbers) == len(invoices) and len(numbers) > 1:
            is_consecutive = all(
                numbers[i] == numbers[i-1] + 1 
                for i in range(1, len(numbers))
            )
            if is_consecutive:
                score += 0.4
        
        # Bonus per date vicine
        dates = []
        for inv in invoices:
            try:
                dates.append(pd.to_datetime(inv['doc_date']).date())
            except:
                pass
        
        if len(dates) == len(invoices) and len(dates) > 1:
            date_range = (max(dates) - min(dates)).days
            if date_range <= 30:  # Entro un mese
                score += 0.2
        
        return min(1.0, score)

# Istanza singleton
_smart_reconciler = None

def get_smart_reconciler() -> SmartClientReconciliation:
    """Ottiene istanza singleton del riconciliatore intelligente"""
    global _smart_reconciler
    if _smart_reconciler is None:
        _smart_reconciler = SmartClientReconciliation()
    return _smart_reconciler

def suggest_client_based_reconciliation(transaction_id: int, anagraphics_id: int) -> List[Dict]:
    """
    Funzione principale per suggerimenti basati sui pattern del cliente.
    Utilizzata da reconciliation.py
    """
    reconciler = get_smart_reconciler()
    return reconciler.suggest_smart_combinations(transaction_id, anagraphics_id)

def enhance_cumulative_matches_with_client_patterns(transaction_id: int, anagraphics_id: int, 
                                                   standard_suggestions: List[Dict]) -> List[Dict]:
    """
    Migliora i suggerimenti standard con informazioni sui pattern del cliente.
    Utilizzata da reconciliation.py
    """
    reconciler = get_smart_reconciler()
    pattern = reconciler.get_client_pattern(anagraphics_id)
    
    if not pattern:
        return standard_suggestions
    
    # Ottieni suggerimenti intelligenti
    smart_suggestions = reconciler.suggest_smart_combinations(transaction_id, anagraphics_id)
    
    # Combina e deduplica
    all_suggestions = smart_suggestions + standard_suggestions
    
    # Rimuovi duplicati basati su invoice_ids
    seen_combinations = set()
    unique_suggestions = []
    
    for sugg in all_suggestions:
        invoice_set = tuple(sorted(sugg['invoice_ids']))
        if invoice_set not in seen_combinations:
            seen_combinations.add(invoice_set)
            unique_suggestions.append(sugg)
    
    # Ordina per qualità (smart suggestions prima)
    def sort_key(s):
        is_smart = 'Pattern Cliente' in s.get('confidence', '')
        confidence_score = s.get('confidence_score', 0)
        return (is_smart, confidence_score)
    
    unique_suggestions.sort(key=sort_key, reverse=True)
    return unique_suggestions

def analyze_client_payment_reliability(anagraphics_id: int) -> Dict[str, Any]:
    """
    Analizza l'affidabilità di pagamento di un cliente.
    Può essere utilizzato per scoring o reporting.
    """
    reconciler = get_smart_reconciler()
    pattern = reconciler.get_client_pattern(anagraphics_id)
    
    if not pattern:
        return {
            'reliability_score': 0.0,
            'payment_history_count': 0,
            'average_delay_days': None,
            'payment_consistency': 'Unknown'
        }
    
    # Calcola metriche di affidabilità
    avg_delay = pd.Series(pattern.payment_intervals).mean() if pattern.payment_intervals else 0
    delay_std = pd.Series(pattern.payment_intervals).std() if pattern.payment_intervals else 0
    
    # Score basato su tempestività e consistenza
    reliability_score = 0.0
    
    if avg_delay <= 30:  # Paga entro 30 giorni
        reliability_score += 0.4
    elif avg_delay <= 60:  # Paga entro 60 giorni
        reliability_score += 0.2
    
    if delay_std <= 10:  # Molto consistente
        reliability_score += 0.3
    elif delay_std <= 20:  # Abbastanza consistente
        reliability_score += 0.1
    
    reliability_score += min(0.3, len(pattern.payment_intervals) / 20)  # Bonus per storico lungo
    
    # Determina categoria consistenza
    if delay_std <= 5:
        consistency = 'Molto Consistente'
    elif delay_std <= 15:
        consistency = 'Consistente'
    elif delay_std <= 30:
        consistency = 'Moderatamente Consistente'
    else:
        consistency = 'Inconsistente'
    
    return {
        'reliability_score': min(1.0, reliability_score),
        'payment_history_count': len(pattern.payment_intervals),
        'average_delay_days': avg_delay,
        'payment_consistency': consistency,
        'confidence_score': pattern.confidence_score
    }
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/core/smart_client_reconciliation.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/core/utils.py ---
import hashlib
import re
import logging
from decimal import Decimal, InvalidOperation, ROUND_HALF_UP
from datetime import datetime, date
import pandas as pd # Necessario per pd.isna e pd.to_datetime in alcune funzioni
import os
import configparser
import base64 # Per serializzare lo stato della UI se non si usa QByteArray direttamente qui
from functools import lru_cache # Per funzioni che beneficiano di caching
from typing import Optional, Dict, Any, List, Union # Per type hinting
try:
    from PySide6.QtCore import QByteArray
    QT_AVAILABLE = True
except ImportError:
    try:
        from PyQt6.QtCore import QByteArray
        QT_AVAILABLE = True
    except ImportError:
        try:
            from PyQt5.QtCore import QByteArray
            QT_AVAILABLE = True
        except ImportError:
            QT_AVAILABLE = False
            logger.warning("Nessuna libreria Qt disponibile per funzioni UI avanzate")


logger = logging.getLogger(__name__)

# --- Costanti ---
AMOUNT_TOLERANCE = Decimal('0.01')
DECIMAL_PRECISION = Decimal('0.01') # Usato da quantize se non specificato diversamente

# Cache per pattern regex compilati
_COMPILED_PATTERNS = {
    'date_pattern_strict': re.compile(r'^\d{4}-\d{2}-\d{2}$'), # Formato ISO per hash
    'date_pattern_flexible': re.compile(r'^(\d{4}[-/.]\d{1,2}[-/.]\d{1,2}|\d{1,2}[-/.]\d{1,2}[-/.]\d{2,4}|\d{8})$'),
    'decimal_clean_basic': re.compile(r'[^\d.,-]'), # Per _clean_numeric_format
    'currency_symbols': re.compile(r'[€$£¥₹]'),
    'percentage_symbol': re.compile(r'[%]'),
    'multiple_whitespace': re.compile(r'\s+'),
    'non_alphanumeric_except_common_separators': re.compile(r'[^\w.\-_]+'), # Per hash transazione
    'normalize_product_remove_details': [], # Sarà popolato in normalize_product_name
    'normalize_product_variety': [],      # Sarà popolato in normalize_product_name
    'invoice_number_extract': [ # Pattern per extract_invoice_number
        re.compile(r'\b(?:fatt|ft|fattura|doc|documento|nr|num|n)\.?\s+([\w/\-]{3,20})', re.IGNORECASE),
        re.compile(r'\b(\d{2,4}[/_-]\d{2,8})\b'), # Numeri con separatori
        re.compile(r'\b([A-Z]{1,4}\s?\d{3,10}[A-Z\d]*)\b', re.IGNORECASE), # Codici alfanumerici
        re.compile(r'\b(\d{4,12})\b') # Numeri puri più lunghi
    ],
    'invoice_number_false_positives': [ # Pattern per _is_false_positive
        re.compile(r'^(?:NR|FT|RIF|DOC|FATT|NUM)$', re.IGNORECASE),
        re.compile(r'^\d{1,3}$'),
        re.compile(r'^(?:19|20)\d{2}$'),
        re.compile(r'^\d{5}$'), # Es. CAP
        re.compile(r'^\d+[.,]\d{2}$'),
        re.compile(r'^0+\d*$'),
        re.compile(r'^\d{16,}$'), # Es. carte di credito
        re.compile(r'^(?:123456|111111|000000)$'),
    ]
}

# --- Percorso Configurazione ---
_CONFIG_FILE_PATH = None

def get_project_root() -> str: # Path è un oggetto, qui restituisco stringa per coerenza con CONFIG_FILE_PATH
    # Tenta di trovare la root del progetto cercando main.py o config.ini
    current = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # Va su di due livelli da core/utils.py
    for _ in range(3): # Cerca fino a 3 livelli sopra
        if os.path.exists(os.path.join(current, 'config.ini')) or \
           os.path.exists(os.path.join(current, 'main.py')):
            return current
        parent = os.path.dirname(current)
        if parent == current: break # Raggiunta la radice del filesystem
        current = parent
    return os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # Fallback se non trovato

def get_config_path() -> str:
    global _CONFIG_FILE_PATH
    if _CONFIG_FILE_PATH is None:
        project_root = get_project_root()
        _CONFIG_FILE_PATH = os.path.join(project_root, 'config.ini')
    return _CONFIG_FILE_PATH

CONFIG_FILE_PATH = get_config_path() # Inizializza la costante globale

# --- Conversioni Numeriche e Quantizzazione ---
def _clean_numeric_format(text: str) -> str:
    """Pulizia ottimizzata formato numerico, usata internamente da to_decimal."""
    if not text: return "0.0"
    cleaned = _COMPILED_PATTERNS['decimal_clean_basic'].sub('', text.strip())
    if not cleaned: return "0.0"

    is_negative = cleaned.startswith('-')
    if cleaned.startswith(('+', '-')): cleaned = cleaned[1:]

    if ',' in cleaned and '.' in cleaned:
        comma_pos = cleaned.rfind(',')
        dot_pos = cleaned.rfind('.')
        cleaned = cleaned.replace('.', '').replace(',', '.') if comma_pos > dot_pos else cleaned.replace(',', '')
    elif ',' in cleaned:
        parts = cleaned.split(',')
        if len(parts) == 2 and len(parts[1]) <= 2 and parts[1].isdigit():
            cleaned = cleaned.replace(',', '.')
        else:
            cleaned = cleaned.replace(',', '')
    
    if cleaned.count('.') > 1: # Gestisce "1.234.567" o "1.2.3"
        parts = cleaned.split('.')
        cleaned = "".join(parts[:-1]) + "." + parts[-1]

    return ('-' + cleaned) if is_negative and cleaned and cleaned != "0.0" else (cleaned if cleaned else "0.0")

def to_decimal(value: Any, default: Union[str, Decimal] = '0.0') -> Decimal:
    """Conversione ottimizzata a Decimal con gestione robusta di vari tipi."""
    default_dec = Decimal(default) if isinstance(default, str) else default
    if value is None or pd.isna(value): return default_dec
    if isinstance(value, Decimal): return value if value.is_finite() else default_dec
    if isinstance(value, (int, float)):
        try: return Decimal(str(value)) # str(value) per float per precisione
        except (InvalidOperation, ValueError): return default_dec
    if isinstance(value, str):
        cleaned_val = value.strip()
        if not cleaned_val or cleaned_val.lower() in ['nan', 'nat', 'none', 'null', '']: return default_dec
        if _COMPILED_PATTERNS['date_pattern_flexible'].match(cleaned_val):
            if re.search(r'[-/]', cleaned_val) or ('.' in cleaned_val and not cleaned_val.replace('.','',1).isdigit()):
                logger.debug(f"Valore '{cleaned_val}' sembra una data, non convertito a Decimal.")
                return default_dec
        try:
            cleaned_num_str = _COMPILED_PATTERNS['currency_symbols'].sub('', cleaned_val)
            cleaned_num_str = _COMPILED_PATTERNS['percentage_symbol'].sub('', cleaned_num_str)
            cleaned_num_format = _clean_numeric_format(cleaned_num_str)
            if not cleaned_num_format or cleaned_num_format in ['-', '+', '.', ',']: return default_dec
            return Decimal(cleaned_num_format)
        except (InvalidOperation, ValueError, OverflowError) as e:
            logger.debug(f"Conversione Decimal fallita per '{cleaned_val}': {e}. Uso default.")
            return default_dec
    try: # Fallback per altri tipi
        str_value = str(value)
        if str_value.lower() in ['nan', 'nat', 'none', 'null', '']: return default_dec
        return to_decimal(str_value, default) # Ricorsione con la stringa
    except Exception: return default_dec

def quantize(decimal_value: Any, precision: Optional[Decimal] = None) -> Decimal:
    """Quantizzazione ottimizzata. Restituisce Decimal('0.00') per NaN o errori."""
    effective_precision = precision if precision is not None else DECIMAL_PRECISION
    if not isinstance(decimal_value, Decimal):
        decimal_value = to_decimal(decimal_value, default='NaN') # Usa 'NaN' per indicare fallimento

    if not decimal_value.is_finite(): # Gestisce NaN, Inf, -Inf
        return Decimal('0.00') # O un altro default sensato, o solleva errore
    try:
        return decimal_value.quantize(effective_precision, rounding=ROUND_HALF_UP)
    except (InvalidOperation, OverflowError):
        return Decimal('0.00') # Come sopra

# --- Normalizzazione Date e Stringhe per Hashing ---
@lru_cache(maxsize=1024)
def _normalize_date_string_for_hash(date_input: Any) -> str:
    """Normalizza date in formato YYYY-MM-DD per hashing consistente."""
    if isinstance(date_input, (datetime, date)):
        return date_input.strftime('%Y-%m-%d')
    if isinstance(date_input, str):
        s_date = date_input.strip()
        if not s_date: return ""
        if _COMPILED_PATTERNS['date_pattern_strict'].match(s_date): return s_date # Già nel formato corretto
        try: # Prova a parsare con pandas, è abbastanza flessibile
            return pd.to_datetime(s_date, errors='raise').strftime('%Y-%m-%d')
        except Exception:
            logger.debug(f"Formato data non standard per hash: '{s_date}'. Uso stringa originale.")
            return s_date # Fallback alla stringa originale se il parsing fallisce
    if pd.isna(date_input) or date_input is None: return ""
    return str(date_input).strip()

# --- Funzioni di Hashing ---
@lru_cache(maxsize=2048) # Aumentato per più fatture
def calculate_invoice_hash(cedente_id: str, cessionario_id: str,
                          doc_type: str, doc_number: str, doc_date: Any) -> str:
    try:
        cedente_clean = str(cedente_id or '').strip().upper()
        cessionario_clean = str(cessionario_id or '').strip().upper()
        doc_type_clean = str(doc_type or '').strip().upper()
        doc_number_clean = _COMPILED_PATTERNS['multiple_whitespace'].sub('', str(doc_number or '')).upper()
        date_normalized = _normalize_date_string_for_hash(doc_date)

        if not all([cedente_clean, cessionario_clean, doc_type_clean, doc_number_clean, date_normalized]):
            logger.warning(f"Dati hash fattura incompleti: C:{cedente_clean},CS:{cessionario_clean},T:{doc_type_clean},N:{doc_number_clean},D:{date_normalized}")

        hash_string = f"INV|{cedente_clean}|{cessionario_clean}|{doc_type_clean}|{doc_number_clean}|{date_normalized}"
        return hashlib.sha256(hash_string.encode('utf-8')).hexdigest()
    except Exception as e:
        logger.error(f"Errore calcolo hash fattura: {e}, Dati: C:{cedente_id},CS:{cessionario_id},T:{doc_type},N:{doc_number},D:{doc_date}", exc_info=True)
        return hashlib.sha256(f"ERROR_INV_{datetime.now().isoformat()}".encode('utf-8')).hexdigest()

@lru_cache(maxsize=2048) # Aumentato per più transazioni
def calculate_transaction_hash(transaction_date: Any, amount: Any, description: str) -> str:
    try:
        date_normalized = _normalize_date_string_for_hash(transaction_date)
        
        amount_decimal = to_decimal(amount, default='NaN')
        amount_normalized = f"{quantize(amount_decimal):.2f}" if amount_decimal.is_finite() else "INVALID_AMOUNT"

        desc_clean = str(description or '').strip().upper()
        desc_clean = _COMPILED_PATTERNS['non_alphanumeric_except_common_separators'].sub(' ', desc_clean)
        desc_clean = _COMPILED_PATTERNS['multiple_whitespace'].sub(' ', desc_clean).strip()
        desc_part = desc_clean[:200] # Limita lunghezza per consistenza e performance

        if not all([date_normalized, amount_normalized != "INVALID_AMOUNT"]):
             logger.warning(f"Dati hash transazione incompleti: D:{date_normalized}, Imp:{amount_normalized}, Desc:'{desc_part[:30]}...'")

        hash_string = f"TRX|{date_normalized}|{amount_normalized}|{desc_part}"
        return hashlib.sha256(hash_string.encode('utf-8')).hexdigest()
    except Exception as e:
        logger.error(f"Errore calcolo hash transazione: {e}, Dati: D:{transaction_date}, Imp:{amount}, Desc:'{str(description)[:30]}...'", exc_info=True)
        return hashlib.sha256(f"ERROR_TRX_{datetime.now().isoformat()}".encode('utf-8')).hexdigest()

# --- Utility per Anagrafiche e Prodotti ---
@lru_cache(maxsize=256)
def _is_own_company(anag_data: Dict[str, Any], my_company_data: Dict[str, Any]) -> bool:
    if not my_company_data or (not my_company_data.get('piva') and not my_company_data.get('cf')):
        if anag_data: logger.error("_is_own_company: Dati azienda (my_company_data) non validi o mancanti in config.")
        return False
    if not anag_data: return False

    anag_piva = str(anag_data.get('piva', '')).strip().upper().replace('IT', '')
    anag_cf = str(anag_data.get('cf', '')).strip().upper()
    my_piva = str(my_company_data.get('piva', '')).strip().upper().replace('IT', '')
    my_cf = str(my_company_data.get('cf', '')).strip().upper()

    if my_piva and anag_piva and my_piva == anag_piva: return True
    if my_cf and anag_cf and my_cf == anag_cf: return True
    if my_piva and anag_cf and my_piva == anag_cf: return True # P.IVA numerica può essere usata come CF per aziende
    if my_cf and anag_piva and my_cf == anag_piva: return True # CF numerico può essere P.IVA

    return False

@lru_cache(maxsize=1024)
def normalize_product_name(name: str) -> Optional[str]:
    """Normalizza i nomi dei prodotti per un'analisi più consistente."""
    if not isinstance(name, str) or not name.strip():
        return None

    norm_name = name.lower().strip()
    norm_name = _COMPILED_PATTERNS['multiple_whitespace'].sub(' ', norm_name)

    # Inizializza i pattern compilati solo una volta
    if not _COMPILED_PATTERNS['normalize_product_remove_details']:
        patterns_to_remove = [
            r'\b\d{1,2}[-/.]\d{1,2}(?:[-/.]\d{2,4})?\b', # Date
            r'\b(?:lotto?|lot|lt|art|cod)\.?\s*[:\-]?\s*[\w\d/\-\.]+', # Lotti, articoli, codici
            r'\b\d{4,}\b', # Numeri lunghi (possibili codici o anni isolati)
            r'\b(?:cal|calibro|gr|kg|lt|pz|nr|collo|conf|iva|sc)\.?\s*[\d\w\.\-/+]+', # Unità di misura, calibri, ecc.
            r'\bx\s*\d+', # Moltiplicatori tipo "x12"
            r'\b(?:italia|italy|sicilia|spagna|grecia|estero|nazionale|nostrano|origine|provenienza|tipo|variet[aà]|qualit[aà]|uso|cat)\s*[\w\.\-/+]+', # Origine, tipo, categoria
            r'\b(?:extra|prima|seconda|scelta|select)\b', # Qualità
            r'\b(?:confezionat[aoe]?|sfuso|gabbia|plateaux|plt|cassetta|cestino|vaschetta|sacchetto|retina|legaccio|mazzo|grappolo|pianta)\b', # Confezionamento
            r'\b(?:bianc[hoaie]|ner[oaei]|ross[oaei]|giall[oaei]|verd[ei]|viol[ae]|arancio|striat[oaei]|tond[oaei]|lung[oaei]|oval[ei])\b', # Colori, forme
            r'\b(?:dolce|piccante|amaro|matur[oaei]|primofiore|primizie|bio|biologico|fresco|fresche|grande|piccolo|medio)\b', # Attributi
            r'\b(?:da\s+industria|da\s+succo|per\s+conserve)\b', # Uso specifico
            # Rimuovi singole lettere o numeri se non parte di un nome composto riconosciuto
            r'\b[a-z]\b', r'\b\d\b',
            r'\s*-\s*', r'\s*/\s*' # Separatori isolati
        ]
        _COMPILED_PATTERNS['normalize_product_remove_details'] = [re.compile(p, re.IGNORECASE) for p in patterns_to_remove]
        
        # Pattern per varietà specifiche da preservare o gestire in modo speciale (qui sono rimossi se non parte del nome principale)
        # Questa parte è complessa e richiederebbe un dizionario di mapping. Per ora, li rimuoviamo se sembrano isolati.
        variety_patterns_to_clean = [
            r'\b(?:red\s+globe|seedless|victoria|black\s+magic|pizzutella|regina|michele\s+palieri)\b',
            r'\b(?:golden(?:\s+delicious)?|gala|fuji|granny\s+smith|red\s+delicious|stark)\b',
            r'\b(?:williams|decana|conference|abate\s+fetel|coscia|kaiser)\b',
            r'\b(?:navel|tarocco|moro|sanguinello|valencia|clementine)\b',
        ]
        _COMPILED_PATTERNS['normalize_product_variety'] = [re.compile(p, re.IGNORECASE) for p in variety_patterns_to_clean]

    for pattern in _COMPILED_PATTERNS['normalize_product_remove_details']:
        norm_name = pattern.sub(' ', norm_name)
    
    # Per le varietà, se rimangono come unica parola o con poche altre, potrebbero essere il nome normalizzato.
    # Questa è una logica semplificata. Una soluzione migliore userebbe un dizionario di mapping.
    # Per ora, le rimuoviamo come gli altri dettagli, poi puliamo gli spazi.
    for pattern in _COMPILED_PATTERNS['normalize_product_variety']:
        norm_name = pattern.sub(' ', norm_name)

    # Pulizia finale
    norm_name = re.sub(r'[^\w\s]', ' ', norm_name) # Rimuovi punteggiatura residua
    norm_name = _COMPILED_PATTERNS['multiple_whitespace'].sub(' ', norm_name).strip()
    
    # Se dopo la pulizia rimangono parole comuni come "uva", "mela", le manteniamo.
    # Se il nome diventa troppo corto o irriconoscibile, potrebbe essere meglio scartarlo.
    if len(norm_name.split()) == 1 and len(norm_name) < 3 and not norm_name.isdigit(): # es. "u"
        return None
    if not norm_name or len(norm_name) < 3 : # Nomi troppo corti non sono utili
        logger.debug(f"Normalizzazione prodotto per '{name}' ha prodotto un risultato troppo corto/vuoto: '{norm_name}'")
        return None
        
    return norm_name

@lru_cache(maxsize=1024)
def extract_invoice_number(text: str) -> List[str]:
    """Estrae potenziali numeri di fattura da una stringa, ottimizzato."""
    if not text or not isinstance(text, str): return []
    
    # Pre-processamento leggero: spazi uniformi, no punteggiatura fine a se stessa
    processed_text = _COMPILED_PATTERNS['multiple_whitespace'].sub(' ', text)
    processed_text = f' {processed_text} ' # Aggiungi spazi per matchare \b

    found_numbers = set()
    for pattern in _COMPILED_PATTERNS['invoice_number_extract']:
        try:
            matches = pattern.findall(processed_text)
            for match_item in matches:
                # findall restituisce tuple se ci sono gruppi nel pattern, altrimenti stringhe
                num_str = match_item if isinstance(match_item, str) else match_item[0] 
                cleaned_num = re.sub(r'\s+', '', num_str).upper().strip('.-/_')
                if cleaned_num and len(cleaned_num) >= 3 and len(cleaned_num) <= 20 and \
                   re.search(r'\d', cleaned_num) and \
                   not any(fp.match(cleaned_num) for fp in _COMPILED_PATTERNS['invoice_number_false_positives']):
                    found_numbers.add(cleaned_num)
        except re.error as e:
            logger.warning(f"Errore Regex in extract_invoice_number pattern '{pattern.pattern}': {e}")
        except Exception as e_gen:
            logger.error(f"Errore generico in extract_invoice_number: {e_gen}")

    # Ordina per rilevanza (lunghezza, contenuto numerico)
    sorted_numbers = sorted(list(found_numbers), key=lambda x: (len(x), -sum(c.isdigit() for c in x)), reverse=True)
    return sorted_numbers[:5] # Limita a top 5 risultati

# --- Gestione Stato UI (Persistente su config.ini) ---
UI_STATE_SECTION = 'UI_Table_States' # Nome della sezione in config.ini

def save_table_state_persistent(table_name_key: str, serialized_state: str):
    """Salva uno stato serializzato (stringa base64) nel file config.ini."""
    if not QT_AVAILABLE: 
        logger.debug(f"Qt non disponibile, salvataggio stato tabella '{table_name_key}' saltato.")
        return False
    
    try:
        config = configparser.ConfigParser()
        if os.path.exists(CONFIG_FILE_PATH):
            config.read(CONFIG_FILE_PATH, encoding='utf-8')
        
        if not config.has_section(UI_STATE_SECTION):
            config.add_section(UI_STATE_SECTION)
        
        config.set(UI_STATE_SECTION, table_name_key, serialized_state)
        
        with open(CONFIG_FILE_PATH, 'w', encoding='utf-8') as configfile:
            config.write(configfile)
        logger.debug(f"Stato tabella '{table_name_key}' salvato persistentemente.")
        return True
    except Exception as e:
        logger.error(f"Errore salvataggio persistente stato tabella '{table_name_key}': {e}")
        return False

def restore_table_state_persistent(table_name_key: str) -> Optional[str]:
    """Ripristina uno stato serializzato (stringa base64) da config.ini."""
    if not QT_AVAILABLE: 
        logger.debug(f"Qt non disponibile, ripristino stato tabella '{table_name_key}' saltato.")
        return None
    try:
        config = configparser.ConfigParser()
        if not os.path.exists(CONFIG_FILE_PATH):
            logger.debug(f"File config non trovato per restore stato tabella '{table_name_key}'.")
            return None
        config.read(CONFIG_FILE_PATH, encoding='utf-8')
        
        if config.has_option(UI_STATE_SECTION, table_name_key):
            serialized_state = config.get(UI_STATE_SECTION, table_name_key)
            logger.debug(f"Stato tabella '{table_name_key}' ripristinato persistentemente.")
            return serialized_state
        else:
            logger.debug(f"Nessuno stato persistente trovato per tabella '{table_name_key}'.")
            return None
    except Exception as e:
        logger.error(f"Errore ripristino persistente stato tabella '{table_name_key}': {e}")
        return None

# Wrapper per le funzioni Qt che ora sono in main_window o viste specifiche
# Queste sono solo placeholder se utils non deve dipendere da Qt.
# Le UI chiameranno direttamente queste versioni in utils:
# save_table_state_persistent e restore_table_state_persistent
# passando la stringa base64.
def save_table_state(table_name: str, header_view_or_state_string: Union[Any, str]) -> bool:
    """
    Wrapper. Se header_view_or_state_string è una stringa (base64), la salva.
    Altrimenti, logga un avviso perché la logica Qt dovrebbe essere nella UI.
    """
    if isinstance(header_view_or_state_string, str):
        return save_table_state_persistent(table_name, header_view_or_state_string)
    else:
        logger.warning(f"save_table_state chiamata con oggetto Qt ({type(header_view_or_state_string)})."
                       "La serializzazione Qt dovrebbe avvenire nel modulo UI.")
        # Qui si potrebbe provare a chiamare header_view_or_state_string.saveState()
        # e base64 encode, ma si introduce dipendenza Qt
        if QT_AVAILABLE and hasattr(header_view_or_state_string, 'saveState'):
             try:
                 state_bytes = header_view_or_state_string.saveState()
                 state_base64 = base64.urlsafe_b64encode(state_bytes.data()).decode('utf-8')
                 return save_table_state_persistent(table_name, state_base64)
             except Exception as e_qt_save:
                 logger.error(f"Errore durante serializzazione Qt in save_table_state: {e_qt_save}")
        return False

def restore_table_state(table_name: str, header_view_to_restore: Any) -> bool:
    """
    Wrapper. Ottiene lo stato stringa e aspetta che la UI lo applichi.
    Restituisce True se lo stato è stato trovato, False altrimenti.
    La UI dovrà poi fare header_view_to_restore.restoreState(QByteArray(base64_decoded_string)).
    """
    serialized_state = restore_table_state_persistent(table_name)
    if serialized_state:
        if QT_AVAILABLE and hasattr(header_view_to_restore, 'restoreState'):
            try:
                state_bytes_data = base64.urlsafe_b64decode(serialized_state.encode('utf-8'))
                state_qbytearray = QByteArray(state_bytes_data)
                if header_view_to_restore.restoreState(state_qbytearray):
                    logger.debug(f"Stato tabella '{table_name}' applicato con successo all'header_view.")
                    return True
                else:
                    logger.warning(f"Fallito restoreState Qt per tabella '{table_name}'.")
                    return False # Stato trovato ma non applicabile
            except Exception as e_qt_restore:
                logger.error(f"Errore durante deserializzazione/applicazione stato Qt in restore_table_state: {e_qt_restore}")
                return False
        else: # Stato trovato, ma non possiamo applicarlo da qui senza Qt
            logger.warning(f"Stato per '{table_name}' trovato, ma l'applicazione Qt è responsabilità della UI.")
            return True # Indica che uno stato è stato trovato
    return False


# --- Funzioni di Formattazione (come quelle della tua ultima dashboard_view) ---
def format_currency(value: Any, default_str: str = "€ 0,00") -> str:
    if value is None: return default_str
    try:
        decimal_value = to_decimal(value) # Usa la to_decimal robusta
        if not decimal_value.is_finite(): return default_str
        
        quantized_value = quantize(decimal_value) # Usa la quantize robusta
        
        # Formattazione italiana
        # Esempio: Decimal('-1234.56') -> "-1.234,56 €"
        # Esempio: Decimal('1234.56') -> "1.234,56 €"
        # Esempio: Decimal('0.00') -> "0,00 €"
        
        sign = "-" if quantized_value.is_signed() and quantized_value.copy_abs() != Decimal('0') else ""
        abs_value_str = f"{quantized_value.copy_abs():,.2f}" # es. 1,234.56 o 0.00
        
        # Sostituisci per formato italiano
        # "1,234.56" -> "1X234Y56" -> "1.234,56"
        # "0.00" -> "0Y00" -> "0,00"
        formatted_abs = abs_value_str.replace(",", "X").replace(".", ",").replace("X", ".")
        
        return f"{sign}{formatted_abs} €"

    except Exception as e:
        logger.warning(f"Errore formattazione valuta per '{value}': {e}")
        return default_str

def format_percentage(value: Any, default_str: str = "--") -> str:
    if value is None or pd.isna(value): return default_str
    try:
        if isinstance(value, Decimal): float_value = float(value)
        elif isinstance(value, (int, float)): float_value = float(value)
        else:
            # Prova a convertire a Decimal e poi a float
            dec_val = to_decimal(value, default='NaN')
            if not dec_val.is_finite(): return default_str
            float_value = float(dec_val)
        
        sign = "+" if float_value >= 0 else ""
        return f"{sign}{float_value:.1f}%"
    except (ValueError, TypeError, InvalidOperation) as e:
        logger.warning(f"Errore formattazione percentuale per '{value}': {e}")
        return default_str

# --- Altre Utility Potenzialmente Utili (da integrare se necessario) ---
# validate_vat_number, validate_tax_code, normalize_anagraphics_data, ecc.
# Se queste erano nella tua utils.py da 1300 righe e sono usate, andrebbero qui.
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/core/utils.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/main.py ---
#!/usr/bin/env python3
"""
FatturaAnalyzer v2 - FastAPI Backend
Main application entry point che integra con il core PySide6 esistente
"""

import logging
import os
import sys
from contextlib import asynccontextmanager
from pathlib import Path

import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles

# Add path per importare il core esistente
backend_path = Path(__file__).parent.parent
sys.path.insert(0, str(backend_path))

from app.config import settings
from app.adapters.database_adapter import db_adapter
from app.middleware.error_handler import ErrorHandlerMiddleware
from app.api import (
    anagraphics,
    analytics,
    invoices,
    transactions,
    reconciliation,
    import_export,
    sync
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/fattura_analyzer_api.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan events"""
    # Startup
    logger.info("🚀 Starting FatturaAnalyzer API...")
    
    # Create logs directory
    os.makedirs('logs', exist_ok=True)
    
    # Initialize database usando il core esistente
    try:
        await db_adapter.create_tables_async()
        logger.info("✅ Database initialized successfully using existing core")
    except Exception as e:
        logger.error(f"❌ Database initialization failed: {e}")
        raise
    
    # Test database connection
    try:
        result = await db_adapter.execute_query_async("SELECT 1 as test")
        if result and result[0]['test'] == 1:
            logger.info("✅ Database connection test successful")
        else:
            raise Exception("Unexpected result from test query")
    except Exception as e:
        logger.error(f"❌ Database connection test failed: {e}")
        raise
    
    # Validate company configuration
    if not settings.COMPANY_VAT and not settings.COMPANY_CF:
        logger.warning("⚠️ Company VAT/CF not configured. Invoice type detection may not work properly.")
    else:
        logger.info(f"✅ Company config loaded: {settings.COMPANY_NAME} (VAT: {settings.COMPANY_VAT}, CF: {settings.COMPANY_CF})")
    
    logger.info("🎉 FatturaAnalyzer API started successfully!")
    
    yield
    
    # Shutdown
    logger.info("👋 Shutting down FatturaAnalyzer API...")

# Create FastAPI application
app = FastAPI(
    title="FatturaAnalyzer API",
    description="Modern invoice and financial management system for Italian businesses - Integrated with PySide6 Core",
    version="2.0.0",
    docs_url="/api/docs" if settings.DEBUG else None,
    redoc_url="/api/redoc" if settings.DEBUG else None,
    lifespan=lifespan
)

# Add middleware
app.add_middleware(GZipMiddleware, minimum_size=1000)
app.add_middleware(ErrorHandlerMiddleware)

# CORS configuration for Tauri frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "tauri://localhost",
        "https://tauri.localhost",
        "http://localhost:3000",  # Development
        "http://127.0.0.1:3000",
        "http://localhost:1420",  # Tauri dev
        "http://127.0.0.1:1420",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Health check endpoint
@app.get("/health")
async def health_check():
    """Health check endpoint"""
    try:
        # Test database connection usando adapter
        test_result = await db_adapter.execute_query_async("SELECT 1 as test")
        
        # Test core availability
        core_status = "available"
        try:
            from app.core.database import get_connection
            conn = get_connection()
            conn.close()
        except Exception as e:
            core_status = f"error: {str(e)}"
        
        return {
            "status": "healthy",
            "version": "2.0.0",
            "database": "connected" if test_result else "error",
            "core_integration": core_status,
            "company_configured": bool(settings.COMPANY_VAT or settings.COMPANY_CF),
            "timestamp": "2025-06-03T00:00:00Z"
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(status_code=503, detail="Service Unavailable")

# API Routes
app.include_router(anagraphics.router, prefix="/api/anagraphics", tags=["Anagraphics"])
app.include_router(invoices.router, prefix="/api/invoices", tags=["Invoices"])
app.include_router(transactions.router, prefix="/api/transactions", tags=["Transactions"])
app.include_router(reconciliation.router, prefix="/api/reconciliation", tags=["Reconciliation"])
app.include_router(analytics.router, prefix="/api/analytics", tags=["Analytics"])
app.include_router(import_export.router, prefix="/api/import", tags=["Import/Export"])
app.include_router(sync.router, prefix="/api/sync", tags=["Cloud Sync"])

# Serve static files (if needed)
if os.path.exists("static"):
    app.mount("/static", StaticFiles(directory="static"), name="static")

# Global exception handler
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    logger.error(f"Global exception: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal server error",
            "message": "An unexpected error occurred" if not settings.DEBUG else str(exc)
        }
    )

# Root endpoint
@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "FatturaAnalyzer API v2.0 - Integrated with PySide6 Core",
        "status": "running",
        "docs": "/api/docs" if settings.DEBUG else "disabled",
        "health": "/health",
        "core_integration": "active",
        "features": [
            "Invoice Management",
            "Bank Transaction Processing", 
            "Smart Reconciliation",
            "Analytics & Reporting",
            "Google Drive Sync",
            "Multi-format Import/Export"
        ]
    }

def main():
    """Main function for running the server"""
    uvicorn.run(
        "app.main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.DEBUG,
        log_level="info",
        access_log=True
    )

if __name__ == "__main__":
    main()
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/main.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/middleware/__init__.py ---
# Path: backend/app/middleware/__init__.py

# Questo file rende la directory un package Python

--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/middleware/__init__.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/middleware/auth.py ---
# Path: backend/app/middleware/auth.py


--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/middleware/auth.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/middleware/cors.py ---
# Path: backend/app/middleware/cors.py


--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/middleware/cors.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/middleware/error_handler.py ---
# Path: backend/app/middleware/error_handler.py


--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/middleware/error_handler.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/models/__init__.py ---
"""
Pydantic models for FastAPI request/response validation
"""

from datetime import date, datetime
from decimal import Decimal
from typing import Optional, List, Dict, Any, Union
from enum import Enum

from pydantic import BaseModel, Field, ConfigDict, validator


# Base configuration for all models
class BaseConfig:
    model_config = ConfigDict(
        from_attributes=True,
        use_enum_values=True,
        validate_assignment=True,
        str_strip_whitespace=True
    )


# Enums for type safety
class AnagraphicsType(str, Enum):
    CLIENTE = "Cliente"
    FORNITORE = "Fornitore"


class InvoiceType(str, Enum):
    ATTIVA = "Attiva"
    PASSIVA = "Passiva"


class PaymentStatus(str, Enum):
    APERTA = "Aperta"
    SCADUTA = "Scaduta"
    PAGATA_PARZ = "Pagata Parz."
    PAGATA_TOT = "Pagata Tot."
    INSOLUTA = "Insoluta"
    RICONCILIATA = "Riconciliata"


class ReconciliationStatus(str, Enum):
    DA_RICONCILIARE = "Da Riconciliare"
    RICONCILIATO_PARZ = "Riconciliato Parz."
    RICONCILIATO_TOT = "Riconciliato Tot."
    RICONCILIATO_ECCESSO = "Riconciliato Eccesso"
    IGNORATO = "Ignorato"


# Anagraphics Models
class AnagraphicsBase(BaseModel, BaseConfig):
    type: AnagraphicsType
    piva: Optional[str] = Field(None, max_length=20, description="Partita IVA")
    cf: Optional[str] = Field(None, max_length=16, description="Codice Fiscale")
    denomination: str = Field(..., min_length=1, max_length=200, description="Denominazione")
    address: Optional[str] = Field(None, max_length=200)
    cap: Optional[str] = Field(None, max_length=10)
    city: Optional[str] = Field(None, max_length=100)
    province: Optional[str] = Field(None, max_length=5)
    country: str = Field(default="IT", max_length=5)
    iban: Optional[str] = Field(None, max_length=34)
    email: Optional[str] = Field(None, max_length=100)
    phone: Optional[str] = Field(None, max_length=20)
    pec: Optional[str] = Field(None, max_length=100)
    codice_destinatario: Optional[str] = Field(None, max_length=10)

    @validator('piva')
    def validate_piva(cls, v):
        if v and len(v) < 5:
            raise ValueError('Partita IVA must be at least 5 characters')
        return v

    @validator('cf')
    def validate_cf(cls, v):
        if v and len(v) < 10:
            raise ValueError('Codice Fiscale must be at least 10 characters')
        return v


class AnagraphicsCreate(AnagraphicsBase):
    pass


class AnagraphicsUpdate(BaseModel, BaseConfig):
    type: Optional[AnagraphicsType] = None
    piva: Optional[str] = None
    cf: Optional[str] = None
    denomination: Optional[str] = None
    address: Optional[str] = None
    cap: Optional[str] = None
    city: Optional[str] = None
    province: Optional[str] = None
    country: Optional[str] = None
    iban: Optional[str] = None
    email: Optional[str] = None
    phone: Optional[str] = None
    pec: Optional[str] = None
    codice_destinatario: Optional[str] = None


class Anagraphics(AnagraphicsBase):
    id: int
    score: float = Field(default=100.0, ge=0, le=100)
    created_at: datetime
    updated_at: datetime


# Invoice Models
class InvoiceLineBase(BaseModel, BaseConfig):
    line_number: int = Field(..., ge=1)
    description: Optional[str] = Field(None, max_length=500)
    quantity: Optional[float] = Field(None, ge=0)
    unit_measure: Optional[str] = Field(None, max_length=20)
    unit_price: Optional[float] = Field(None, ge=0)
    total_price: float = Field(..., ge=0)
    vat_rate: float = Field(..., ge=0, le=100)
    item_code: Optional[str] = Field(None, max_length=50)
    item_type: Optional[str] = Field(None, max_length=50)


class InvoiceLineCreate(InvoiceLineBase):
    pass


class InvoiceLine(InvoiceLineBase):
    id: int
    invoice_id: int


class InvoiceVATSummaryBase(BaseModel, BaseConfig):
    vat_rate: float = Field(..., ge=0, le=100)
    taxable_amount: float = Field(..., ge=0)
    vat_amount: float = Field(..., ge=0)


class InvoiceVATSummaryCreate(InvoiceVATSummaryBase):
    pass


class InvoiceVATSummary(InvoiceVATSummaryBase):
    id: int
    invoice_id: int


class InvoiceBase(BaseModel, BaseConfig):
    anagraphics_id: int = Field(..., gt=0)
    type: InvoiceType
    doc_type: Optional[str] = Field(None, max_length=10)
    doc_number: str = Field(..., min_length=1, max_length=50)
    doc_date: date
    total_amount: float = Field(..., ge=0)
    due_date: Optional[date] = None
    payment_method: Optional[str] = Field(None, max_length=50)
    notes: Optional[str] = Field(None, max_length=1000)
    xml_filename: Optional[str] = Field(None, max_length=200)
    p7m_source_file: Optional[str] = Field(None, max_length=200)


class InvoiceCreate(InvoiceBase):
    lines: Optional[List[InvoiceLineCreate]] = []
    vat_summary: Optional[List[InvoiceVATSummaryCreate]] = []


class InvoiceUpdate(BaseModel, BaseConfig):
    anagraphics_id: Optional[int] = None
    type: Optional[InvoiceType] = None
    doc_type: Optional[str] = None
    doc_number: Optional[str] = None
    doc_date: Optional[date] = None
    total_amount: Optional[float] = None
    due_date: Optional[date] = None
    payment_status: Optional[PaymentStatus] = None
    paid_amount: Optional[float] = None
    payment_method: Optional[str] = None
    notes: Optional[str] = None


class Invoice(InvoiceBase):
    id: int
    payment_status: PaymentStatus = PaymentStatus.APERTA
    paid_amount: float = Field(default=0.0, ge=0)
    unique_hash: str
    created_at: datetime
    updated_at: datetime
    
    # Computed fields
    open_amount: Optional[float] = None
    counterparty_name: Optional[str] = None
    
    # Relations
    lines: List[InvoiceLine] = []
    vat_summary: List[InvoiceVATSummary] = []


# Transaction Models
class BankTransactionBase(BaseModel, BaseConfig):
    transaction_date: date
    value_date: Optional[date] = None
    amount: float = Field(..., description="Transaction amount (positive for income, negative for expense)")
    description: Optional[str] = Field(None, max_length=500)
    causale_abi: Optional[int] = Field(None, ge=0)


class BankTransactionCreate(BankTransactionBase):
    unique_hash: str = Field(..., min_length=1, max_length=100)


class BankTransactionUpdate(BaseModel, BaseConfig):
    transaction_date: Optional[date] = None
    value_date: Optional[date] = None
    amount: Optional[float] = None
    description: Optional[str] = None
    causale_abi: Optional[int] = None
    reconciliation_status: Optional[ReconciliationStatus] = None
    reconciled_amount: Optional[float] = None


class BankTransaction(BankTransactionBase):
    id: int
    reconciliation_status: ReconciliationStatus = ReconciliationStatus.DA_RICONCILIARE
    reconciled_amount: float = Field(default=0.0, ge=0)
    unique_hash: str
    created_at: datetime
    updated_at: Optional[datetime] = None
    
    # Computed fields
    remaining_amount: Optional[float] = None


# Reconciliation Models
class ReconciliationLinkBase(BaseModel, BaseConfig):
    transaction_id: int = Field(..., gt=0)
    invoice_id: int = Field(..., gt=0)
    reconciled_amount: float = Field(..., gt=0)
    notes: Optional[str] = Field(None, max_length=500)


class ReconciliationLinkCreate(ReconciliationLinkBase):
    pass


class ReconciliationLink(ReconciliationLinkBase):
    id: int
    reconciliation_date: datetime
    created_at: datetime


class ReconciliationSuggestion(BaseModel, BaseConfig):
    confidence: str = Field(..., description="Alta, Media, Bassa")
    confidence_score: float = Field(..., ge=0, le=1)
    invoice_ids: List[int]
    transaction_ids: Optional[List[int]] = None
    description: str
    total_amount: float
    match_details: Optional[Dict[str, Any]] = None
    reasons: Optional[List[str]] = []


class ReconciliationRequest(BaseModel, BaseConfig):
    invoice_id: int = Field(..., gt=0)
    transaction_id: int = Field(..., gt=0)
    amount: float = Field(..., gt=0)


class ReconciliationBatchRequest(BaseModel, BaseConfig):
    invoice_ids: List[int] = Field(..., min_items=1)
    transaction_ids: List[int] = Field(..., min_items=1)


# Analytics Models
class KPIData(BaseModel, BaseConfig):
    total_receivables: float
    total_payables: float
    overdue_receivables_count: int
    overdue_receivables_amount: float
    overdue_payables_count: int
    overdue_payables_amount: float
    revenue_ytd: float
    revenue_prev_year_ytd: float
    revenue_yoy_change_ytd: Optional[float] = None
    gross_margin_ytd: float
    margin_percent_ytd: Optional[float] = None
    avg_days_to_payment: Optional[float] = None
    inventory_turnover_estimate: Optional[float] = None
    active_customers_month: int
    new_customers_month: int


class CashFlowData(BaseModel, BaseConfig):
    month: str
    incassi_clienti: float
    incassi_contanti: float
    altri_incassi: float
    pagamenti_fornitori: float
    spese_carte: float
    carburanti: float
    trasporti: float
    utenze: float
    tasse_tributi: float
    commissioni_bancarie: float
    altri_pagamenti: float
    net_operational_flow: float
    total_inflows: float
    total_outflows: float
    net_cash_flow: float


class MonthlyRevenueData(BaseModel, BaseConfig):
    month: str
    revenue: float
    cost: float
    gross_margin: float
    margin_percent: float


class TopClientData(BaseModel, BaseConfig):
    id: int
    denomination: str
    total_revenue: float
    num_invoices: int
    score: float
    avg_order_value: float
    last_order_date: Optional[date] = None


class ProductAnalysisData(BaseModel, BaseConfig):
    normalized_product: str
    total_quantity: float
    total_value: float
    num_invoices: int
    avg_unit_price: float
    original_descriptions: List[str]


class AgingBucket(BaseModel, BaseConfig):
    label: str
    amount: float
    count: int


class AgingSummary(BaseModel, BaseConfig):
    buckets: List[AgingBucket]
    total_amount: float
    total_count: int


# Import/Export Models
class ImportResult(BaseModel, BaseConfig):
    processed: int
    success: int
    duplicates: int
    errors: int
    unsupported: int
    files: List[Dict[str, str]]


class FileUploadResponse(BaseModel, BaseConfig):
    filename: str
    size: int
    content_type: str
    status: str
    message: Optional[str] = None


# Cloud Sync Models
class SyncStatus(BaseModel, BaseConfig):
    enabled: bool
    service_available: bool
    remote_file_id: Optional[str] = None
    last_sync_time: Optional[datetime] = None
    auto_sync_running: bool


class SyncResult(BaseModel, BaseConfig):
    success: bool
    action: Optional[str] = None
    message: str
    timestamp: datetime


# Filter and Pagination Models
class PaginationParams(BaseModel, BaseConfig):
    page: int = Field(default=1, ge=1)
    size: int = Field(default=50, ge=1, le=1000)
    
    @property
    def offset(self) -> int:
        return (self.page - 1) * self.size
    
    @property
    def limit(self) -> int:
        return self.size


class DateRangeFilter(BaseModel, BaseConfig):
    start_date: Optional[date] = None
    end_date: Optional[date] = None
    
    @validator('end_date')
    def end_date_must_be_after_start_date(cls, v, values):
        if v and values.get('start_date') and v < values['start_date']:
            raise ValueError('end_date must be after start_date')
        return v


class AnagraphicsFilter(BaseModel, BaseConfig):
    type: Optional[AnagraphicsType] = None
    search: Optional[str] = Field(None, max_length=100)
    city: Optional[str] = Field(None, max_length=100)
    province: Optional[str] = Field(None, max_length=5)


class InvoiceFilter(BaseModel, BaseConfig):
    type: Optional[InvoiceType] = None
    status: Optional[PaymentStatus] = None
    anagraphics_id: Optional[int] = None
    search: Optional[str] = Field(None, max_length=100)
    date_range: Optional[DateRangeFilter] = None
    min_amount: Optional[float] = Field(None, ge=0)
    max_amount: Optional[float] = Field(None, ge=0)


class TransactionFilter(BaseModel, BaseConfig):
    status: Optional[ReconciliationStatus] = None
    search: Optional[str] = Field(None, max_length=100)
    date_range: Optional[DateRangeFilter] = None
    min_amount: Optional[float] = None
    max_amount: Optional[float] = None
    anagraphics_id_heuristic: Optional[int] = None
    hide_pos: bool = False
    hide_worldline: bool = False
    hide_cash: bool = False
    hide_commissions: bool = False


# Response Models with Pagination
class PaginatedResponse(BaseModel, BaseConfig):
    items: List[Any]
    total: int
    page: int
    size: int
    pages: int


class AnagraphicsListResponse(BaseModel, BaseConfig):
    items: List[Anagraphics]
    total: int
    page: int
    size: int
    pages: int


class InvoiceListResponse(BaseModel, BaseConfig):
    items: List[Invoice]
    total: int
    page: int
    size: int
    pages: int


class TransactionListResponse(BaseModel, BaseConfig):
    items: List[BankTransaction]
    total: int
    page: int
    size: int
    pages: int


# API Response Models
class APIResponse(BaseModel, BaseConfig):
    success: bool
    message: str
    data: Optional[Any] = None
    timestamp: datetime = Field(default_factory=datetime.now)


class ErrorResponse(BaseModel, BaseConfig):
    success: bool = False
    error: str
    message: str
    timestamp: datetime = Field(default_factory=datetime.now)


# Dashboard Models
class DashboardKPIs(BaseModel, BaseConfig):
    # Financial KPIs
    total_receivables: float = Field(description="Total amount receivable")
    total_payables: float = Field(description="Total amount payable")
    overdue_receivables_amount: float = Field(description="Overdue receivables amount")
    overdue_receivables_count: int = Field(description="Number of overdue receivables")
    overdue_payables_amount: float = Field(description="Overdue payables amount")
    overdue_payables_count: int = Field(description="Number of overdue payables")
    
    # Revenue KPIs
    revenue_ytd: float = Field(description="Revenue year-to-date")
    revenue_prev_year_ytd: float = Field(description="Revenue previous year YTD")
    revenue_yoy_change_ytd: Optional[float] = Field(None, description="YoY revenue change percentage")
    gross_margin_ytd: float = Field(description="Gross margin YTD")
    margin_percent_ytd: Optional[float] = Field(None, description="Margin percentage YTD")
    
    # Operational KPIs
    avg_days_to_payment: Optional[float] = Field(None, description="Average days to payment")
    inventory_turnover_estimate: Optional[float] = Field(None, description="Estimated inventory turnover")
    active_customers_month: int = Field(description="Active customers this month")
    new_customers_month: int = Field(description="New customers this month")


class DashboardData(BaseModel, BaseConfig):
    kpis: DashboardKPIs
    recent_invoices: List[Invoice]
    recent_transactions: List[BankTransaction]
    cash_flow_summary: List[CashFlowData]
    top_clients: List[TopClientData]
    overdue_invoices: List[Invoice]


# Search Models
class SearchResult(BaseModel, BaseConfig):
    type: str = Field(description="Type of result: invoice, transaction, anagraphics")
    id: int
    title: str
    subtitle: str
    amount: Optional[float] = None
    date: Optional[date] = None
    status: Optional[str] = None


class SearchResponse(BaseModel, BaseConfig):
    query: str
    results: List[SearchResult]
    total: int
    took_ms: int


# Export all models
__all__ = [
    # Enums
    'AnagraphicsType', 'InvoiceType', 'PaymentStatus', 'ReconciliationStatus',
    
    # Anagraphics
    'AnagraphicsBase', 'AnagraphicsCreate', 'AnagraphicsUpdate', 'Anagraphics',
    
    # Invoices
    'InvoiceLineBase', 'InvoiceLineCreate', 'InvoiceLine',
    'InvoiceVATSummaryBase', 'InvoiceVATSummaryCreate', 'InvoiceVATSummary',
    'InvoiceBase', 'InvoiceCreate', 'InvoiceUpdate', 'Invoice',
    
    # Transactions
    'BankTransactionBase', 'BankTransactionCreate', 'BankTransactionUpdate', 'BankTransaction',
    
    # Reconciliation
    'ReconciliationLinkBase', 'ReconciliationLinkCreate', 'ReconciliationLink',
    'ReconciliationSuggestion', 'ReconciliationRequest', 'ReconciliationBatchRequest',
    
    # Analytics
    'KPIData', 'CashFlowData', 'MonthlyRevenueData', 'TopClientData', 'ProductAnalysisData',
    'AgingBucket', 'AgingSummary',
    
    # Import/Export
    'ImportResult', 'FileUploadResponse',
    
    # Sync
    'SyncStatus', 'SyncResult',
    
    # Filters and Pagination
    'PaginationParams', 'DateRangeFilter', 'AnagraphicsFilter', 'InvoiceFilter', 'TransactionFilter',
    
    # Responses
    'PaginatedResponse', 'AnagraphicsListResponse', 'InvoiceListResponse', 'TransactionListResponse',
    'APIResponse', 'ErrorResponse',
    
    # Dashboard
    'DashboardKPIs', 'DashboardData',
    
    # Search
    'SearchResult', 'SearchResponse'
]
--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/models/__init__.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/models/anagraphics.py ---
# Path: backend/app/models/anagraphics.py


--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/models/anagraphics.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/models/invoice.py ---
# Path: backend/app/models/invoice.py


--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/models/invoice.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/models/reconciliation.py ---
# Path: backend/app/models/reconciliation.py


--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/models/reconciliation.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/app/models/transaction.py ---
# Path: backend/app/models/transaction.py


--- Fine contenuto di: FatturaAnalyzer-v2/backend/app/models/transaction.py ---

--- Contenuto di: FatturaAnalyzer-v2/backend/requirements.txt ---
# Backend Python Dependencies
fastapi>=0.95.0,<0.110.0
uvicorn[standard]>=0.20.0,<0.24.0
pydantic>=1.10.0,<3.0.0
sqlalchemy>=1.4.0,<2.1.0
databases[sqlite]>=0.7.0,<0.9.0
python-dotenv>=0.20.0,<1.1.0
python-jose[cryptography]>=3.3.0,<3.4.0
passlib[bcrypt]>=1.7.4,<1.8.0
lxml>=4.9.0,<5.0.0
# pyOpenSSL>=22.0.0,<24.0.0
# cryptography>=38.0.0,<42.0.0

--- Fine contenuto di: FatturaAnalyzer-v2/backend/requirements.txt ---

--- Contenuto di: FatturaAnalyzer-v2/backend/run.py ---
# Path: backend/run.py


--- Fine contenuto di: FatturaAnalyzer-v2/backend/run.py ---

--- Contenuto di: FatturaAnalyzer-v2/config/database_schema.sql ---

--- Fine contenuto di: FatturaAnalyzer-v2/config/database_schema.sql ---

--- Contenuto di: FatturaAnalyzer-v2/config/development.ini ---

--- Fine contenuto di: FatturaAnalyzer-v2/config/development.ini ---

--- Contenuto di: FatturaAnalyzer-v2/config/production.ini ---

--- Fine contenuto di: FatturaAnalyzer-v2/config/production.ini ---

--- Contenuto di: FatturaAnalyzer-v2/docs/API.md ---
# API.md

Path: docs/API.md

--- Fine contenuto di: FatturaAnalyzer-v2/docs/API.md ---

--- Contenuto di: FatturaAnalyzer-v2/docs/DEPLOYMENT.md ---
# DEPLOYMENT.md

Path: docs/DEPLOYMENT.md

--- Fine contenuto di: FatturaAnalyzer-v2/docs/DEPLOYMENT.md ---

--- Contenuto di: FatturaAnalyzer-v2/docs/SETUP.md ---
# SETUP.md

Path: docs/SETUP.md

--- Fine contenuto di: FatturaAnalyzer-v2/docs/SETUP.md ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/package.json ---
{
  "name": "frontend",
"private": true,
"version": "0.0.0",
"type": "module",
  "scripts": {
    "dev": "vite",
"build": "tsc && vite build",
    "lint": "eslint . --ext ts,tsx --report-unused-disable-directives --max-warnings 0",
    "preview": "vite preview",
"tauri": "tauri"
  },
  "dependencies": {
    "react": "^18.2.0",
"react-dom": "^18.2.0",
"react-router-dom": "^6.15.0",
    "zustand": "^4.4.1",
"axios": "^1.4.0",
"clsx": "^2.0.0",
    "tailwind-merge": "^1.14.0",
"lucide-react": "^0.274.0",
    "@radix-ui/react-slot": "^1.0.2",
"tailwindcss-animate": "^1.0.7"
  },
  "devDependencies": {
    "@tauri-apps/api": "^1.4.0",
"@types/react": "^18.2.15",
"@types/react-dom": "^18.2.7",
    "@typescript-eslint/eslint-plugin": "^6.0.0",
"@typescript-eslint/parser": "^6.0.0",
    "@vitejs/plugin-react": "^4.0.3",
"autoprefixer": "^10.4.14",
"eslint": "^8.45.0",
    "eslint-plugin-react-hooks": "^4.6.0",
"eslint-plugin-react-refresh": "^0.4.3",
    "postcss": "^8.4.27",
"tailwindcss": "^3.3.3",
"typescript": "^5.0.2",
"vite": "^4.4.5"
  }
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/package.json ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/postcss.config.js ---
// Path: frontend/postcss.config.js

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/postcss.config.js ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/public/favicon.ico ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/public/favicon.ico ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/App.tsx ---
import React, { useEffect } from 'react';
import { BrowserRouter as Router, Routes, Route, Navigate } from 'react-router-dom';
import { QueryClient, QueryClientProvider } from '@tanstack/react-query';
import { invoke } from '@tauri-apps/api/tauri';

// Layout components
import { Layout } from '@/components/layout/Layout';
import { Toaster } from '@/components/ui/sonner';
import { ThemeProvider } from '@/components/providers/ThemeProvider';

// Page components
import { DashboardPage } from '@/pages/DashboardPage';
import { InvoicesPage } from '@/pages/InvoicesPage';
import { InvoiceDetailPage } from '@/pages/InvoiceDetailPage';
import { TransactionsPage } from '@/pages/TransactionsPage';
import { TransactionDetailPage } from '@/pages/TransactionDetailPage';
import { ReconciliationPage } from '@/pages/ReconciliationPage';
import { AnagraphicsPage } from '@/pages/AnagraphicsPage';
import { AnagraphicsDetailPage } from '@/pages/AnagraphicsDetailPage';
import { AnalyticsPage } from '@/pages/AnalyticsPage';
import { ImportExportPage } from '@/pages/ImportExportPage';
import { SettingsPage } from '@/pages/SettingsPage';

// Store
import { useUIStore } from '@/store';

// Styles
import './globals.css';

// React Query client
const queryClient = new QueryClient({
  defaultOptions: {
    queries: {
      retry: 1,
      refetchOnWindowFocus: false,
      staleTime: 5 * 60 * 1000, // 5 minutes
    },
    mutations: {
      retry: 1,
    },
  },
});

function App() {
  const { setLoading, setError, addNotification } = useUIStore();

  useEffect(() => {
    // Initialize Tauri app
    const initializeApp = async () => {
      try {
        setLoading('app-init', true);
        
        // Check if running in Tauri
        if (window.__TAURI_IPC__) {
          // Setup Tauri-specific configurations
          await invoke('app_ready');
          
          addNotification({
            type: 'success',
            title: 'Applicazione avviata',
            message: 'FatturaAnalyzer è pronto all\'uso',
            duration: 3000,
          });
        }
        
        // Test API connection
        try {
          const response = await fetch('http://127.0.0.1:8000/health');
          if (!response.ok) {
            throw new Error('API non raggiungibile');
          }
          
          const health = await response.json();
          if (health.status !== 'healthy') {
            throw new Error('API non funzionante');
          }
          
          addNotification({
            type: 'success',
            title: 'Connessione API',
            message: 'Backend collegato correttamente',
            duration: 2000,
          });
        } catch (apiError) {
          setError('api-connection', 'Impossibile connettersi al backend FastAPI');
          addNotification({
            type: 'error',
            title: 'Errore connessione',
            message: 'Verificare che il backend sia avviato su porta 8000',
            duration: 0, // Persistent
            action: {
              label: 'Riprova',
              onClick: () => window.location.reload(),
            },
          });
        }
        
      } catch (error) {
        console.error('App initialization error:', error);
        setError('app-init', 'Errore durante l\'inizializzazione');
        addNotification({
          type: 'error',
          title: 'Errore inizializzazione',
          message: 'Si è verificato un errore durante l\'avvio',
          duration: 0,
        });
      } finally {
        setLoading('app-init', false);
      }
    };

    initializeApp();
  }, [setLoading, setError, addNotification]);

  return (
    <QueryClientProvider client={queryClient}>
      <ThemeProvider>
        <Router>
          <div className="min-h-screen bg-background font-sans antialiased">
            <Routes>
              <Route path="/" element={<Layout />}>
                {/* Dashboard */}
                <Route index element={<Navigate to="/dashboard" replace />} />
                <Route path="dashboard" element={<DashboardPage />} />
                
                {/* Invoices */}
                <Route path="invoices" element={<InvoicesPage />} />
                <Route path="invoices/:id" element={<InvoiceDetailPage />} />
                
                {/* Transactions */}
                <Route path="transactions" element={<TransactionsPage />} />
                <Route path="transactions/:id" element={<TransactionDetailPage />} />
                
                {/* Reconciliation */}
                <Route path="reconciliation" element={<ReconciliationPage />} />
                
                {/* Anagraphics */}
                <Route path="anagraphics" element={<AnagraphicsPage />} />
                <Route path="anagraphics/:id" element={<AnagraphicsDetailPage />} />
                
                {/* Analytics */}
                <Route path="analytics" element={<AnalyticsPage />} />
                
                {/* Import/Export */}
                <Route path="import" element={<ImportExportPage />} />
                
                {/* Settings */}
                <Route path="settings" element={<SettingsPage />} />
                
                {/* Catch all */}
                <Route path="*" element={<Navigate to="/dashboard" replace />} />
              </Route>
            </Routes>
            
            {/* Global Notifications */}
            <Toaster 
              position="top-right"
              expand={true}
              richColors
              closeButton
            />
          </div>
        </Router>
      </ThemeProvider>
    </QueryClientProvider>
  );
}

export default App;
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/App.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/anagraphics/AnagraphicsForm.tsx ---
// Path: frontend/src/components/anagraphics/AnagraphicsForm.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/anagraphics/AnagraphicsForm.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/anagraphics/AnagraphicsList.tsx ---
// Path: frontend/src/components/anagraphics/AnagraphicsList.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/anagraphics/AnagraphicsList.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/analytics/ChartsLibrary.tsx ---
// Path: frontend/src/components/analytics/ChartsLibrary.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/analytics/ChartsLibrary.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/analytics/ExportTools.tsx ---
// Path: frontend/src/components/analytics/ExportTools.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/analytics/ExportTools.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/analytics/ReportsView.tsx ---
// Path: frontend/src/components/analytics/ReportsView.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/analytics/ReportsView.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/dashboard/CashFlowChart.tsx ---
import React from 'react';
import {
  BarChart,
  Bar,
  XAxis,
  YAxis,
  CartesianGrid,
  Tooltip,
  ResponsiveContainer,
  ReferenceLine,
} from 'recharts';

// Utils
import { formatCurrency, formatCurrencyCompact } from '@/lib/formatters';

// Types
import type { CashFlowData } from '@/types';

interface CashFlowChartProps {
  data: CashFlowData[];
  variant?: 'detailed' | 'simple';
}

export function CashFlowChart({ data, variant = 'simple' }: CashFlowChartProps) {
  // Custom tooltip
  const CustomTooltip = ({ active, payload, label }: any) => {
    if (active && payload && payload.length) {
      const data = payload[0].payload;
      
      return (
        <div className="bg-background border border-border rounded-lg shadow-lg p-4 min-w-64">
          <p className="font-medium text-foreground mb-3">{label}</p>
          
          <div className="space-y-2">
            <div className="flex justify-between">
              <span className="text-sm text-green-600">📈 Entrate Totali:</span>
              <span className="font-medium text-green-600">{formatCurrency(data.total_inflows)}</span>
            </div>
            <div className="flex justify-between">
              <span className="text-sm text-red-600">📉 Uscite Totali:</span>
              <span className="font-medium text-red-600">{formatCurrency(data.total_outflows)}</span>
            </div>
            <div className="border-t pt-2 mt-2">
              <div className="flex justify-between">
                <span className="text-sm font-medium">💰 Flusso Netto:</span>
                <span className={`font-bold ${data.net_cash_flow >= 0 ? 'text-green-600' : 'text-red-600'}`}>
                  {formatCurrency(data.net_cash_flow)}
                </span>
              </div>
            </div>
            
            {variant === 'detailed' && (
              <div className="mt-3 pt-2 border-t space-y-1 text-xs">
                <div className="flex justify-between">
                  <span className="text-muted-foreground">Incassi Clienti:</span>
                  <span>{formatCurrency(data.incassi_clienti)}</span>
                </div>
                <div className="flex justify-between">
                  <span className="text-muted-foreground">Pagamenti Fornitori:</span>
                  <span>{formatCurrency(data.pagamenti_fornitori)}</span>
                </div>
                <div className="flex justify-between">
                  <span className="text-muted-foreground">Commissioni:</span>
                  <span>{formatCurrency(data.commissioni_bancarie)}</span>
                </div>
              </div>
            )}
          </div>
        </div>
      );
    }
    return null;
  };

  // Process data for chart
  const chartData = data.map(item => ({
    ...item,
    month: new Date(item.month + '-01').toLocaleDateString('it-IT', { 
      month: 'short', 
      year: '2-digit' 
    }),
    // Negative values for outflows to show them below zero line
    total_outflows_negative: -Math.abs(item.total_outflows),
  }));

  // Calculate summary stats
  const totalInflows = data.reduce((sum, item) => sum + item.total_inflows, 0);
  const totalOutflows = data.reduce((sum, item) => sum + item.total_outflows, 0);
  const netCashFlow = totalInflows - totalOutflows;
  const avgMonthlyFlow = netCashFlow / (data.length || 1);

  // No data state
  if (!data || data.length === 0) {
    return (
      <div className="h-64 w-full flex items-center justify-center text-muted-foreground">
        <div className="text-center">
          <p className="text-sm">Nessun dato cash flow disponibile</p>
          <p className="text-xs mt-1">Importa movimenti bancari per visualizzare il cash flow</p>
        </div>
      </div>
    );
  }

  return (
    <div className="space-y-4">
      {/* Summary Stats */}
      <div className="grid grid-cols-2 lg:grid-cols-4 gap-4">
        <div className="bg-green-50 dark:bg-green-950 rounded-lg p-3 text-center">
          <p className="text-xs text-green-600 dark:text-green-400">Entrate Totali</p>
          <p className="text-lg font-bold text-green-600 dark:text-green-400">
            {formatCurrencyCompact(totalInflows)}
          </p>
        </div>
        <div className="bg-red-50 dark:bg-red-950 rounded-lg p-3 text-center">
          <p className="text-xs text-red-600 dark:text-red-400">Uscite Totali</p>
          <p className="text-lg font-bold text-red-600 dark:text-red-400">
            {formatCurrencyCompact(totalOutflows)}
          </p>
        </div>
        <div className={`rounded-lg p-3 text-center ${
          netCashFlow >= 0 
            ? 'bg-blue-50 dark:bg-blue-950' 
            : 'bg-orange-50 dark:bg-orange-950'
        }`}>
          <p className={`text-xs ${
            netCashFlow >= 0 
              ? 'text-blue-600 dark:text-blue-400' 
              : 'text-orange-600 dark:text-orange-400'
          }`}>
            Cash Flow Netto
          </p>
          <p className={`text-lg font-bold ${
            netCashFlow >= 0 
              ? 'text-blue-600 dark:text-blue-400' 
              : 'text-orange-600 dark:text-orange-400'
          }`}>
            {formatCurrencyCompact(netCashFlow)}
          </p>
        </div>
        <div className="bg-muted/30 rounded-lg p-3 text-center">
          <p className="text-xs text-muted-foreground">Media Mensile</p>
          <p className={`text-lg font-bold ${
            avgMonthlyFlow >= 0 ? 'text-primary' : 'text-destructive'
          }`}>
            {formatCurrencyCompact(avgMonthlyFlow)}
          </p>
        </div>
      </div>

      {/* Chart */}
      <div className="h-64 w-full">
        <ResponsiveContainer width="100%" height="100%">
          <BarChart 
            data={chartData} 
            margin={{ top: 20, right: 30, left: 20, bottom: 5 }}
          >
            <CartesianGrid strokeDasharray="3 3" className="opacity-30" />
            <XAxis 
              dataKey="month" 
              tick={{ fontSize: 12 }}
              axisLine={false}
              tickLine={false}
            />
            <YAxis 
              tick={{ fontSize: 12 }}
              axisLine={false}
              tickLine={false}
              tickFormatter={(value) => formatCurrencyCompact(value)}
            />
            <Tooltip content={<CustomTooltip />} />
            
            {/* Zero reference line */}
            <ReferenceLine y={0} stroke="hsl(var(--muted-foreground))" strokeDasharray="2 2" />
            
            {/* Inflows (positive) */}
            <Bar
              dataKey="total_inflows"
              fill="#10b981"
              name="Entrate"
              radius={[2, 2, 0, 0]}
            />
            
            {/* Outflows (negative) */}
            <Bar
              dataKey="total_outflows_negative"
              fill="#ef4444"
              name="Uscite"
              radius={[0, 0, 2, 2]}
            />
          </BarChart>
        </ResponsiveContainer>
      </div>

      {/* Legend & Insights */}
      <div className="flex flex-col lg:flex-row justify-between items-start lg:items-center space-y-3 lg:space-y-0">
        {/* Legend */}
        <div className="flex space-x-6 text-sm">
          <div className="flex items-center space-x-2">
            <div className="w-3 h-3 rounded bg-green-500" />
            <span>Entrate</span>
          </div>
          <div className="flex items-center space-x-2">
            <div className="w-3 h-3 rounded bg-red-500" />
            <span>Uscite</span>
          </div>
        </div>

        {/* Quick Insights */}
        <div className="flex space-x-4 text-xs text-muted-foreground">
          {netCashFlow > 0 ? (
            <div className="flex items-center space-x-1">
              <div className="w-2 h-2 rounded-full bg-green-500" />
              <span>Cash flow positivo</span>
            </div>
          ) : (
            <div className="flex items-center space-x-1">
              <div className="w-2 h-2 rounded-full bg-red-500" />
              <span>Cash flow negativo</span>
            </div>
          )}
          
          {/* Trend analysis */}
          {data.length >= 2 && (
            <div className="flex items-center space-x-1">
              {data[data.length - 1].net_cash_flow > data[data.length - 2].net_cash_flow ? (
                <>
                  <div className="w-2 h-2 rounded-full bg-blue-500" />
                  <span>Trend crescente</span>
                </>
              ) : (
                <>
                  <div className="w-2 h-2 rounded-full bg-orange-500" />
                  <span>Trend decrescente</span>
                </>
              )}
            </div>
          )}
        </div>
      </div>
    </div>
  );
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/dashboard/CashFlowChart.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/dashboard/Charts.tsx ---
// Path: frontend/src/components/dashboard/Charts.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/dashboard/Charts.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/dashboard/DashboardView.tsx ---
// Path: frontend/src/components/dashboard/DashboardView.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/dashboard/DashboardView.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/dashboard/KPICards.tsx ---
import React from 'react';
import {
  TrendingUp,
  TrendingDown,
  DollarSign,
  CreditCard,
  AlertTriangle,
  Users,
  Calendar,
  Target,
} from 'lucide-react';

// Components
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';

// Types
import type { DashboardKPIs } from '@/types';

// Utils
import { formatCurrency, formatPercentage } from '@/lib/formatters';
import { cn } from '@/lib/utils';

interface KPICardsProps {
  data: DashboardKPIs;
}

interface KPICardData {
  title: string;
  value: string;
  description?: string;
  icon: React.ComponentType<{ className?: string }>;
  trend?: {
    value: number;
    isPositive: boolean;
    label: string;
  };
  variant?: 'default' | 'success' | 'warning' | 'destructive';
}

export function KPICards({ data }: KPICardsProps) {
  const kpiCards: KPICardData[] = [
    {
      title: 'Crediti Totali',
      value: formatCurrency(data.total_receivables),
      description: `${data.overdue_receivables_count} fatture scadute`,
      icon: DollarSign,
      variant: data.overdue_receivables_amount > 0 ? 'warning' : 'default',
    },
    {
      title: 'Fatturato YTD',
      value: formatCurrency(data.revenue_ytd),
      description: 'Anno corrente',
      icon: TrendingUp,
      trend: data.revenue_yoy_change_ytd !== undefined ? {
        value: data.revenue_yoy_change_ytd,
        isPositive: data.revenue_yoy_change_ytd > 0,
        label: 'vs anno precedente',
      } : undefined,
      variant: data.revenue_yoy_change_ytd && data.revenue_yoy_change_ytd > 0 ? 'success' : 'default',
    },
    {
      title: 'Margine Lordo',
      value: formatCurrency(data.gross_margin_ytd),
      description: data.margin_percent_ytd ? `${formatPercentage(data.margin_percent_ytd)} del fatturato` : undefined,
      icon: Target,
      variant: data.margin_percent_ytd && data.margin_percent_ytd > 20 ? 'success' : 'default',
    },
    {
      title: 'Clienti Attivi',
      value: data.active_customers_month.toString(),
      description: `+${data.new_customers_month} nuovi questo mese`,
      icon: Users,
      variant: data.new_customers_month > 0 ? 'success' : 'default',
    },
  ];

  // Se ci sono importi scaduti significativi, aggiungi una card specifica
  if (data.overdue_receivables_amount > 1000) {
    kpiCards.push({
      title: 'Crediti Scaduti',
      value: formatCurrency(data.overdue_receivables_amount),
      description: `${data.overdue_receivables_count} fatture da sollecitare`,
      icon: AlertTriangle,
      variant: 'destructive',
    });
  }

  // Se abbiamo dati sui tempi di pagamento, aggiungi una card
  if (data.avg_days_to_payment !== undefined) {
    kpiCards.push({
      title: 'Giorni Medi Incasso',
      value: `${Math.round(data.avg_days_to_payment)}gg`,
      description: 'Tempo medio di pagamento',
      icon: Calendar,
      variant: data.avg_days_to_payment > 60 ? 'warning' : 'success',
    });
  }

  return (
    <div className="grid gap-4 md:grid-cols-2 lg:grid-cols-4">
      {kpiCards.slice(0, 4).map((card, index) => (
        <KPICard key={index} {...card} />
      ))}
    </div>
  );
}

function KPICard({ title, value, description, icon: Icon, trend, variant = 'default' }: KPICardData) {
  return (
    <Card className={cn(
      "transition-all hover:shadow-md",
      variant === 'success' && "border-green-200 bg-green-50 dark:border-green-800 dark:bg-green-950",
      variant === 'warning' && "border-yellow-200 bg-yellow-50 dark:border-yellow-800 dark:bg-yellow-950",
      variant === 'destructive' && "border-red-200 bg-red-50 dark:border-red-800 dark:bg-red-950"
    )}>
      <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
        <CardTitle className="text-sm font-medium text-muted-foreground">
          {title}
        </CardTitle>
        <Icon className={cn(
          "h-4 w-4",
          variant === 'success' && "text-green-600",
          variant === 'warning' && "text-yellow-600",
          variant === 'destructive' && "text-red-600",
          variant === 'default' && "text-muted-foreground"
        )} />
      </CardHeader>
      <CardContent>
        <div className="text-2xl font-bold">{value}</div>
        
        <div className="flex items-center justify-between mt-2">
          <div className="text-xs text-muted-foreground">
            {description}
          </div>
          
          {trend && (
            <Badge 
              variant={trend.isPositive ? "default" : "secondary"}
              className={cn(
                "text-xs",
                trend.isPositive 
                  ? "bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200" 
                  : "bg-red-100 text-red-800 dark:bg-red-900 dark:text-red-200"
              )}
            >
              {trend.isPositive ? (
                <TrendingUp className="w-3 h-3 mr-1" />
              ) : (
                <TrendingDown className="w-3 h-3 mr-1" />
              )}
              {formatPercentage(Math.abs(trend.value))}
            </Badge>
          )}
        </div>
        
        {trend && (
          <p className="text-xs text-muted-foreground mt-1">
            {trend.label}
          </p>
        )}
      </CardContent>
    </Card>
  );
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/dashboard/KPICards.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/dashboard/OverdueInvoices.tsx ---
// Path: frontend/src/components/dashboard/OverdueInvoices.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/dashboard/OverdueInvoices.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/dashboard/RecentActivity.tsx ---
// Path: frontend/src/components/dashboard/RecentActivity.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/dashboard/RecentActivity.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/dashboard/RevenueChart.tsx ---
import React from 'react';
import {
  LineChart,
  Line,
  XAxis,
  YAxis,
  CartesianGrid,
  Tooltip,
  ResponsiveContainer,
  AreaChart,
  Area,
} from 'recharts';
import { useQuery } from '@tanstack/react-query';

// API & Utils
import { apiClient } from '@/services/api';
import { formatCurrency, formatCurrencyCompact } from '@/lib/formatters';

// Types
interface RevenueData {
  month: string;
  revenue: number;
  cost: number;
  gross_margin: number;
  margin_percent: number;
}

interface RevenueChartProps {
  months?: number;
  variant?: 'line' | 'area';
  showCosts?: boolean;
}

export function RevenueChart({ 
  months = 12, 
  variant = 'area',
  showCosts = true 
}: RevenueChartProps) {
  const { data: revenueData, isLoading, error } = useQuery({
    queryKey: ['revenue-trends', months],
    queryFn: async () => {
      const response = await apiClient.getMonthlyRevenue(months, 'Attiva');
      if (response.success && response.data) {
        return response.data as RevenueData[];
      }
      throw new Error(response.message || 'Errore nel caricamento dati fatturato');
    },
    staleTime: 5 * 60 * 1000, // 5 minutes
  });

  // Custom tooltip
  const CustomTooltip = ({ active, payload, label }: any) => {
    if (active && payload && payload.length) {
      return (
        <div className="bg-background border border-border rounded-lg shadow-lg p-3">
          <p className="font-medium text-foreground mb-2">{label}</p>
          {payload.map((entry: any, index: number) => (
            <div key={index} className="flex items-center space-x-2 text-sm">
              <div 
                className="w-3 h-3 rounded-full" 
                style={{ backgroundColor: entry.color }}
              />
              <span className="text-muted-foreground">{entry.dataKey === 'revenue' ? 'Fatturato' : entry.dataKey === 'cost' ? 'Costi' : 'Margine'}:</span>
              <span className="font-medium text-foreground">
                {formatCurrency(entry.value)}
              </span>
              {entry.dataKey === 'gross_margin' && entry.payload.margin_percent && (
                <span className="text-muted-foreground text-xs">
                  ({entry.payload.margin_percent.toFixed(1)}%)
                </span>
              )}
            </div>
          ))}
        </div>
      );
    }
    return null;
  };

  // Loading state
  if (isLoading) {
    return (
      <div className="h-64 w-full flex items-center justify-center">
        <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-primary"></div>
      </div>
    );
  }

  // Error state
  if (error || !revenueData) {
    return (
      <div className="h-64 w-full flex items-center justify-center text-muted-foreground">
        <div className="text-center">
          <p className="text-sm">Errore nel caricamento dei dati</p>
          <p className="text-xs mt-1">
            {error instanceof Error ? error.message : 'Errore sconosciuto'}
          </p>
        </div>
      </div>
    );
  }

  // No data state
  if (!revenueData || revenueData.length === 0) {
    return (
      <div className="h-64 w-full flex items-center justify-center text-muted-foreground">
        <div className="text-center">
          <p className="text-sm">Nessun dato disponibile</p>
          <p className="text-xs mt-1">Non ci sono dati di fatturato per il periodo selezionato</p>
        </div>
      </div>
    );
  }

  // Process data for chart
  const chartData = revenueData.map(item => ({
    ...item,
    month: new Date(item.month + '-01').toLocaleDateString('it-IT', { 
      month: 'short', 
      year: '2-digit' 
    }),
  }));

  // Calculate totals for summary
  const totalRevenue = revenueData.reduce((sum, item) => sum + item.revenue, 0);
  const totalCosts = revenueData.reduce((sum, item) => sum + item.cost, 0);
  const totalMargin = totalRevenue - totalCosts;
  const avgMarginPercent = revenueData.reduce((sum, item) => sum + item.margin_percent, 0) / revenueData.length;

  const Chart = variant === 'area' ? AreaChart : LineChart;

  return (
    <div className="space-y-4">
      {/* Summary Stats */}
      <div className="grid grid-cols-2 lg:grid-cols-4 gap-4 text-center">
        <div className="bg-muted/30 rounded-lg p-3">
          <p className="text-xs text-muted-foreground">Fatturato Totale</p>
          <p className="text-lg font-bold text-green-600">{formatCurrencyCompact(totalRevenue)}</p>
        </div>
        {showCosts && (
          <div className="bg-muted/30 rounded-lg p-3">
            <p className="text-xs text-muted-foreground">Costi Totali</p>
            <p className="text-lg font-bold text-red-600">{formatCurrencyCompact(totalCosts)}</p>
          </div>
        )}
        <div className="bg-muted/30 rounded-lg p-3">
          <p className="text-xs text-muted-foreground">Margine Lordo</p>
          <p className="text-lg font-bold text-primary">{formatCurrencyCompact(totalMargin)}</p>
        </div>
        <div className="bg-muted/30 rounded-lg p-3">
          <p className="text-xs text-muted-foreground">% Margine Medio</p>
          <p className="text-lg font-bold text-blue-600">{avgMarginPercent.toFixed(1)}%</p>
        </div>
      </div>

      {/* Chart */}
      <div className="h-64 w-full">
        <ResponsiveContainer width="100%" height="100%">
          <Chart data={chartData} margin={{ top: 5, right: 30, left: 20, bottom: 5 }}>
            <CartesianGrid strokeDasharray="3 3" className="opacity-30" />
            <XAxis 
              dataKey="month" 
              tick={{ fontSize: 12 }}
              axisLine={false}
              tickLine={false}
            />
            <YAxis 
              tick={{ fontSize: 12 }}
              axisLine={false}
              tickLine={false}
              tickFormatter={(value) => formatCurrencyCompact(value)}
            />
            <Tooltip content={<CustomTooltip />} />
            
            {variant === 'area' ? (
              <>
                <defs>
                  <linearGradient id="revenueGradient" x1="0" y1="0" x2="0" y2="1">
                    <stop offset="5%" stopColor="hsl(var(--primary))" stopOpacity={0.3}/>
                    <stop offset="95%" stopColor="hsl(var(--primary))" stopOpacity={0}/>
                  </linearGradient>
                  {showCosts && (
                    <linearGradient id="costGradient" x1="0" y1="0" x2="0" y2="1">
                      <stop offset="5%" stopColor="#ef4444" stopOpacity={0.3}/>
                      <stop offset="95%" stopColor="#ef4444" stopOpacity={0}/>
                    </linearGradient>
                  )}
                  <linearGradient id="marginGradient" x1="0" y1="0" x2="0" y2="1">
                    <stop offset="5%" stopColor="#10b981" stopOpacity={0.3}/>
                    <stop offset="95%" stopColor="#10b981" stopOpacity={0}/>
                  </linearGradient>
                </defs>
                
                <Area
                  type="monotone"
                  dataKey="revenue"
                  stroke="hsl(var(--primary))"
                  strokeWidth={2}
                  fill="url(#revenueGradient)"
                  name="Fatturato"
                />
                {showCosts && (
                  <Area
                    type="monotone"
                    dataKey="cost"
                    stroke="#ef4444"
                    strokeWidth={2}
                    fill="url(#costGradient)"
                    name="Costi"
                  />
                )}
                <Area
                  type="monotone"
                  dataKey="gross_margin"
                  stroke="#10b981"
                  strokeWidth={2}
                  fill="url(#marginGradient)"
                  name="Margine"
                />
              </>
            ) : (
              <>
                <Line
                  type="monotone"
                  dataKey="revenue"
                  stroke="hsl(var(--primary))"
                  strokeWidth={3}
                  dot={{ r: 4 }}
                  activeDot={{ r: 6 }}
                  name="Fatturato"
                />
                {showCosts && (
                  <Line
                    type="monotone"
                    dataKey="cost"
                    stroke="#ef4444"
                    strokeWidth={2}
                    dot={{ r: 3 }}
                    activeDot={{ r: 5 }}
                    name="Costi"
                  />
                )}
                <Line
                  type="monotone"
                  dataKey="gross_margin"
                  stroke="#10b981"
                  strokeWidth={2}
                  dot={{ r: 3 }}
                  activeDot={{ r: 5 }}
                  name="Margine"
                />
              </>
            )}
          </Chart>
        </ResponsiveContainer>
      </div>

      {/* Legend */}
      <div className="flex justify-center space-x-6 text-sm">
        <div className="flex items-center space-x-2">
          <div className="w-3 h-3 rounded-full bg-primary" />
          <span>Fatturato</span>
        </div>
        {showCosts && (
          <div className="flex items-center space-x-2">
            <div className="w-3 h-3 rounded-full bg-red-500" />
            <span>Costi</span>
          </div>
        )}
        <div className="flex items-center space-x-2">
          <div className="w-3 h-3 rounded-full bg-green-500" />
          <span>Margine Lordo</span>
        </div>
      </div>
    </div>
  );
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/dashboard/RevenueChart.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/dashboard/TopClientsTable.tsx ---
import React from 'react';
import { Crown, TrendingUp, Calendar, Star } from 'lucide-react';

// Components
import { Badge } from '@/components/ui/badge';
import { Button } from '@/components/ui/button';

// Utils
import { formatCurrency, formatCurrencyCompact, formatDate, formatScore } from '@/lib/formatters';
import { cn } from '@/lib/utils';

// Types
import type { TopClientData } from '@/types';

interface TopClientsTableProps {
  data: TopClientData[];
  maxItems?: number;
  showActions?: boolean;
}

export function TopClientsTable({ 
  data, 
  maxItems = 10, 
  showActions = false 
}: TopClientsTableProps) {
  // Ordina per fatturato e limita
  const sortedClients = data
    .sort((a, b) => b.total_revenue - a.total_revenue)
    .slice(0, maxItems);

  if (!sortedClients || sortedClients.length === 0) {
    return (
      <div className="text-center py-8">
        <div className="text-muted-foreground">
          <Crown className="h-8 w-8 mx-auto mb-2 opacity-50" />
          <p className="text-sm">Nessun cliente trovato</p>
          <p className="text-xs mt-1">Importa fatture per visualizzare i top clienti</p>
        </div>
      </div>
    );
  }

  // Calcola totali per percentuali
  const totalRevenue = sortedClients.reduce((sum, client) => sum + client.total_revenue, 0);

  return (
    <div className="space-y-4">
      {/* Header con statistiche veloci */}
      <div className="flex items-center justify-between">
        <div className="text-sm text-muted-foreground">
          Top {sortedClients.length} clienti per fatturato
        </div>
        <div className="text-xs text-muted-foreground">
          Totale: {formatCurrencyCompact(totalRevenue)}
        </div>
      </div>

      {/* Lista clienti */}
      <div className="space-y-3">
        {sortedClients.map((client, index) => {
          const revenuePercentage = totalRevenue > 0 ? (client.total_revenue / totalRevenue) * 100 : 0;
          const scoreData = formatScore(client.score);
          
          return (
            <div
              key={client.id}
              className={cn(
                "flex items-center space-x-4 p-3 rounded-lg border transition-colors hover:bg-accent/50",
                index === 0 && "bg-yellow-50 border-yellow-200 dark:bg-yellow-950/20 dark:border-yellow-800",
                index === 1 && "bg-gray-50 border-gray-200 dark:bg-gray-950/20 dark:border-gray-800",
                index === 2 && "bg-orange-50 border-orange-200 dark:bg-orange-950/20 dark:border-orange-800"
              )}
            >
              {/* Posizione e Badge */}
              <div className="flex items-center space-x-2 min-w-0">
                <div className={cn(
                  "flex items-center justify-center w-6 h-6 rounded-full text-xs font-bold",
                  index === 0 && "bg-yellow-500 text-white",
                  index === 1 && "bg-gray-400 text-white", 
                  index === 2 && "bg-orange-500 text-white",
                  index > 2 && "bg-muted text-muted-foreground"
                )}>
                  {index === 0 ? <Crown className="w-3 h-3" /> : index + 1}
                </div>
                
                {/* Badges speciali */}
                {index === 0 && (
                  <Badge variant="warning" className="text-xs">🏆 Top</Badge>
                )}
                {client.score >= 90 && (
                  <Badge variant="success" className="text-xs">⭐ VIP</Badge>
                )}
              </div>

              {/* Info Cliente */}
              <div className="flex-1 min-w-0">
                <div className="flex items-start justify-between">
                  <div className="min-w-0 flex-1">
                    <p className="font-medium text-sm truncate" title={client.denomination}>
                      {client.denomination}
                    </p>
                    <div className="flex items-center space-x-3 mt-1">
                      <div className="flex items-center space-x-1 text-xs text-muted-foreground">
                        <TrendingUp className="w-3 h-3" />
                        <span>{client.num_invoices} fatture</span>
                      </div>
                      <div className="flex items-center space-x-1 text-xs text-muted-foreground">
                        <Star className="w-3 h-3" />
                        <span className={scoreData.variant === 'success' ? 'text-green-600' : 
                                       scoreData.variant === 'warning' ? 'text-yellow-600' : 'text-red-600'}>
                          {scoreData.text}
                        </span>
                      </div>
                      {client.last_order_date && (
                        <div className="flex items-center space-x-1 text-xs text-muted-foreground">
                          <Calendar className="w-3 h-3" />
                          <span>Ultimo: {formatDate(client.last_order_date)}</span>
                        </div>
                      )}
                    </div>
                  </div>

                  {/* Valori Revenue */}
                  <div className="text-right ml-3">
                    <p className="font-bold text-sm">
                      {formatCurrency(client.total_revenue)}
                    </p>
                    <div className="flex items-center space-x-2 mt-1">
                      <p className="text-xs text-muted-foreground">
                        {revenuePercentage.toFixed(1)}%
                      </p>
                      <p className="text-xs text-muted-foreground">
                        Avg: {formatCurrencyCompact(client.avg_order_value)}
                      </p>
                    </div>
                  </div>
                </div>

                {/* Progress Bar per Revenue Percentage */}
                <div className="mt-2">
                  <div className="w-full bg-muted rounded-full h-1.5">
                    <div
                      className={cn(
                        "h-1.5 rounded-full transition-all duration-300",
                        index === 0 && "bg-yellow-500",
                        index === 1 && "bg-gray-400",
                        index === 2 && "bg-orange-500",
                        index > 2 && "bg-primary"
                      )}
                      style={{ width: `${Math.min(revenuePercentage, 100)}%` }}
                    />
                  </div>
                </div>
              </div>

              {/* Actions */}
              {showActions && (
                <div className="flex items-center space-x-1">
                  <Button 
                    variant="ghost" 
                    size="sm"
                    className="h-8 w-8 p-0"
                    title="Vedi dettagli cliente"
                  >
                    👁️
                  </Button>
                  <Button 
                    variant="ghost" 
                    size="sm"
                    className="h-8 w-8 p-0"
                    title="Nuova fattura"
                  >
                    ➕
                  </Button>
                </div>
              )}
            </div>
          );
        })}
      </div>

      {/* Footer con insights */}
      {sortedClients.length >= 3 && (
        <div className="bg-muted/30 rounded-lg p-3 text-center">
          <div className="grid grid-cols-2 gap-4 text-xs">
            <div>
              <p className="text-muted-foreground">Top 3 clienti</p>
              <p className="font-medium">
                {((sortedClients.slice(0, 3).reduce((sum, c) => sum + c.total_revenue, 0) / totalRevenue) * 100).toFixed(1)}%
              </p>
              <p className="text-muted-foreground">del fatturato</p>
            </div>
            <div>
              <p className="text-muted-foreground">Valore medio ordine</p>
              <p className="font-medium">
                {formatCurrencyCompact(
                  sortedClients.reduce((sum, c) => sum + c.avg_order_value, 0) / sortedClients.length
                )}
              </p>
              <p className="text-muted-foreground">per cliente</p>
            </div>
          </div>
        </div>
      )}

      {/* Link per vedere tutti */}
      {data.length > maxItems && (
        <div className="text-center">
          <Button variant="outline" size="sm">
            Vedi tutti i {data.length} clienti
          </Button>
        </div>
      )}
    </div>
  );
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/dashboard/TopClientsTable.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/invoices/InvoiceDetail.tsx ---
// Path: frontend/src/components/invoices/InvoiceDetail.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/invoices/InvoiceDetail.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/invoices/InvoiceForm.tsx ---
// Path: frontend/src/components/invoices/InvoiceForm.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/invoices/InvoiceForm.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/invoices/InvoiceList.tsx ---
// Path: frontend/src/components/invoices/InvoiceList.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/invoices/InvoiceList.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/layout/Header.tsx ---
import React from 'react';
import { Bell, Search, Settings, User, Moon, Sun, Monitor } from 'lucide-react';
import { useLocation } from 'react-router-dom';

// UI Components
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuLabel,
  DropdownMenuSeparator,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu';
import { Badge } from '@/components/ui/badge';
import {
  Tooltip,
  TooltipContent,
  TooltipProvider,
  TooltipTrigger,
} from '@/components/ui/tooltip';

// Store
import { useUIStore, useNotifications } from '@/store';
import type { Theme } from '@/types';

// Utils
import { cn } from '@/lib/utils';

const pageNames: Record<string, string> = {
  '/dashboard': 'Dashboard',
  '/invoices': 'Fatture',
  '/transactions': 'Movimenti Bancari',
  '/reconciliation': 'Riconciliazione',
  '/anagraphics': 'Anagrafiche',
  '/analytics': 'Analytics',
  '/import': 'Import/Export',
  '/settings': 'Impostazioni',
};

export function Header() {
  const location = useLocation();
  const { theme, setTheme } = useUIStore();
  const notifications = useNotifications();
  
  const currentPageName = pageNames[location.pathname] || 'FatturaAnalyzer';
  const unreadNotifications = notifications.filter(n => n.type === 'error' || n.type === 'warning').length;

  const handleThemeChange = (newTheme: Theme) => {
    setTheme(newTheme);
    // Apply theme immediately
    const root = window.document.documentElement;
    root.classList.remove('light', 'dark');
    
    if (newTheme === 'system') {
      const systemTheme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
      root.classList.add(systemTheme);
    } else {
      root.classList.add(newTheme);
    }
  };

  return (
    <header className="sticky top-0 z-40 w-full border-b bg-background/95 backdrop-blur supports-[backdrop-filter]:bg-background/60">
      <div className="flex h-16 items-center justify-between px-6">
        {/* Left section - Page title and search */}
        <div className="flex items-center space-x-4 flex-1">
          <div>
            <h1 className="text-xl font-semibold text-foreground">
              {currentPageName}
            </h1>
            <p className="text-sm text-muted-foreground">
              Gestione fatture e riconciliazioni
            </p>
          </div>
          
          <div className="hidden md:flex relative w-96">
            <Search className="absolute left-3 top-1/2 h-4 w-4 -translate-y-1/2 text-muted-foreground" />
            <Input
              placeholder="Cerca fatture, clienti, movimenti..."
              className="pl-10 bg-muted/50"
            />
          </div>
        </div>

        {/* Right section - Actions and user menu */}
        <div className="flex items-center space-x-2">
          {/* Search button for mobile */}
          <Button variant="ghost" size="icon" className="md:hidden">
            <Search className="h-4 w-4" />
          </Button>

          {/* Theme toggle */}
          <TooltipProvider>
            <DropdownMenu>
              <Tooltip>
                <TooltipTrigger asChild>
                  <DropdownMenuTrigger asChild>
                    <Button variant="ghost" size="icon">
                      {theme === 'light' && <Sun className="h-4 w-4" />}
                      {theme === 'dark' && <Moon className="h-4 w-4" />}
                      {theme === 'system' && <Monitor className="h-4 w-4" />}
                    </Button>
                  </DropdownMenuTrigger>
                </TooltipTrigger>
                <TooltipContent>
                  <p>Cambia tema</p>
                </TooltipContent>
              </Tooltip>
              
              <DropdownMenuContent align="end">
                <DropdownMenuLabel>Tema</DropdownMenuLabel>
                <DropdownMenuSeparator />
                <DropdownMenuItem
                  onClick={() => handleThemeChange('light')}
                  className={cn(theme === 'light' && 'bg-accent')}
                >
                  <Sun className="mr-2 h-4 w-4" />
                  Chiaro
                </DropdownMenuItem>
                <DropdownMenuItem
                  onClick={() => handleThemeChange('dark')}
                  className={cn(theme === 'dark' && 'bg-accent')}
                >
                  <Moon className="mr-2 h-4 w-4" />
                  Scuro
                </DropdownMenuItem>
                <DropdownMenuItem
                  onClick={() => handleThemeChange('system')}
                  className={cn(theme === 'system' && 'bg-accent')}
                >
                  <Monitor className="mr-2 h-4 w-4" />
                  Sistema
                </DropdownMenuItem>
              </DropdownMenuContent>
            </DropdownMenu>
          </TooltipProvider>

          {/* Notifications */}
          <TooltipProvider>
            <DropdownMenu>
              <Tooltip>
                <TooltipTrigger asChild>
                  <DropdownMenuTrigger asChild>
                    <Button variant="ghost" size="icon" className="relative">
                      <Bell className="h-4 w-4" />
                      {unreadNotifications > 0 && (
                        <Badge 
                          className="absolute -top-1 -right-1 h-5 w-5 rounded-full p-0 text-xs"
                          variant="destructive"
                        >
                          {unreadNotifications}
                        </Badge>
                      )}
                    </Button>
                  </DropdownMenuTrigger>
                </TooltipTrigger>
                <TooltipContent>
                  <p>Notifiche {unreadNotifications > 0 && `(${unreadNotifications})`}</p>
                </TooltipContent>
              </Tooltip>
              
              <DropdownMenuContent align="end" className="w-80">
                <DropdownMenuLabel>Notifiche</DropdownMenuLabel>
                <DropdownMenuSeparator />
                
                {notifications.length === 0 ? (
                  <div className="p-4 text-center text-sm text-muted-foreground">
                    Nessuna notifica
                  </div>
                ) : (
                  <div className="max-h-64 overflow-y-auto">
                    {notifications.slice(0, 5).map((notification) => (
                      <DropdownMenuItem
                        key={notification.id}
                        className="flex flex-col items-start p-3 cursor-default"
                      >
                        <div className="flex items-center space-x-2 w-full">
                          <div className={cn(
                            "h-2 w-2 rounded-full",
                            notification.type === 'error' && "bg-destructive",
                            notification.type === 'warning' && "bg-yellow-500",
                            notification.type === 'success' && "bg-green-500",
                            notification.type === 'info' && "bg-blue-500"
                          )} />
                          <span className="font-medium text-sm">{notification.title}</span>
                        </div>
                        {notification.message && (
                          <p className="text-xs text-muted-foreground mt-1">
                            {notification.message}
                          </p>
                        )}
                      </DropdownMenuItem>
                    ))}
                    
                    {notifications.length > 5 && (
                      <DropdownMenuItem className="text-center text-sm">
                        E altre {notifications.length - 5} notifiche...
                      </DropdownMenuItem>
                    )}
                  </div>
                )}
              </DropdownMenuContent>
            </DropdownMenu>
          </TooltipProvider>

          {/* Settings */}
          <TooltipProvider>
            <Tooltip>
              <TooltipTrigger asChild>
                <Button variant="ghost" size="icon" asChild>
                  <a href="/settings">
                    <Settings className="h-4 w-4" />
                  </a>
                </Button>
              </TooltipTrigger>
              <TooltipContent>
                <p>Impostazioni</p>
              </TooltipContent>
            </Tooltip>
          </TooltipProvider>

          {/* User menu */}
          <DropdownMenu>
            <DropdownMenuTrigger asChild>
              <Button variant="ghost" size="icon">
                <User className="h-4 w-4" />
              </Button>
            </DropdownMenuTrigger>
            <DropdownMenuContent align="end">
              <DropdownMenuLabel>Account</DropdownMenuLabel>
              <DropdownMenuSeparator />
              <DropdownMenuItem>
                <User className="mr-2 h-4 w-4" />
                Profilo
              </DropdownMenuItem>
              <DropdownMenuItem>
                <Settings className="mr-2 h-4 w-4" />
                Impostazioni
              </DropdownMenuItem>
              <DropdownMenuSeparator />
              <DropdownMenuItem className="text-destructive">
                Disconnetti
              </DropdownMenuItem>
            </DropdownMenuContent>
          </DropdownMenu>
        </div>
      </div>
    </header>
  );
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/layout/Header.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/layout/Layout.tsx ---
import React from 'react';
import { Outlet } from 'react-router-dom';
import { cn } from '@/lib/utils';

// Components
import { Header } from './Header';
import { Sidebar } from './Sidebar';

// Store
import { useUIStore } from '@/store';

export function Layout() {
  const sidebarCollapsed = useUIStore(state => state.sidebarCollapsed);
  
  return (
    <div className="min-h-screen bg-background">
      {/* Sidebar */}
      <Sidebar />
      
      {/* Main content area */}
      <div className={cn(
        "transition-all duration-300 ease-in-out",
        sidebarCollapsed ? "ml-16" : "ml-64"
      )}>
        {/* Header */}
        <Header />
        
        {/* Page content */}
        <main className="p-6">
          <div className="mx-auto max-w-7xl">
            <Outlet />
          </div>
        </main>
      </div>
    </div>
  );
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/layout/Layout.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/layout/Sidebar.tsx ---
import React from 'react';
import { NavLink } from 'react-router-dom';
import {
  LayoutDashboard,
  FileText,
  CreditCard,
  GitMerge,
  Users,
  BarChart3,
  Upload,
  Settings,
  ChevronLeft,
  ChevronRight,
  Building2,
} from 'lucide-react';

// UI Components
import { Button } from '@/components/ui/button';
import { Badge } from '@/components/ui/badge';
import {
  Tooltip,
  TooltipContent,
  TooltipProvider,
  TooltipTrigger,
} from '@/components/ui/tooltip';

// Store
import { useUIStore } from '@/store';

// Utils
import { cn } from '@/lib/utils';

interface NavigationItem {
  title: string;
  href: string;
  icon: React.ComponentType<{ className?: string }>;
  badge?: string | number;
  description?: string;
}

const navigationItems: NavigationItem[] = [
  {
    title: 'Dashboard',
    href: '/dashboard',
    icon: LayoutDashboard,
    description: 'Panoramica generale',
  },
  {
    title: 'Fatture',
    href: '/invoices',
    icon: FileText,
    description: 'Gestione fatture attive e passive',
  },
  {
    title: 'Movimenti',
    href: '/transactions',
    icon: CreditCard,
    description: 'Movimenti bancari e transazioni',
  },
  {
    title: 'Riconciliazione',
    href: '/reconciliation',
    icon: GitMerge,
    description: 'Riconciliazione fatture e pagamenti',
    badge: 'NEW',
  },
  {
    title: 'Anagrafiche',
    href: '/anagraphics',
    icon: Users,
    description: 'Clienti e fornitori',
  },
  {
    title: 'Analytics',
    href: '/analytics',
    icon: BarChart3,
    description: 'Report e analisi dati',
  },
  {
    title: 'Import/Export',
    href: '/import',
    icon: Upload,
    description: 'Importazione ed esportazione dati',
  },
  {
    title: 'Impostazioni',
    href: '/settings',
    icon: Settings,
    description: 'Configurazione applicazione',
  },
];

export function Sidebar() {
  const { sidebarCollapsed, toggleSidebar } = useUIStore();

  return (
    <TooltipProvider>
      <aside className={cn(
        "fixed left-0 top-0 z-50 h-screen border-r bg-background transition-all duration-300 ease-in-out",
        sidebarCollapsed ? "w-16" : "w-64"
      )}>
        <div className="flex h-full flex-col">
          {/* Header */}
          <div className="flex h-16 items-center justify-between border-b px-4">
            {!sidebarCollapsed && (
              <div className="flex items-center space-x-3">
                <div className="flex h-8 w-8 items-center justify-center rounded-lg bg-primary text-primary-foreground">
                  <Building2 className="h-5 w-5" />
                </div>
                <div className="flex flex-col">
                  <span className="text-sm font-semibold">FatturaAnalyzer</span>
                  <span className="text-xs text-muted-foreground">v2.0</span>
                </div>
              </div>
            )}
            
            <Button
              variant="ghost"
              size="icon"
              onClick={toggleSidebar}
              className={cn(
                "h-8 w-8",
                sidebarCollapsed && "mx-auto"
              )}
            >
              {sidebarCollapsed ? (
                <ChevronRight className="h-4 w-4" />
              ) : (
                <ChevronLeft className="h-4 w-4" />
              )}
            </Button>
          </div>

          {/* Navigation */}
          <nav className="flex-1 space-y-1 p-2">
            {navigationItems.map((item) => {
              const Icon = item.icon;
              
              if (sidebarCollapsed) {
                return (
                  <Tooltip key={item.href} delayDuration={0}>
                    <TooltipTrigger asChild>
                      <NavLink
                        to={item.href}
                        className={({ isActive }) =>
                          cn(
                            "flex h-10 w-10 items-center justify-center rounded-lg transition-colors hover:bg-accent hover:text-accent-foreground",
                            isActive
                              ? "bg-accent text-accent-foreground"
                              : "text-muted-foreground"
                          )
                        }
                      >
                        <Icon className="h-5 w-5" />
                        {item.badge && (
                          <Badge className="absolute -top-1 -right-1 h-4 w-4 rounded-full p-0 text-xs">
                            {typeof item.badge === 'string' ? item.badge.slice(0, 2) : item.badge}
                          </Badge>
                        )}
                      </NavLink>
                    </TooltipTrigger>
                    <TooltipContent side="right" className="ml-2">
                      <div className="text-center">
                        <p className="font-medium">{item.title}</p>
                        {item.description && (
                          <p className="text-xs text-muted-foreground">
                            {item.description}
                          </p>
                        )}
                      </div>
                    </TooltipContent>
                  </Tooltip>
                );
              }

              return (
                <NavLink
                  key={item.href}
                  to={item.href}
                  className={({ isActive }) =>
                    cn(
                      "flex items-center space-x-3 rounded-lg px-3 py-2 transition-colors hover:bg-accent hover:text-accent-foreground",
                      isActive
                        ? "bg-accent text-accent-foreground"
                        : "text-muted-foreground"
                    )
                  }
                >
                  <Icon className="h-5 w-5" />
                  <div className="flex-1">
                    <div className="flex items-center justify-between">
                      <span className="text-sm font-medium">{item.title}</span>
                      {item.badge && (
                        <Badge variant="secondary" className="text-xs">
                          {item.badge}
                        </Badge>
                      )}
                    </div>
                    {item.description && (
                      <p className="text-xs text-muted-foreground">
                        {item.description}
                      </p>
                    )}
                  </div>
                </NavLink>
              );
            })}
          </nav>

          {/* Footer */}
          <div className="border-t p-4">
            {!sidebarCollapsed ? (
              <div className="space-y-2">
                <div className="text-xs text-muted-foreground">
                  Stato Sistema
                </div>
                <div className="flex items-center space-x-2">
                  <div className="h-2 w-2 rounded-full bg-green-500" />
                  <span className="text-xs">Backend connesso</span>
                </div>
                <div className="flex items-center space-x-2">
                  <div className="h-2 w-2 rounded-full bg-blue-500" />
                  <span className="text-xs">Sincronizzazione attiva</span>
                </div>
              </div>
            ) : (
              <Tooltip>
                <TooltipTrigger asChild>
                  <div className="flex justify-center">
                    <div className="h-2 w-2 rounded-full bg-green-500" />
                  </div>
                </TooltipTrigger>
                <TooltipContent side="right">
                  <p>Sistema operativo</p>
                </TooltipContent>
              </Tooltip>
            )}
          </div>
        </div>
      </aside>
    </TooltipProvider>
  );
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/layout/Sidebar.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/reconciliation/DragDropReconciliation.tsx ---
// Path: frontend/src/components/reconciliation/DragDropReconciliation.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/reconciliation/DragDropReconciliation.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/reconciliation/MatchSuggestions.tsx ---
// Path: frontend/src/components/reconciliation/MatchSuggestions.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/reconciliation/MatchSuggestions.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/reconciliation/ReconciliationActions.tsx ---
// Path: frontend/src/components/reconciliation/ReconciliationActions.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/reconciliation/ReconciliationActions.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/reconciliation/ReconciliationView.tsx ---
// Path: frontend/src/components/reconciliation/ReconciliationView.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/reconciliation/ReconciliationView.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/transactions/TransactionImport.tsx ---
// Path: frontend/src/components/transactions/TransactionImport.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/transactions/TransactionImport.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/transactions/TransactionList.tsx ---
// Path: frontend/src/components/transactions/TransactionList.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/transactions/TransactionList.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/badge.tsx ---
import * as React from "react"
import { cva, type VariantProps } from "class-variance-authority"

import { cn } from "@/lib/utils"

const badgeVariants = cva(
  "inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2",
  {
    variants: {
      variant: {
        default:
          "border-transparent bg-primary text-primary-foreground hover:bg-primary/80",
        secondary:
          "border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80",
        destructive:
          "border-transparent bg-destructive text-destructive-foreground hover:bg-destructive/80",
        outline: "text-foreground",
        success: 
          "border-transparent bg-green-100 text-green-800 dark:bg-green-900/20 dark:text-green-400",
        warning: 
          "border-transparent bg-yellow-100 text-yellow-800 dark:bg-yellow-900/20 dark:text-yellow-400",
        info: 
          "border-transparent bg-blue-100 text-blue-800 dark:bg-blue-900/20 dark:text-blue-400",
      },
    },
    defaultVariants: {
      variant: "default",
    },
  }
)

export interface BadgeProps
  extends React.HTMLAttributes<HTMLDivElement>,
    VariantProps<typeof badgeVariants> {}

function Badge({ className, variant, ...props }: BadgeProps) {
  return (
    <div className={cn(badgeVariants({ variant }), className)} {...props} />
  )
}

export { Badge, badgeVariants }
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/badge.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/button.tsx ---
import * as React from "react"
import { Slot } from "@radix-ui/react-slot"
import { cva, type VariantProps } from "class-variance-authority"

import { cn } from "@/lib/utils"

const buttonVariants = cva(
  "inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50",
  {
    variants: {
      variant: {
        default: "bg-primary text-primary-foreground hover:bg-primary/90",
        destructive:
          "bg-destructive text-destructive-foreground hover:bg-destructive/90",
        outline:
          "border border-input bg-background hover:bg-accent hover:text-accent-foreground",
        secondary:
          "bg-secondary text-secondary-foreground hover:bg-secondary/80",
        ghost: "hover:bg-accent hover:text-accent-foreground",
        link: "text-primary underline-offset-4 hover:underline",
      },
      size: {
        default: "h-10 px-4 py-2",
        sm: "h-9 rounded-md px-3",
        lg: "h-11 rounded-md px-8",
        icon: "h-10 w-10",
      },
    },
    defaultVariants: {
      variant: "default",
      size: "default",
    },
  }
)

export interface ButtonProps
  extends React.ButtonHTMLAttributes<HTMLButtonElement>,
    VariantProps<typeof buttonVariants> {
  asChild?: boolean
}

const Button = React.forwardRef<HTMLButtonElement, ButtonProps>(
  ({ className, variant, size, asChild = false, ...props }, ref) => {
    const Comp = asChild ? Slot : "button"
    return (
      <Comp
        className={cn(buttonVariants({ variant, size, className }))}
        ref={ref}
        {...props}
      />
    )
  }
)
Button.displayName = "Button"

export { Button, buttonVariants }
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/button.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/card.tsx ---
import * as React from "react"

import { cn } from "@/lib/utils"

const Card = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn(
      "rounded-lg border bg-card text-card-foreground shadow-sm",
      className
    )}
    {...props}
  />
))
Card.displayName = "Card"

const CardHeader = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn("flex flex-col space-y-1.5 p-6", className)}
    {...props}
  />
))
CardHeader.displayName = "CardHeader"

const CardTitle = React.forwardRef<
  HTMLParagraphElement,
  React.HTMLAttributes<HTMLHeadingElement>
>(({ className, ...props }, ref) => (
  <h3
    ref={ref}
    className={cn(
      "text-2xl font-semibold leading-none tracking-tight",
      className
    )}
    {...props}
  />
))
CardTitle.displayName = "CardTitle"

const CardDescription = React.forwardRef<
  HTMLParagraphElement,
  React.HTMLAttributes<HTMLParagraphElement>
>(({ className, ...props }, ref) => (
  <p
    ref={ref}
    className={cn("text-sm text-muted-foreground", className)}
    {...props}
  />
))
CardDescription.displayName = "CardDescription"

const CardContent = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div ref={ref} className={cn("p-6 pt-0", className)} {...props} />
))
CardContent.displayName = "CardContent"

const CardFooter = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn("flex items-center p-6 pt-0", className)}
    {...props}
  />
))
CardFooter.displayName = "CardFooter"

export { Card, CardHeader, CardFooter, CardTitle, CardDescription, CardContent }
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/card.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/checkbox.tsx ---
import * as React from "react"
import * as CheckboxPrimitive from "@radix-ui/react-checkbox"
import { Check } from "lucide-react"

import { cn } from "@/lib/utils"

const Checkbox = React.forwardRef<
  React.ElementRef<typeof CheckboxPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof CheckboxPrimitive.Root>
>(({ className, ...props }, ref) => (
  <CheckboxPrimitive.Root
    ref={ref}
    className={cn(
      "peer h-4 w-4 shrink-0 rounded-sm border border-primary ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50 data-[state=checked]:bg-primary data-[state=checked]:text-primary-foreground",
      className
    )}
    {...props}
  >
    <CheckboxPrimitive.Indicator
      className={cn("flex items-center justify-center text-current")}
    >
      <Check className="h-4 w-4" />
    </CheckboxPrimitive.Indicator>
  </CheckboxPrimitive.Root>
))
Checkbox.displayName = CheckboxPrimitive.Root.displayName

export { Checkbox }
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/checkbox.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/dialog.tsx ---
import * as React from "react"
import * as DialogPrimitive from "@radix-ui/react-dialog"
import { X } from "lucide-react"

import { cn } from "@/lib/utils"

const Dialog = DialogPrimitive.Root

const DialogTrigger = DialogPrimitive.Trigger

const DialogPortal = DialogPrimitive.Portal

const DialogClose = DialogPrimitive.Close

const DialogOverlay = React.forwardRef<
  React.ElementRef<typeof DialogPrimitive.Overlay>,
  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Overlay>
>(({ className, ...props }, ref) => (
  <DialogPrimitive.Overlay
    ref={ref}
    className={cn(
      "fixed inset-0 z-50 bg-black/80  data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0",
      className
    )}
    {...props}
  />
))
DialogOverlay.displayName = DialogPrimitive.Overlay.displayName

const DialogContent = React.forwardRef<
  React.ElementRef<typeof DialogPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Content>
>(({ className, children, ...props }, ref) => (
  <DialogPortal>
    <DialogOverlay />
    <DialogPrimitive.Content
      ref={ref}
      className={cn(
        "fixed left-[50%] top-[50%] z-50 grid w-full max-w-lg translate-x-[-50%] translate-y-[-50%] gap-4 border bg-background p-6 shadow-lg duration-200 data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[state=closed]:slide-out-to-left-1/2 data-[state=closed]:slide-out-to-top-[48%] data-[state=open]:slide-in-from-left-1/2 data-[state=open]:slide-in-from-top-[48%] sm:rounded-lg",
        className
      )}
      {...props}
    >
      {children}
      <DialogPrimitive.Close className="absolute right-4 top-4 rounded-sm opacity-70 ring-offset-background transition-opacity hover:opacity-100 focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 disabled:pointer-events-none data-[state=open]:bg-accent data-[state=open]:text-muted-foreground">
        <X className="h-4 w-4" />
        <span className="sr-only">Close</span>
      </DialogPrimitive.Close>
    </DialogPrimitive.Content>
  </DialogPortal>
))
DialogContent.displayName = DialogPrimitive.Content.displayName

const DialogHeader = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      "flex flex-col space-y-1.5 text-center sm:text-left",
      className
    )}
    {...props}
  />
)
DialogHeader.displayName = "DialogHeader"

const DialogFooter = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      "flex flex-col-reverse sm:flex-row sm:justify-end sm:space-x-2",
      className
    )}
    {...props}
  />
)
DialogFooter.displayName = "DialogFooter"

const DialogTitle = React.forwardRef<
  React.ElementRef<typeof DialogPrimitive.Title>,
  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Title>
>(({ className, ...props }, ref) => (
  <DialogPrimitive.Title
    ref={ref}
    className={cn(
      "text-lg font-semibold leading-none tracking-tight",
      className
    )}
    {...props}
  />
))
DialogTitle.displayName = DialogPrimitive.Title.displayName

const DialogDescription = React.forwardRef<
  React.ElementRef<typeof DialogPrimitive.Description>,
  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Description>
>(({ className, ...props }, ref) => (
  <DialogPrimitive.Description
    ref={ref}
    className={cn("text-sm text-muted-foreground", className)}
    {...props}
  />
))
DialogDescription.displayName = DialogPrimitive.Description.displayName

export {
  Dialog,
  DialogPortal,
  DialogOverlay,
  DialogClose,
  DialogTrigger,
  DialogContent,
  DialogHeader,
  DialogFooter,
  DialogTitle,
  DialogDescription,
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/dialog.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/dropdown-menu.tsx ---
import * as React from "react"
import * as DropdownMenuPrimitive from "@radix-ui/react-dropdown-menu"
import { Check, ChevronRight, Circle } from "lucide-react"

import { cn } from "@/lib/utils"

const DropdownMenu = DropdownMenuPrimitive.Root

const DropdownMenuTrigger = DropdownMenuPrimitive.Trigger

const DropdownMenuGroup = DropdownMenuPrimitive.Group

const DropdownMenuPortal = DropdownMenuPrimitive.Portal

const DropdownMenuSub = DropdownMenuPrimitive.Sub

const DropdownMenuRadioGroup = DropdownMenuPrimitive.RadioGroup

const DropdownMenuSubTrigger = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.SubTrigger>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.SubTrigger> & {
    inset?: boolean
  }
>(({ className, inset, children, ...props }, ref) => (
  <DropdownMenuPrimitive.SubTrigger
    ref={ref}
    className={cn(
      "flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-sm outline-none focus:bg-accent data-[state=open]:bg-accent",
      inset && "pl-8",
      className
    )}
    {...props}
  >
    {children}
    <ChevronRight className="ml-auto h-4 w-4" />
  </DropdownMenuPrimitive.SubTrigger>
))
DropdownMenuSubTrigger.displayName =
  DropdownMenuPrimitive.SubTrigger.displayName

const DropdownMenuSubContent = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.SubContent>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.SubContent>
>(({ className, ...props }, ref) => (
  <DropdownMenuPrimitive.SubContent
    ref={ref}
    className={cn(
      "z-50 min-w-[8rem] overflow-hidden rounded-md border bg-popover p-1 text-popover-foreground shadow-lg data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
      className
    )}
    {...props}
  />
))
DropdownMenuSubContent.displayName =
  DropdownMenuPrimitive.SubContent.displayName

const DropdownMenuContent = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Content>
>(({ className, sideOffset = 4, ...props }, ref) => (
  <DropdownMenuPrimitive.Portal>
    <DropdownMenuPrimitive.Content
      ref={ref}
      sideOffset={sideOffset}
      className={cn(
        "z-50 min-w-[8rem] overflow-hidden rounded-md border bg-popover p-1 text-popover-foreground shadow-md data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        className
      )}
      {...props}
    />
  </DropdownMenuPrimitive.Portal>
))
DropdownMenuContent.displayName = DropdownMenuPrimitive.Content.displayName

const DropdownMenuItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Item> & {
    inset?: boolean
  }
>(({ className, inset, ...props }, ref) => (
  <DropdownMenuPrimitive.Item
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      inset && "pl-8",
      className
    )}
    {...props}
  />
))
DropdownMenuItem.displayName = DropdownMenuPrimitive.Item.displayName

const DropdownMenuCheckboxItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.CheckboxItem>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.CheckboxItem>
>(({ className, children, checked, ...props }, ref) => (
  <DropdownMenuPrimitive.CheckboxItem
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className
    )}
    checked={checked}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <DropdownMenuPrimitive.ItemIndicator>
        <Check className="h-4 w-4" />
      </DropdownMenuPrimitive.ItemIndicator>
    </span>
    {children}
  </DropdownMenuPrimitive.CheckboxItem>
))
DropdownMenuCheckboxItem.displayName =
  DropdownMenuPrimitive.CheckboxItem.displayName

const DropdownMenuRadioItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.RadioItem>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.RadioItem>
>(({ className, children, ...props }, ref) => (
  <DropdownMenuPrimitive.RadioItem
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className
    )}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <DropdownMenuPrimitive.ItemIndicator>
        <Circle className="h-2 w-2 fill-current" />
      </DropdownMenuPrimitive.ItemIndicator>
    </span>
    {children}
  </DropdownMenuPrimitive.RadioItem>
))
DropdownMenuRadioItem.displayName = DropdownMenuPrimitive.RadioItem.displayName

const DropdownMenuLabel = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Label>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Label> & {
    inset?: boolean
  }
>(({ className, inset, ...props }, ref) => (
  <DropdownMenuPrimitive.Label
    ref={ref}
    className={cn(
      "px-2 py-1.5 text-sm font-semibold",
      inset && "pl-8",
      className
    )}
    {...props}
  />
))
DropdownMenuLabel.displayName = DropdownMenuPrimitive.Label.displayName

const DropdownMenuSeparator = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <DropdownMenuPrimitive.Separator
    ref={ref}
    className={cn("-mx-1 my-1 h-px bg-muted", className)}
    {...props}
  />
))
DropdownMenuSeparator.displayName = DropdownMenuPrimitive.Separator.displayName

const DropdownMenuShortcut = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLSpanElement>) => {
  return (
    <span
      className={cn("ml-auto text-xs tracking-widest opacity-60", className)}
      {...props}
    />
  )
}
DropdownMenuShortcut.displayName = "DropdownMenuShortcut"

export {
  DropdownMenu,
  DropdownMenuTrigger,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuCheckboxItem,
  DropdownMenuRadioItem,
  DropdownMenuLabel,
  DropdownMenuSeparator,
  DropdownMenuShortcut,
  DropdownMenuGroup,
  DropdownMenuPortal,
  DropdownMenuSub,
  DropdownMenuSubContent,
  DropdownMenuSubTrigger,
  DropdownMenuRadioGroup,
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/dropdown-menu.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/index.ts ---
// shadcn/ui Components Export Index
// Tutti i componenti UI per facile importazione

export { Button, buttonVariants } from './button';
export type { ButtonProps } from './button';

export { 
  Card, 
  CardHeader, 
  CardFooter, 
  CardTitle, 
  CardDescription, 
  CardContent 
} from './card';

export { Input } from './input';
export type { InputProps } from './input';

export {
  Table,
  TableHeader,
  TableBody,
  TableFooter,
  TableHead,
  TableRow,
  TableCell,
  TableCaption,
} from './table';

export {
  Dialog,
  DialogPortal,
  DialogOverlay,
  DialogClose,
  DialogTrigger,
  DialogContent,
  DialogHeader,
  DialogFooter,
  DialogTitle,
  DialogDescription,
} from './dialog';

export { Badge, badgeVariants } from './badge';
export type { BadgeProps } from './badge';

export { Skeleton } from './skeleton';

export {
  DropdownMenu,
  DropdownMenuTrigger,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuCheckboxItem,
  DropdownMenuRadioItem,
  DropdownMenuLabel,
  DropdownMenuSeparator,
  DropdownMenuShortcut,
  DropdownMenuGroup,
  DropdownMenuPortal,
  DropdownMenuSub,
  DropdownMenuSubContent,
  DropdownMenuSubTrigger,
  DropdownMenuRadioGroup,
} from './dropdown-menu';

export { 
  Tooltip, 
  TooltipTrigger, 
  TooltipContent, 
  TooltipProvider 
} from './tooltip';

export {
  Select,
  SelectGroup,
  SelectValue,
  SelectTrigger,
  SelectContent,
  SelectLabel,
  SelectItem,
  SelectSeparator,
  SelectScrollUpButton,
  SelectScrollDownButton,
} from './select';

export { Label } from './label';

export { Checkbox } from './checkbox';

export { Toaster } from './sonner';
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/index.ts ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/input.tsx ---
import * as React from "react"

import { cn } from "@/lib/utils"

export interface InputProps
  extends React.InputHTMLAttributes<HTMLInputElement> {}

const Input = React.forwardRef<HTMLInputElement, InputProps>(
  ({ className, type, ...props }, ref) => {
    return (
      <input
        type={type}
        className={cn(
          "flex h-10 w-full rounded-md border border-input bg-background px-3 py-2 text-sm ring-offset-background file:border-0 file:bg-transparent file:text-sm file:font-medium placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50",
          className
        )}
        ref={ref}
        {...props}
      />
    )
  }
)
Input.displayName = "Input"

export { Input }
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/input.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/label.tsx ---
import * as React from "react"
import * as LabelPrimitive from "@radix-ui/react-label"
import { cva, type VariantProps } from "class-variance-authority"

import { cn } from "@/lib/utils"

const labelVariants = cva(
  "text-sm font-medium leading-none peer-disabled:cursor-not-allowed peer-disabled:opacity-70"
)

const Label = React.forwardRef<
  React.ElementRef<typeof LabelPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof LabelPrimitive.Root> &
    VariantProps<typeof labelVariants>
>(({ className, ...props }, ref) => (
  <LabelPrimitive.Root
    ref={ref}
    className={cn(labelVariants(), className)}
    {...props}
  />
))
Label.displayName = LabelPrimitive.Root.displayName

export { Label }
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/label.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/select.tsx ---
import * as React from "react"
import * as SelectPrimitive from "@radix-ui/react-select"
import { Check, ChevronDown, ChevronUp } from "lucide-react"

import { cn } from "@/lib/utils"

const Select = SelectPrimitive.Root

const SelectGroup = SelectPrimitive.Group

const SelectValue = SelectPrimitive.Value

const SelectTrigger = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Trigger>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Trigger>
>(({ className, children, ...props }, ref) => (
  <SelectPrimitive.Trigger
    ref={ref}
    className={cn(
      "flex h-10 w-full items-center justify-between rounded-md border border-input bg-background px-3 py-2 text-sm ring-offset-background placeholder:text-muted-foreground focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50 [&>span]:line-clamp-1",
      className
    )}
    {...props}
  >
    {children}
    <SelectPrimitive.Icon asChild>
      <ChevronDown className="h-4 w-4 opacity-50" />
    </SelectPrimitive.Icon>
  </SelectPrimitive.Trigger>
))
SelectTrigger.displayName = SelectPrimitive.Trigger.displayName

const SelectScrollUpButton = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.ScrollUpButton>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.ScrollUpButton>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.ScrollUpButton
    ref={ref}
    className={cn(
      "flex cursor-default items-center justify-center py-1",
      className
    )}
    {...props}
  >
    <ChevronUp className="h-4 w-4" />
  </SelectPrimitive.ScrollUpButton>
))
SelectScrollUpButton.displayName = SelectPrimitive.ScrollUpButton.displayName

const SelectScrollDownButton = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.ScrollDownButton>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.ScrollDownButton>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.ScrollDownButton
    ref={ref}
    className={cn(
      "flex cursor-default items-center justify-center py-1",
      className
    )}
    {...props}
  >
    <ChevronDown className="h-4 w-4" />
  </SelectPrimitive.ScrollDownButton>
))
SelectScrollDownButton.displayName =
  SelectPrimitive.ScrollDownButton.displayName

const SelectContent = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Content>
>(({ className, children, position = "popper", ...props }, ref) => (
  <SelectPrimitive.Portal>
    <SelectPrimitive.Content
      ref={ref}
      className={cn(
        "relative z-50 max-h-96 min-w-[8rem] overflow-hidden rounded-md border bg-popover text-popover-foreground shadow-md data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        position === "popper" &&
          "data-[side=bottom]:translate-y-1 data-[side=left]:-translate-x-1 data-[side=right]:translate-x-1 data-[side=top]:-translate-y-1",
        className
      )}
      position={position}
      {...props}
    >
      <SelectScrollUpButton />
      <SelectPrimitive.Viewport
        className={cn(
          "p-1",
          position === "popper" &&
            "h-[var(--radix-select-trigger-height)] w-full min-w-[var(--radix-select-trigger-width)]"
        )}
      >
        {children}
      </SelectPrimitive.Viewport>
      <SelectScrollDownButton />
    </SelectPrimitive.Content>
  </SelectPrimitive.Portal>
))
SelectContent.displayName = SelectPrimitive.Content.displayName

const SelectLabel = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Label>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Label>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.Label
    ref={ref}
    className={cn("py-1.5 pl-8 pr-2 text-sm font-semibold", className)}
    {...props}
  />
))
SelectLabel.displayName = SelectPrimitive.Label.displayName

const SelectItem = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Item>
>(({ className, children, ...props }, ref) => (
  <SelectPrimitive.Item
    ref={ref}
    className={cn(
      "relative flex w-full cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className
    )}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <SelectPrimitive.ItemIndicator>
        <Check className="h-4 w-4" />
      </SelectPrimitive.ItemIndicator>
    </span>

    <SelectPrimitive.ItemText>{children}</SelectPrimitive.ItemText>
  </SelectPrimitive.Item>
))
SelectItem.displayName = SelectPrimitive.Item.displayName

const SelectSeparator = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.Separator
    ref={ref}
    className={cn("-mx-1 my-1 h-px bg-muted", className)}
    {...props}
  />
))
SelectSeparator.displayName = SelectPrimitive.Separator.displayName

export {
  Select,
  SelectGroup,
  SelectValue,
  SelectTrigger,
  SelectContent,
  SelectLabel,
  SelectItem,
  SelectSeparator,
  SelectScrollUpButton,
  SelectScrollDownButton,
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/select.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/skeleton.tsx ---
import { cn } from "@/lib/utils"

function Skeleton({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) {
  return (
    <div
      className={cn("animate-pulse rounded-md bg-muted", className)}
      {...props}
    />
  )
}

export { Skeleton }
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/skeleton.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/sonner.tsx ---
import { useTheme } from "next-themes"
import { Toaster as Sonner } from "sonner"

type ToasterProps = React.ComponentProps<typeof Sonner>

const Toaster = ({ ...props }: ToasterProps) => {
  const { theme = "system" } = useTheme()

  return (
    <Sonner
      theme={theme as ToasterProps["theme"]}
      className="toaster group"
      toastOptions={{
        classNames: {
          toast:
            "group toast group-[.toaster]:bg-background group-[.toaster]:text-foreground group-[.toaster]:border-border group-[.toaster]:shadow-lg",
          description: "group-[.toast]:text-muted-foreground",
          actionButton:
            "group-[.toast]:bg-primary group-[.toast]:text-primary-foreground",
          cancelButton:
            "group-[.toast]:bg-muted group-[.toast]:text-muted-foreground",
        },
      }}
      {...props}
    />
  )
}

export { Toaster }
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/sonner.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/table.tsx ---
import * as React from "react"

import { cn } from "@/lib/utils"

const Table = React.forwardRef<
  HTMLTableElement,
  React.HTMLAttributes<HTMLTableElement>
>(({ className, ...props }, ref) => (
  <div className="relative w-full overflow-auto">
    <table
      ref={ref}
      className={cn("w-full caption-bottom text-sm", className)}
      {...props}
    />
  </div>
))
Table.displayName = "Table"

const TableHeader = React.forwardRef<
  HTMLTableSectionElement,
  React.HTMLAttributes<HTMLTableSectionElement>
>(({ className, ...props }, ref) => (
  <thead ref={ref} className={cn("[&_tr]:border-b", className)} {...props} />
))
TableHeader.displayName = "TableHeader"

const TableBody = React.forwardRef<
  HTMLTableSectionElement,
  React.HTMLAttributes<HTMLTableSectionElement>
>(({ className, ...props }, ref) => (
  <tbody
    ref={ref}
    className={cn("[&_tr:last-child]:border-0", className)}
    {...props}
  />
))
TableBody.displayName = "TableBody"

const TableFooter = React.forwardRef<
  HTMLTableSectionElement,
  React.HTMLAttributes<HTMLTableSectionElement>
>(({ className, ...props }, ref) => (
  <tfoot
    ref={ref}
    className={cn(
      "border-t bg-muted/50 font-medium [&>tr]:last:border-b-0",
      className
    )}
    {...props}
  />
))
TableFooter.displayName = "TableFooter"

const TableRow = React.forwardRef<
  HTMLTableRowElement,
  React.HTMLAttributes<HTMLTableRowElement>
>(({ className, ...props }, ref) => (
  <tr
    ref={ref}
    className={cn(
      "border-b transition-colors hover:bg-muted/50 data-[state=selected]:bg-muted",
      className
    )}
    {...props}
  />
))
TableRow.displayName = "TableRow"

const TableHead = React.forwardRef<
  HTMLTableCellElement,
  React.ThHTMLAttributes<HTMLTableCellElement>
>(({ className, ...props }, ref) => (
  <th
    ref={ref}
    className={cn(
      "h-12 px-4 text-left align-middle font-medium text-muted-foreground [&:has([role=checkbox])]:pr-0",
      className
    )}
    {...props}
  />
))
TableHead.displayName = "TableHead"

const TableCell = React.forwardRef<
  HTMLTableCellElement,
  React.TdHTMLAttributes<HTMLTableCellElement>
>(({ className, ...props }, ref) => (
  <td
    ref={ref}
    className={cn("p-4 align-middle [&:has([role=checkbox])]:pr-0", className)}
    {...props}
  />
))
TableCell.displayName = "TableCell"

const TableCaption = React.forwardRef<
  HTMLTableCaptionElement,
  React.HTMLAttributes<HTMLTableCaptionElement>
>(({ className, ...props }, ref) => (
  <caption
    ref={ref}
    className={cn("mt-4 text-sm text-muted-foreground", className)}
    {...props}
  />
))
TableCaption.displayName = "TableCaption"

export {
  Table,
  TableHeader,
  TableBody,
  TableFooter,
  TableHead,
  TableRow,
  TableCell,
  TableCaption,
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/table.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/tooltip.tsx ---
import * as React from "react"
import * as TooltipPrimitive from "@radix-ui/react-tooltip"

import { cn } from "@/lib/utils"

const TooltipProvider = TooltipPrimitive.Provider

const Tooltip = TooltipPrimitive.Root

const TooltipTrigger = TooltipPrimitive.Trigger

const TooltipContent = React.forwardRef<
  React.ElementRef<typeof TooltipPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof TooltipPrimitive.Content>
>(({ className, sideOffset = 4, ...props }, ref) => (
  <TooltipPrimitive.Content
    ref={ref}
    sideOffset={sideOffset}
    className={cn(
      "z-50 overflow-hidden rounded-md border bg-popover px-3 py-1.5 text-sm text-popover-foreground shadow-md animate-in fade-in-0 zoom-in-95 data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=closed]:zoom-out-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
      className
    )}
    {...props}
  />
))
TooltipContent.displayName = TooltipPrimitive.Content.displayName

export { Tooltip, TooltipTrigger, TooltipContent, TooltipProvider }
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/components/ui/tooltip.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/global.css ---
@tailwind base;
@tailwind components;
@tailwind utilities;

@layer base {
  :root {
    --background: 0 0% 100%;
    --foreground: 222.2 84% 4.9%;
    --card: 0 0% 100%;
    --card-foreground: 222.2 84% 4.9%;
    --popover: 0 0% 100%;
    --popover-foreground: 222.2 84% 4.9%;
    --primary: 221.2 83.2% 53.3%;
    --primary-foreground: 210 40% 98%;
    --secondary: 210 40% 96%;
    --secondary-foreground: 222.2 84% 4.9%;
    --muted: 210 40% 96%;
    --muted-foreground: 215.4 16.3% 46.9%;
    --accent: 210 40% 96%;
    --accent-foreground: 222.2 84% 4.9%;
    --destructive: 0 84.2% 60.2%;
    --destructive-foreground: 210 40% 98%;
    --border: 214.3 31.8% 91.4%;
    --input: 214.3 31.8% 91.4%;
    --ring: 221.2 83.2% 53.3%;
    --radius: 0.75rem;
  }

  .dark {
    --background: 222.2 84% 4.9%;
    --foreground: 210 40% 98%;
    --card: 222.2 84% 4.9%;
    --card-foreground: 210 40% 98%;
    --popover: 222.2 84% 4.9%;
    --popover-foreground: 210 40% 98%;
    --primary: 217.2 91.2% 59.8%;
    --primary-foreground: 222.2 84% 4.9%;
    --secondary: 217.2 32.6% 17.5%;
    --secondary-foreground: 210 40% 98%;
    --muted: 217.2 32.6% 17.5%;
    --muted-foreground: 215 20.2% 65.1%;
    --accent: 217.2 32.6% 17.5%;
    --accent-foreground: 210 40% 98%;
    --destructive: 0 62.8% 30.6%;
    --destructive-foreground: 210 40% 98%;
    --border: 217.2 32.6% 17.5%;
    --input: 217.2 32.6% 17.5%;
    --ring: 224.3 76.3% 94.1%;
  }
}

@layer base {
  * {
    @apply border-border;
  }
  
  body {
    @apply bg-background text-foreground;
    font-feature-settings: "rlig" 1, "calt" 1;
  }
  
  /* Scrollbar styling */
  ::-webkit-scrollbar {
    @apply w-2 h-2;
  }
  
  ::-webkit-scrollbar-track {
    @apply bg-muted;
  }
  
  ::-webkit-scrollbar-thumb {
    @apply bg-muted-foreground/30 rounded-full;
  }
  
  ::-webkit-scrollbar-thumb:hover {
    @apply bg-muted-foreground/50;
  }
  
  /* Focus styles */
  *:focus-visible {
    @apply outline-none ring-2 ring-ring ring-offset-2 ring-offset-background;
  }
  
  /* Selection styles */
  ::selection {
    @apply bg-primary/20;
  }
}

@layer components {
  /* Custom button variants */
  .btn-glass {
    @apply bg-background/80 backdrop-blur-sm border border-border/50 hover:bg-background/90 transition-all duration-200;
  }
  
  /* Loading states */
  .loading-dots {
    @apply inline-flex space-x-1;
  }
  
  .loading-dots > div {
    @apply w-1 h-1 bg-current rounded-full animate-pulse;
    animation-delay: calc(var(--i) * 0.2s);
  }
  
  /* Status indicators */
  .status-dot {
    @apply inline-block w-2 h-2 rounded-full;
  }
  
  .status-dot-success {
    @apply bg-green-500;
  }
  
  .status-dot-warning {
    @apply bg-yellow-500;
  }
  
  .status-dot-error {
    @apply bg-red-500;
  }
  
  .status-dot-info {
    @apply bg-blue-500;
  }
  
  /* Card hover effects */
  .card-hover {
    @apply transition-all duration-200 hover:shadow-lg hover:-translate-y-1;
  }
  
  /* Table improvements */
  .table-zebra tbody tr:nth-child(even) {
    @apply bg-muted/30;
  }
  
  .table-hover tbody tr:hover {
    @apply bg-accent/50;
  }
  
  /* Form improvements */
  .form-section {
    @apply space-y-4 p-4 border border-border rounded-lg bg-card;
  }
  
  .form-section-title {
    @apply text-lg font-semibold mb-4 text-foreground border-b border-border pb-2;
  }
  
  /* Dashboard specific */
  .dashboard-card {
    @apply bg-card border border-border rounded-lg p-6 shadow-sm hover:shadow-md transition-shadow;
  }
  
  .kpi-card {
    @apply dashboard-card hover:scale-105 transition-transform duration-200;
  }
  
  /* Sidebar improvements */
  .nav-item {
    @apply flex items-center space-x-3 px-3 py-2 rounded-lg text-muted-foreground hover:text-foreground hover:bg-accent transition-colors;
  }
  
  .nav-item.active {
    @apply bg-accent text-accent-foreground;
  }
  
  /* Mobile responsiveness */
  @media (max-width: 768px) {
    .mobile-hide {
      @apply hidden;
    }
    
    .mobile-full {
      @apply w-full;
    }
    
    .mobile-stack {
      @apply flex-col space-y-2 space-x-0;
    }
  }
  
  /* Print styles */
  @media print {
    .print-hide {
      @apply hidden;
    }
    
    .print-break {
      break-after: page;
    }
    
    body {
      @apply text-black bg-white;
    }
  }
  
  /* High contrast mode */
  @media (prefers-contrast: high) {
    .card {
      @apply border-2;
    }
    
    .button {
      @apply border-2;
    }
  }
  
  /* Reduced motion */
  @media (prefers-reduced-motion: reduce) {
    * {
      animation-duration: 0.01ms !important;
      animation-iteration-count: 1 !important;
      transition-duration: 0.01ms !important;
      scroll-behavior: auto !important;
    }
  }
}

@layer utilities {
  /* Custom animations */
  .animate-slide-up {
    animation: slideUp 0.3s ease-out;
  }
  
  .animate-slide-down {
    animation: slideDown 0.3s ease-out;
  }
  
  .animate-slide-left {
    animation: slideLeft 0.3s ease-out;
  }
  
  .animate-slide-right {
    animation: slideRight 0.3s ease-out;
  }
  
  .animate-scale-in {
    animation: scaleIn 0.2s ease-out;
  }
  
  .animate-scale-out {
    animation: scaleOut 0.2s ease-out;
  }
  
  /* Glass morphism effect */
  .glass {
    @apply bg-background/80 backdrop-blur-md border border-border/50;
  }
  
  /* Gradient backgrounds */
  .bg-gradient-primary {
    background: linear-gradient(135deg, hsl(var(--primary)) 0%, hsl(var(--primary)/0.8) 100%);
  }
  
  .bg-gradient-success {
    background: linear-gradient(135deg, #10b981 0%, #059669 100%);
  }
  
  .bg-gradient-warning {
    background: linear-gradient(135deg, #f59e0b 0%, #d97706 100%);
  }
  
  .bg-gradient-error {
    background: linear-gradient(135deg, #ef4444 0%, #dc2626 100%);
  }
  
  /* Text gradients */
  .text-gradient-primary {
    background: linear-gradient(135deg, hsl(var(--primary)) 0%, hsl(var(--primary)/0.8) 100%);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
  }
  
  /* Custom shadows */
  .shadow-glass {
    box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.37);
  }
  
  .shadow-soft {
    box-shadow: 0 2px 15px -3px rgba(0, 0, 0, 0.07), 0 10px 20px -2px rgba(0, 0, 0, 0.04);
  }
  
  /* Aspect ratios */
  .aspect-golden {
    aspect-ratio: 1.618;
  }
  
  /* Safe areas for mobile */
  .safe-top {
    padding-top: env(safe-area-inset-top);
  }
  
  .safe-bottom {
    padding-bottom: env(safe-area-inset-bottom);
  }
  
  .safe-left {
    padding-left: env(safe-area-inset-left);
  }
  
  .safe-right {
    padding-right: env(safe-area-inset-right);
  }
}

/* Animation keyframes */
@keyframes slideUp {
  from {
    opacity: 0;
    transform: translateY(20px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

@keyframes slideDown {
  from {
    opacity: 0;
    transform: translateY(-20px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

@keyframes slideLeft {
  from {
    opacity: 0;
    transform: translateX(20px);
  }
  to {
    opacity: 1;
    transform: translateX(0);
  }
}

@keyframes slideRight {
  from {
    opacity: 0;
    transform: translateX(-20px);
  }
  to {
    opacity: 1;
    transform: translateX(0);
  }
}

@keyframes scaleIn {
  from {
    opacity: 0;
    transform: scale(0.9);
  }
  to {
    opacity: 1;
    transform: scale(1);
  }
}

@keyframes scaleOut {
  from {
    opacity: 1;
    transform: scale(1);
  }
  to {
    opacity: 0;
    transform: scale(0.9);
  }
}

/* Loading animations */
@keyframes shimmer {
  0% {
    background-position: -468px 0;
  }
  100% {
    background-position: 468px 0;
  }
}

.shimmer {
  animation: shimmer 1.5s ease-in-out infinite;
  background: linear-gradient(90deg, 
    hsl(var(--muted)) 0%, 
    hsl(var(--muted-foreground)/0.1) 50%, 
    hsl(var(--muted)) 100%);
  background-size: 200% 100%;
}

/* Custom scrollbars for specific containers */
.custom-scrollbar {
  scrollbar-width: thin;
  scrollbar-color: hsl(var(--muted-foreground)/0.3) hsl(var(--muted));
}

.custom-scrollbar::-webkit-scrollbar {
  width: 6px;
  height: 6px;
}

.custom-scrollbar::-webkit-scrollbar-track {
  background: hsl(var(--muted));
  border-radius: 3px;
}

.custom-scrollbar::-webkit-scrollbar-thumb {
  background: hsl(var(--muted-foreground)/0.3);
  border-radius: 3px;
}

.custom-scrollbar::-webkit-scrollbar-thumb:hover {
  background: hsl(var(--muted-foreground)/0.5);
}

/* Data visualization improvements */
.chart-tooltip {
  @apply bg-background border border-border rounded-lg shadow-lg p-3 text-sm;
}

.chart-legend {
  @apply flex flex-wrap gap-4 justify-center mt-4;
}

.chart-legend-item {
  @apply flex items-center gap-2 text-sm;
}

.chart-legend-color {
  @apply w-3 h-3 rounded-full;
}

/* Status indicators for different data states */
.status-online {
  @apply text-green-600 dark:text-green-400;
}

.status-offline {
  @apply text-red-600 dark:text-red-400;
}

.status-pending {
  @apply text-yellow-600 dark:text-yellow-400;
}

.status-processing {
  @apply text-blue-600 dark:text-blue-400;
}

/* Accessibility improvements */
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  white-space: nowrap;
  border: 0;
}

.focus-trap {
  @apply outline-none;
}

/* RTL support (future enhancement) */
[dir="rtl"] .rtl\:right-auto {
  right: auto;
}

[dir="rtl"] .rtl\:left-auto {
  left: auto;
}

/* Content-specific styles */
.invoice-preview {
  @apply bg-white dark:bg-gray-900 p-8 rounded-lg shadow-lg max-w-4xl mx-auto;
}

.invoice-header {
  @apply border-b border-gray-200 dark:border-gray-700 pb-6 mb-6;
}

.invoice-table {
  @apply w-full border-collapse;
}

.invoice-table th,
.invoice-table td {
  @apply border border-gray-200 dark:border-gray-700 px-4 py-2 text-left;
}

.invoice-table th {
  @apply bg-gray-50 dark:bg-gray-800 font-semibold;
}

.invoice-total {
  @apply text-right font-bold text-lg;
}

/* Error boundaries and loading states */
.error-boundary {
  @apply min-h-screen flex items-center justify-center bg-background;
}

.error-content {
  @apply text-center p-8 max-w-md mx-auto;
}

.loading-screen {
  @apply min-h-screen flex items-center justify-center bg-background;
}

.loading-spinner {
  @apply animate-spin rounded-full h-8 w-8 border-b-2 border-primary;
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/global.css ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/hooks/useAnalytics.ts ---
// Path: frontend/src/hooks/useAnalytics.ts

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/hooks/useAnalytics.ts ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/hooks/useApi.ts ---
import { useState, useCallback } from 'react';
import { useUIStore } from '@/store';
import type { UseApiResult, UseMutationResult } from '@/types';

/**
 * Custom hook per gestire chiamate API con loading, error e success states
 */
export function useApi<T>(
  apiFunction: () => Promise<T>,
  options?: {
    onSuccess?: (data: T) => void;
    onError?: (error: Error) => void;
    loadingKey?: string;
  }
): UseApiResult<T> {
  const [data, setData] = useState<T | null>(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  
  const { setLoading: setGlobalLoading, setError: setGlobalError, addNotification } = useUIStore();

  const execute = useCallback(async () => {
    try {
      setLoading(true);
      setError(null);
      
      if (options?.loadingKey) {
        setGlobalLoading(options.loadingKey, true);
      }

      const result = await apiFunction();
      setData(result);
      
      if (options?.onSuccess) {
        options.onSuccess(result);
      }

      return result;
    } catch (err) {
      const errorMessage = err instanceof Error ? err.message : 'Errore sconosciuto';
      setError(errorMessage);
      
      if (options?.loadingKey) {
        setGlobalError(options.loadingKey, errorMessage);
      }

      if (options?.onError && err instanceof Error) {
        options.onError(err);
      } else {
        addNotification({
          type: 'error',
          title: 'Errore API',
          message: errorMessage,
          duration: 5000,
        });
      }

      throw err;
    } finally {
      setLoading(false);
      
      if (options?.loadingKey) {
        setGlobalLoading(options.loadingKey, false);
      }
    }
  }, [apiFunction, options, setGlobalLoading, setGlobalError, addNotification]);

  return {
    data,
    loading,
    error,
    refetch: execute,
  };
}

/**
 * Custom hook per mutazioni (POST, PUT, DELETE)
 */
export function useMutation<T, V = any>(
  mutationFunction: (variables: V) => Promise<T>,
  options?: {
    onSuccess?: (data: T, variables: V) => void;
    onError?: (error: Error, variables: V) => void;
    successMessage?: string;
    errorMessage?: string;
    loadingKey?: string;
  }
): UseMutationResult<T, V> {
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  
  const { setLoading: setGlobalLoading, addNotification } = useUIStore();

  const mutate = useCallback(async (variables: V): Promise<T> => {
    try {
      setLoading(true);
      setError(null);
      
      if (options?.loadingKey) {
        setGlobalLoading(options.loadingKey, true);
      }

      const result = await mutationFunction(variables);
      
      if (options?.onSuccess) {
        options.onSuccess(result, variables);
      }

      if (options?.successMessage) {
        addNotification({
          type: 'success',
          title: 'Operazione completata',
          message: options.successMessage,
          duration: 3000,
        });
      }

      return result;
    } catch (err) {
      const errorMessage = err instanceof Error ? err.message : 'Errore sconosciuto';
      setError(errorMessage);
      
      if (options?.onError && err instanceof Error) {
        options.onError(err, variables);
      }

      addNotification({
        type: 'error',
        title: 'Errore operazione',
        message: options?.errorMessage || errorMessage,
        duration: 5000,
      });

      throw err;
    } finally {
      setLoading(false);
      
      if (options?.loadingKey) {
        setGlobalLoading(options.loadingKey, false);
      }
    }
  }, [mutationFunction, options, setGlobalLoading, addNotification]);

  const reset = useCallback(() => {
    setError(null);
  }, []);

  return {
    mutate,
    loading,
    error,
    reset,
  };
}

/**
 * Hook per operazioni batch con progress tracking
 */
export function useBatchOperation<T, V = any>(
  batchFunction: (items: V[]) => Promise<T>,
  options?: {
    onProgress?: (completed: number, total: number) => void;
    onSuccess?: (data: T) => void;
    onError?: (error: Error) => void;
    chunkSize?: number;
  }
) {
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [progress, setProgress] = useState({ completed: 0, total: 0 });
  
  const { addNotification } = useUIStore();

  const execute = useCallback(async (items: V[]): Promise<T> => {
    try {
      setLoading(true);
      setError(null);
      setProgress({ completed: 0, total: items.length });

      // Se specificato chunk size, processa a blocchi
      if (options?.chunkSize && items.length > options.chunkSize) {
        const chunks = [];
        for (let i = 0; i < items.length; i += options.chunkSize) {
          chunks.push(items.slice(i, i + options.chunkSize));
        }

        const results = [];
        for (let i = 0; i < chunks.length; i++) {
          const chunkResult = await batchFunction(chunks[i]);
          results.push(chunkResult);
          
          const completed = (i + 1) * options.chunkSize;
          setProgress({ completed: Math.min(completed, items.length), total: items.length });
          
          if (options?.onProgress) {
            options.onProgress(Math.min(completed, items.length), items.length);
          }
        }
        
        // Combina risultati (assumendo che sia un array)
        const finalResult = results.flat() as T;
        
        if (options?.onSuccess) {
          options.onSuccess(finalResult);
        }

        return finalResult;
      } else {
        // Operazione singola
        const result = await batchFunction(items);
        setProgress({ completed: items.length, total: items.length });
        
        if (options?.onProgress) {
          options.onProgress(items.length, items.length);
        }

        if (options?.onSuccess) {
          options.onSuccess(result);
        }

        return result;
      }
    } catch (err) {
      const errorMessage = err instanceof Error ? err.message : 'Errore operazione batch';
      setError(errorMessage);
      
      if (options?.onError && err instanceof Error) {
        options.onError(err);
      }

      addNotification({
        type: 'error',
        title: 'Errore operazione batch',
        message: errorMessage,
        duration: 5000,
      });

      throw err;
    } finally {
      setLoading(false);
    }
  }, [batchFunction, options, addNotification]);

  const reset = useCallback(() => {
    setError(null);
    setProgress({ completed: 0, total: 0 });
  }, []);

  return {
    execute,
    loading,
    error,
    progress,
    reset,
  };
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/hooks/useApi.ts ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/hooks/useInvoices.ts ---
import { useQuery, useMutation, useQueryClient } from '@tanstack/react-query';
import { apiClient } from '@/services/api';
import { useDataStore, useUIStore } from '@/store';
import type { 
  Invoice, 
  InvoiceFilters, 
  InvoiceCreate, 
  InvoiceUpdate, 
  PaginatedResponse 
} from '@/types';

export function useInvoices(filters?: InvoiceFilters) {
  const { setInvoices, addRecentInvoice } = useDataStore();
  
  return useQuery({
    queryKey: ['invoices', filters],
    queryFn: async () => {
      const response = await apiClient.getInvoices(filters);
      if (response.success && response.data) {
        const data = response.data as PaginatedResponse<Invoice>;
        setInvoices(data.items, data.total);
        return data;
      }
      throw new Error(response.message || 'Errore nel caricamento fatture');
    },
    staleTime: 2 * 60 * 1000, // 2 minutes
  });
}

export function useInvoice(id: number) {
  const { addRecentInvoice } = useDataStore();
  
  return useQuery({
    queryKey: ['invoice', id],
    queryFn: async () => {
      const invoice = await apiClient.getInvoiceById(id);
      addRecentInvoice(invoice);
      return invoice;
    },
    enabled: !!id,
    staleTime: 5 * 60 * 1000, // 5 minutes
  });
}

export function useCreateInvoice() {
  const queryClient = useQueryClient();
  const { addNotification } = useUIStore();
  const { updateInvoice } = useDataStore();

  return useMutation({
    mutationFn: (data: InvoiceCreate) => apiClient.createInvoice(data),
    onSuccess: (newInvoice) => {
      // Invalida cache liste
      queryClient.invalidateQueries({ queryKey: ['invoices'] });
      queryClient.invalidateQueries({ queryKey: ['dashboard'] });
      
      // Aggiorna store
      updateInvoice(newInvoice.id, newInvoice);
      
      addNotification({
        type: 'success',
        title: 'Fattura creata',
        message: `Fattura ${newInvoice.doc_number} creata con successo`,
        duration: 3000,
      });
    },
    onError: (error) => {
      addNotification({
        type: 'error',
        title: 'Errore creazione fattura',
        message: error.message,
        duration: 5000,
      });
    },
  });
}

export function useUpdateInvoice() {
  const queryClient = useQueryClient();
  const { addNotification } = useUIStore();
  const { updateInvoice } = useDataStore();

  return useMutation({
    mutationFn: ({ id, data }: { id: number; data: InvoiceUpdate }) => 
      apiClient.updateInvoice(id, data),
    onSuccess: (updatedInvoice) => {
      // Aggiorna cache specifica
      queryClient.setQueryData(['invoice', updatedInvoice.id], updatedInvoice);
      
      // Invalida liste
      queryClient.invalidateQueries({ queryKey: ['invoices'] });
      
      // Aggiorna store
      updateInvoice(updatedInvoice.id, updatedInvoice);
      
      addNotification({
        type: 'success',
        title: 'Fattura aggiornata',
        message: `Fattura ${updatedInvoice.doc_number} aggiornata`,
        duration: 3000,
      });
    },
  });
}

export function useDeleteInvoice() {
  const queryClient = useQueryClient();
  const { addNotification } = useUIStore();
  const { removeInvoice } = useDataStore();

  return useMutation({
    mutationFn: (id: number) => apiClient.deleteInvoice(id),
    onSuccess: (_, invoiceId) => {
      // Rimuovi da cache
      queryClient.removeQueries({ queryKey: ['invoice', invoiceId] });
      queryClient.invalidateQueries({ queryKey: ['invoices'] });
      queryClient.invalidateQueries({ queryKey: ['dashboard'] });
      
      // Aggiorna store
      removeInvoice(invoiceId);
      
      addNotification({
        type: 'success',
        title: 'Fattura eliminata',
        message: 'Fattura eliminata con successo',
        duration: 3000,
      });
    },
  });
}

export function useUpdateInvoicePaymentStatus() {
  const queryClient = useQueryClient();
  const { addNotification } = useUIStore();
  const { updateInvoice } = useDataStore();

  return useMutation({
    mutationFn: ({ 
      id, 
      payment_status, 
      paid_amount 
    }: { 
      id: number; 
      payment_status: string; 
      paid_amount?: number;
    }) => apiClient.updateInvoicePaymentStatus(id, payment_status, paid_amount),
    onSuccess: (response, { id, payment_status, paid_amount }) => {
      // Aggiorna cache locale
      queryClient.invalidateQueries({ queryKey: ['invoice', id] });
      queryClient.invalidateQueries({ queryKey: ['invoices'] });
      queryClient.invalidateQueries({ queryKey: ['dashboard'] });
      
      // Aggiorna store
      updateInvoice(id, { 
        payment_status: payment_status as any, 
        paid_amount: paid_amount 
      });
      
      addNotification({
        type: 'success',
        title: 'Stato pagamento aggiornato',
        message: `Stato fattura aggiornato a: ${payment_status}`,
        duration: 3000,
      });
    },
  });
}

export function useInvoiceReconciliationLinks(invoiceId: number) {
  return useQuery({
    queryKey: ['invoice-reconciliation-links', invoiceId],
    queryFn: () => apiClient.getInvoiceReconciliationLinks(invoiceId),
    enabled: !!invoiceId,
    staleTime: 30 * 1000, // 30 seconds
  });
}

export function useOverdueInvoices(limit: number = 20) {
  return useQuery({
    queryKey: ['overdue-invoices', limit],
    queryFn: () => apiClient.getOverdueInvoices(limit),
    staleTime: 5 * 60 * 1000, // 5 minutes
  });
}

export function useAgingSummary(invoice_type: 'Attiva' | 'Passiva' = 'Attiva') {
  return useQuery({
    queryKey: ['aging-summary', invoice_type],
    queryFn: () => apiClient.getAgingSummary(invoice_type),
    staleTime: 5 * 60 * 1000,
  });
}

export function useSearchInvoices() {
  const { addNotification } = useUIStore();

  return useMutation({
    mutationFn: ({ query, type_filter }: { query: string; type_filter?: string }) =>
      apiClient.searchInvoices(query, type_filter),
    onError: () => {
      addNotification({
        type: 'error',
        title: 'Errore ricerca',
        message: 'Errore durante la ricerca fatture',
        duration: 3000,
      });
    },
  });
}

// Hook per bulk operations
export function useBulkInvoiceOperations() {
  const queryClient = useQueryClient();
  const { addNotification } = useUIStore();

  const bulkUpdateStatus = useMutation({
    mutationFn: async ({ 
      invoiceIds, 
      payment_status 
    }: { 
      invoiceIds: number[]; 
      payment_status: string;
    }) => {
      const results = await Promise.allSettled(
        invoiceIds.map(id => 
          apiClient.updateInvoicePaymentStatus(id, payment_status)
        )
      );
      
      const successful = results.filter(r => r.status === 'fulfilled').length;
      const failed = results.filter(r => r.status === 'rejected').length;
      
      return { successful, failed, total: invoiceIds.length };
    },
    onSuccess: ({ successful, failed, total }) => {
      queryClient.invalidateQueries({ queryKey: ['invoices'] });
      queryClient.invalidateQueries({ queryKey: ['dashboard'] });
      
      addNotification({
        type: successful === total ? 'success' : 'warning',
        title: 'Operazione bulk completata',
        message: `${successful}/${total} fatture aggiornate con successo${failed > 0 ? `, ${failed} errori` : ''}`,
        duration: 5000,
      });
    },
  });

  const bulkDelete = useMutation({
    mutationFn: async (invoiceIds: number[]) => {
      const results = await Promise.allSettled(
        invoiceIds.map(id => apiClient.deleteInvoice(id))
      );
      
      const successful = results.filter(r => r.status === 'fulfilled').length;
      const failed = results.filter(r => r.status === 'rejected').length;
      
      return { successful, failed, total: invoiceIds.length };
    },
    onSuccess: ({ successful, failed, total }) => {
      queryClient.invalidateQueries({ queryKey: ['invoices'] });
      queryClient.invalidateQueries({ queryKey: ['dashboard'] });
      
      addNotification({
        type: successful === total ? 'success' : 'warning',
        title: 'Eliminazione bulk completata',
        message: `${successful}/${total} fatture eliminate${failed > 0 ? `, ${failed} errori` : ''}`,
        duration: 5000,
      });
    },
  });

  return {
    bulkUpdateStatus,
    bulkDelete,
  };
}

// Hook per statistiche fatture
export function useInvoiceStats() {
  return useQuery({
    queryKey: ['invoice-stats'],
    queryFn: () => apiClient.getInvoicesStats(),
    staleTime: 10 * 60 * 1000, // 10 minutes
  });
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/hooks/useInvoices.ts ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/hooks/useReconciliation.ts ---
// Path: frontend/src/hooks/useReconciliation.ts

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/hooks/useReconciliation.ts ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/hooks/useTransactions.ts ---
import { useQuery, useMutation, useQueryClient } from '@tanstack/react-query';
import { apiClient } from '@/services/api';
import { useDataStore, useUIStore } from '@/store';
import type { 
  BankTransaction, 
  TransactionFilters, 
  BankTransactionCreate, 
  BankTransactionUpdate, 
  PaginatedResponse,
  ReconciliationStatus
} from '@/types';

export function useTransactions(filters?: TransactionFilters) {
  const { setTransactions, addRecentTransaction } = useDataStore();
  
  return useQuery({
    queryKey: ['transactions', filters],
    queryFn: async () => {
      const response = await apiClient.getTransactions(filters);
      if (response.success && response.data) {
        const data = response.data as PaginatedResponse<BankTransaction>;
        setTransactions(data.items, data.total);
        return data;
      }
      throw new Error(response.message || 'Errore nel caricamento transazioni');
    },
    staleTime: 2 * 60 * 1000, // 2 minutes
  });
}

export function useTransaction(id: number) {
  const { addRecentTransaction } = useDataStore();
  
  return useQuery({
    queryKey: ['transaction', id],
    queryFn: async () => {
      const transaction = await apiClient.getTransactionById(id);
      addRecentTransaction(transaction);
      return transaction;
    },
    enabled: !!id,
    staleTime: 5 * 60 * 1000, // 5 minutes
  });
}

export function useCreateTransaction() {
  const queryClient = useQueryClient();
  const { addNotification } = useUIStore();
  const { updateTransaction } = useDataStore();

  return useMutation({
    mutationFn: (data: BankTransactionCreate) => apiClient.createTransaction(data),
    onSuccess: (newTransaction) => {
      // Invalida cache liste
      queryClient.invalidateQueries({ queryKey: ['transactions'] });
      queryClient.invalidateQueries({ queryKey: ['dashboard'] });
      
      // Aggiorna store
      updateTransaction(newTransaction.id, newTransaction);
      
      addNotification({
        type: 'success',
        title: 'Transazione creata',
        message: 'Nuova transazione aggiunta con successo',
        duration: 3000,
      });
    },
    onError: (error) => {
      addNotification({
        type: 'error',
        title: 'Errore creazione transazione',
        message: error.message,
        duration: 5000,
      });
    },
  });
}

export function useUpdateTransaction() {
  const queryClient = useQueryClient();
  const { addNotification } = useUIStore();
  const { updateTransaction } = useDataStore();

  return useMutation({
    mutationFn: ({ id, data }: { id: number; data: BankTransactionUpdate }) => 
      apiClient.updateTransaction(id, data),
    onSuccess: (updatedTransaction) => {
      // Aggiorna cache specifica
      queryClient.setQueryData(['transaction', updatedTransaction.id], updatedTransaction);
      
      // Invalida liste
      queryClient.invalidateQueries({ queryKey: ['transactions'] });
      
      // Aggiorna store
      updateTransaction(updatedTransaction.id, updatedTransaction);
      
      addNotification({
        type: 'success',
        title: 'Transazione aggiornata',
        message: 'Transazione aggiornata con successo',
        duration: 3000,
      });
    },
  });
}

export function useDeleteTransaction() {
  const queryClient = useQueryClient();
  const { addNotification } = useUIStore();
  const { removeTransaction } = useDataStore();

  return useMutation({
    mutationFn: (id: number) => apiClient.deleteTransaction(id),
    onSuccess: (_, transactionId) => {
      // Rimuovi da cache
      queryClient.removeQueries({ queryKey: ['transaction', transactionId] });
      queryClient.invalidateQueries({ queryKey: ['transactions'] });
      queryClient.invalidateQueries({ queryKey: ['dashboard'] });
      
      // Aggiorna store
      removeTransaction(transactionId);
      
      addNotification({
        type: 'success',
        title: 'Transazione eliminata',
        message: 'Transazione eliminata con successo',
        duration: 3000,
      });
    },
  });
}

export function useUpdateTransactionStatus() {
  const queryClient = useQueryClient();
  const { addNotification } = useUIStore();
  const { updateTransaction } = useDataStore();

  return useMutation({
    mutationFn: ({ 
      id, 
      reconciliation_status, 
      reconciled_amount 
    }: { 
      id: number; 
      reconciliation_status: ReconciliationStatus; 
      reconciled_amount?: number;
    }) => apiClient.updateTransactionStatus(id, reconciliation_status, reconciled_amount),
    onSuccess: (response, { id, reconciliation_status, reconciled_amount }) => {
      // Aggiorna cache locale
      queryClient.invalidateQueries({ queryKey: ['transaction', id] });
      queryClient.invalidateQueries({ queryKey: ['transactions'] });
      queryClient.invalidateQueries({ queryKey: ['dashboard'] });
      
      // Aggiorna store
      updateTransaction(id, { 
        reconciliation_status, 
        reconciled_amount 
      });
      
      addNotification({
        type: 'success',
        title: 'Stato riconciliazione aggiornato',
        message: `Stato aggiornato a: ${reconciliation_status}`,
        duration: 3000,
      });
    },
  });
}

export function useBatchUpdateTransactionStatus() {
  const queryClient = useQueryClient();
  const { addNotification } = useUIStore();

  return useMutation({
    mutationFn: ({ 
      transaction_ids, 
      reconciliation_status 
    }: { 
      transaction_ids: number[]; 
      reconciliation_status: ReconciliationStatus;
    }) => apiClient.batchUpdateTransactionStatus(transaction_ids, reconciliation_status),
    onSuccess: (response) => {
      queryClient.invalidateQueries({ queryKey: ['transactions'] });
      queryClient.invalidateQueries({ queryKey: ['dashboard'] });
      
      addNotification({
        type: 'success',
        title: 'Aggiornamento batch completato',
        message: `${response.data.successful} transazioni aggiornate con successo`,
        duration: 5000,
      });
    },
  });
}

export function useTransactionReconciliationLinks(transactionId: number) {
  return useQuery({
    queryKey: ['transaction-reconciliation-links', transactionId],
    queryFn: () => apiClient.getTransactionReconciliationLinks(transactionId),
    enabled: !!transactionId,
    staleTime: 30 * 1000, // 30 seconds
  });
}

export function useTransactionPotentialMatches(transactionId: number, limit: number = 10) {
  return useQuery({
    queryKey: ['transaction-potential-matches', transactionId, limit],
    queryFn: () => apiClient.getTransactionPotentialMatches(transactionId, limit),
    enabled: !!transactionId,
    staleTime: 60 * 1000, // 1 minute
  });
}

export function useSearchTransactions() {
  const { addNotification } = useUIStore();

  return useMutation({
    mutationFn: ({ query, include_reconciled }: { query: string; include_reconciled?: boolean }) =>
      apiClient.searchTransactions(query, include_reconciled),
    onError: () => {
      addNotification({
        type: 'error',
        title: 'Errore ricerca',
        message: 'Errore durante la ricerca transazioni',
        duration: 3000,
      });
    },
  });
}

export function useTransactionStats() {
  return useQuery({
    queryKey: ['transaction-stats'],
    queryFn: () => apiClient.getTransactionStats(),
    staleTime: 10 * 60 * 1000, // 10 minutes
  });
}

export function useCashFlowAnalysis(months: number = 12) {
  return useQuery({
    queryKey: ['cash-flow-analysis', months],
    queryFn: () => apiClient.getCashFlowAnalysis(months),
    staleTime: 5 * 60 * 1000, // 5 minutes
  });
}

// Hook per import CSV
export function useImportTransactionsCSV() {
  const queryClient = useQueryClient();
  const { addNotification } = useUIStore();

  return useMutation({
    mutationFn: (file: File) => apiClient.importTransactionsCSV(file),
    onSuccess: (result) => {
      queryClient.invalidateQueries({ queryKey: ['transactions'] });
      queryClient.invalidateQueries({ queryKey: ['dashboard'] });
      
      const { success, duplicates, errors } = result.data;
      
      if (success > 0) {
        addNotification({
          type: 'success',
          title: 'Import completato',
          message: `${success} transazioni importate con successo${duplicates > 0 ? `, ${duplicates} duplicati` : ''}${errors > 0 ? `, ${errors} errori` : ''}`,
          duration: 5000,
        });
      } else {
        addNotification({
          type: 'warning',
          title: 'Import senza nuove transazioni',
          message: `Nessuna nuova transazione importata. ${duplicates} duplicati, ${errors} errori`,
          duration: 5000,
        });
      }
    },
    onError: (error) => {
      addNotification({
        type: 'error',
        title: 'Errore import',
        message: error.message,
        duration: 5000,
      });
    },
  });
}

// Hook per export
export function useExportTransactions() {
  const { addNotification } = useUIStore();

  return useMutation({
    mutationFn: ({ 
      format, 
      filters 
    }: { 
      format: 'excel' | 'csv' | 'json'; 
      filters?: any;
    }) => apiClient.exportTransactions(format, filters),
    onSuccess: (result, { format }) => {
      if (format !== 'json') {
        // Per Excel e CSV, result è un Blob
        const blob = result as Blob;
        const url = window.URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.style.display = 'none';
        a.href = url;
        a.download = `transazioni_export.${format === 'excel' ? 'xlsx' : 'csv'}`;
        document.body.appendChild(a);
        a.click();
        window.URL.revokeObjectURL(url);
        document.body.removeChild(a);
        
        addNotification({
          type: 'success',
          title: 'Export completato',
          message: `File ${format.toUpperCase()} scaricato con successo`,
          duration: 3000,
        });
      }
    },
    onError: (error) => {
      addNotification({
        type: 'error',
        title: 'Errore export',
        message: error.message,
        duration: 5000,
      });
    },
  });
}

// Hook per download template CSV
export function useDownloadTransactionTemplate() {
  const { addNotification } = useUIStore();

  return useMutation({
    mutationFn: () => apiClient.downloadTransactionTemplate(),
    onSuccess: (blob) => {
      const url = window.URL.createObjectURL(blob);
      const a = document.createElement('a');
      a.style.display = 'none';
      a.href = url;
      a.download = 'template_transazioni_bancarie.csv';
      document.body.appendChild(a);
      a.click();
      window.URL.revokeObjectURL(url);
      document.body.removeChild(a);
      
      addNotification({
        type: 'success',
        title: 'Template scaricato',
        message: 'Template CSV scaricato con successo',
        duration: 3000,
      });
    },
    onError: (error) => {
      addNotification({
        type: 'error',
        title: 'Errore download',
        message: error.message,
        duration: 3000,
      });
    },
  });
}

// Hook per operazioni bulk
export function useBulkTransactionOperations() {
  const queryClient = useQueryClient();
  const { addNotification } = useUIStore();

  const bulkDelete = useMutation({
    mutationFn: async (transactionIds: number[]) => {
      const results = await Promise.allSettled(
        transactionIds.map(id => apiClient.deleteTransaction(id))
      );
      
      const successful = results.filter(r => r.status === 'fulfilled').length;
      const failed = results.filter(r => r.status === 'rejected').length;
      
      return { successful, failed, total: transactionIds.length };
    },
    onSuccess: ({ successful, failed, total }) => {
      queryClient.invalidateQueries({ queryKey: ['transactions'] });
      queryClient.invalidateQueries({ queryKey: ['dashboard'] });
      
      addNotification({
        type: successful === total ? 'success' : 'warning',
        title: 'Eliminazione bulk completata',
        message: `${successful}/${total} transazioni eliminate${failed > 0 ? `, ${failed} errori` : ''}`,
        duration: 5000,
      });
    },
  });

  return {
    bulkDelete,
  };
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/hooks/useTransactions.ts ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/lib/formatters.ts ---
/**
 * Utility functions for formatting data
 * Funzioni per formattare valute, date, numeri per l'UI italiana
 */

import { format, formatDistanceToNow, parseISO, isValid } from 'date-fns';
import { it } from 'date-fns/locale';

// Currency formatting
export const formatCurrency = (
  amount: number | string | null | undefined,
  currency: string = 'EUR',
  locale: string = 'it-IT'
): string => {
  if (amount === null || amount === undefined || amount === '') {
    return '€ 0,00';
  }

  const numericAmount = typeof amount === 'string' ? parseFloat(amount) : amount;
  
  if (isNaN(numericAmount)) {
    return '€ 0,00';
  }

  try {
    return new Intl.NumberFormat(locale, {
      style: 'currency',
      currency: currency,
      minimumFractionDigits: 2,
      maximumFractionDigits: 2,
    }).format(numericAmount);
  } catch (error) {
    // Fallback formatting
    return `€ ${numericAmount.toFixed(2).replace('.', ',')}`;
  }
};

// Compact currency formatting for large numbers
export const formatCurrencyCompact = (
  amount: number | string | null | undefined,
  currency: string = 'EUR',
  locale: string = 'it-IT'
): string => {
  if (amount === null || amount === undefined || amount === '') {
    return '€ 0';
  }

  const numericAmount = typeof amount === 'string' ? parseFloat(amount) : amount;
  
  if (isNaN(numericAmount)) {
    return '€ 0';
  }

  try {
    return new Intl.NumberFormat(locale, {
      style: 'currency',
      currency: currency,
      notation: 'compact',
      minimumFractionDigits: 0,
      maximumFractionDigits: 1,
    }).format(numericAmount);
  } catch (error) {
    // Manual compact formatting
    const abs = Math.abs(numericAmount);
    if (abs >= 1000000) {
      return `€ ${(numericAmount / 1000000).toFixed(1)}M`;
    } else if (abs >= 1000) {
      return `€ ${(numericAmount / 1000).toFixed(1)}K`;
    } else {
      return `€ ${numericAmount.toFixed(0)}`;
    }
  }
};

// Number formatting
export const formatNumber = (
  value: number | string | null | undefined,
  locale: string = 'it-IT',
  options: Intl.NumberFormatOptions = {}
): string => {
  if (value === null || value === undefined || value === '') {
    return '0';
  }

  const numericValue = typeof value === 'string' ? parseFloat(value) : value;
  
  if (isNaN(numericValue)) {
    return '0';
  }

  try {
    return new Intl.NumberFormat(locale, {
      minimumFractionDigits: 0,
      maximumFractionDigits: 2,
      ...options,
    }).format(numericValue);
  } catch (error) {
    return numericValue.toString();
  }
};

// Percentage formatting
export const formatPercentage = (
  value: number | string | null | undefined,
  locale: string = 'it-IT',
  decimals: number = 1
): string => {
  if (value === null || value === undefined || value === '') {
    return '0%';
  }

  const numericValue = typeof value === 'string' ? parseFloat(value) : value;
  
  if (isNaN(numericValue)) {
    return '0%';
  }

  try {
    return new Intl.NumberFormat(locale, {
      style: 'percent',
      minimumFractionDigits: decimals,
      maximumFractionDigits: decimals,
    }).format(numericValue / 100);
  } catch (error) {
    return `${numericValue.toFixed(decimals)}%`;
  }
};

// Date formatting
export const formatDate = (
  date: string | Date | null | undefined,
  formatStr: string = 'dd/MM/yyyy'
): string => {
  if (!date) {
    return '-';
  }

  try {
    const dateObj = typeof date === 'string' ? parseISO(date) : date;
    
    if (!isValid(dateObj)) {
      return '-';
    }

    return format(dateObj, formatStr, { locale: it });
  } catch (error) {
    return '-';
  }
};

// DateTime formatting
export const formatDateTime = (
  date: string | Date | null | undefined,
  formatStr: string = 'dd/MM/yyyy HH:mm'
): string => {
  return formatDate(date, formatStr);
};

// Relative time formatting
export const formatRelativeTime = (
  date: string | Date | null | undefined
): string => {
  if (!date) {
    return '-';
  }

  try {
    const dateObj = typeof date === 'string' ? parseISO(date) : date;
    
    if (!isValid(dateObj)) {
      return '-';
    }

    return formatDistanceToNow(dateObj, { 
      addSuffix: true, 
      locale: it 
    });
  } catch (error) {
    return '-';
  }
};

// Due date formatting with status
export const formatDueDate = (
  dueDate: string | Date | null | undefined,
  isPaid: boolean = false
): { text: string; variant: 'default' | 'warning' | 'destructive' | 'success' } => {
  if (!dueDate) {
    return { text: 'Senza scadenza', variant: 'default' };
  }

  if (isPaid) {
    return { text: 'Pagata', variant: 'success' };
  }

  try {
    const dateObj = typeof dueDate === 'string' ? parseISO(dueDate) : dueDate;
    
    if (!isValid(dateObj)) {
      return { text: 'Data non valida', variant: 'default' };
    }

    const now = new Date();
    const diffDays = Math.ceil((dateObj.getTime() - now.getTime()) / (1000 * 60 * 60 * 24));

    if (diffDays < 0) {
      return { 
        text: `Scaduta ${Math.abs(diffDays)} giorni fa`, 
        variant: 'destructive' 
      };
    } else if (diffDays === 0) {
      return { text: 'Scade oggi', variant: 'warning' };
    } else if (diffDays <= 7) {
      return { 
        text: `Scade tra ${diffDays} giorni`, 
        variant: 'warning' 
      };
    } else {
      return { 
        text: `Scade il ${formatDate(dateObj)}`, 
        variant: 'default' 
      };
    }
  } catch (error) {
    return { text: 'Errore data', variant: 'default' };
  }
};

// Payment status formatting
export const formatPaymentStatus = (
  status: string | null | undefined
): { label: string; variant: 'default' | 'secondary' | 'success' | 'warning' | 'destructive' } => {
  switch (status) {
    case 'Pagata Tot.':
      return { label: 'Pagata', variant: 'success' };
    case 'Pagata Parz.':
      return { label: 'Parziale', variant: 'warning' };
    case 'Scaduta':
      return { label: 'Scaduta', variant: 'destructive' };
    case 'Aperta':
      return { label: 'Aperta', variant: 'default' };
    case 'Insoluta':
      return { label: 'Insoluta', variant: 'destructive' };
    case 'Riconciliata':
      return { label: 'Riconciliata', variant: 'success' };
    default:
      return { label: status || 'Sconosciuto', variant: 'secondary' };
  }
};

// Reconciliation status formatting
export const formatReconciliationStatus = (
  status: string | null | undefined
): { label: string; variant: 'default' | 'secondary' | 'success' | 'warning' | 'destructive' } => {
  switch (status) {
    case 'Riconciliato Tot.':
      return { label: 'Completa', variant: 'success' };
    case 'Riconciliato Parz.':
      return { label: 'Parziale', variant: 'warning' };
    case 'Da Riconciliare':
      return { label: 'Da fare', variant: 'default' };
    case 'Riconciliato Eccesso':
      return { label: 'Eccesso', variant: 'warning' };
    case 'Ignorato':
      return { label: 'Ignorato', variant: 'secondary' };
    default:
      return { label: status || 'Sconosciuto', variant: 'secondary' };
  }
};

// File size formatting
export const formatFileSize = (bytes: number | null | undefined): string => {
  if (!bytes || bytes === 0) {
    return '0 B';
  }

  const sizes = ['B', 'KB', 'MB', 'GB'];
  const i = Math.floor(Math.log(bytes) / Math.log(1024));
  
  return `${(bytes / Math.pow(1024, i)).toFixed(i === 0 ? 0 : 1)} ${sizes[i]}`;
};

// Document number formatting
export const formatDocumentNumber = (
  docNumber: string | null | undefined,
  docType?: string
): string => {
  if (!docNumber) {
    return '-';
  }

  // Add document type prefix if provided
  if (docType) {
    return `${docType} ${docNumber}`;
  }

  return docNumber;
};

// VAT number formatting
export const formatVATNumber = (vat: string | null | undefined): string => {
  if (!vat) {
    return '-';
  }

  // Italian VAT format: IT + 11 digits
  if (vat.length === 11 && /^\d+$/.test(vat)) {
    return `IT${vat}`;
  }

  // Already formatted or different format
  return vat;
};

// Tax code formatting
export const formatTaxCode = (taxCode: string | null | undefined): string => {
  if (!taxCode) {
    return '-';
  }

  // Italian tax code format (16 characters)
  if (taxCode.length === 16) {
    return taxCode.toUpperCase();
  }

  return taxCode;
};

// IBAN formatting
export const formatIBAN = (iban: string | null | undefined): string => {
  if (!iban) {
    return '-';
  }

  // Remove spaces and convert to uppercase
  const cleanIBAN = iban.replace(/\s/g, '').toUpperCase();
  
  // Add spaces every 4 characters for readability
  return cleanIBAN.replace(/(.{4})/g, '$1 ').trim();
};

// Phone number formatting
export const formatPhoneNumber = (phone: string | null | undefined): string => {
  if (!phone) {
    return '-';
  }

  // Remove all non-numeric characters
  const cleanPhone = phone.replace(/\D/g, '');
  
  // Italian mobile format
  if (cleanPhone.length === 10 && cleanPhone.startsWith('3')) {
    return `+39 ${cleanPhone.slice(0, 3)} ${cleanPhone.slice(3, 6)} ${cleanPhone.slice(6)}`;
  }
  
  // Italian landline format
  if (cleanPhone.length >= 9 && cleanPhone.length <= 11) {
    return `+39 ${cleanPhone}`;
  }

  // Return as-is if doesn't match Italian formats
  return phone;
};

// Address formatting
export const formatAddress = (
  address?: string,
  city?: string,
  cap?: string,
  province?: string,
  country?: string
): string => {
  const parts = [];
  
  if (address) parts.push(address);
  
  const locationParts = [];
  if (cap) locationParts.push(cap);
  if (city) locationParts.push(city);
  if (province) locationParts.push(`(${province})`);
  
  if (locationParts.length > 0) {
    parts.push(locationParts.join(' '));
  }
  
  if (country && country !== 'IT') {
    parts.push(country);
  }
  
  return parts.join(', ') || '-';
};

// Amount difference formatting
export const formatAmountDifference = (
  amount1: number,
  amount2: number
): { text: string; variant: 'default' | 'success' | 'warning' | 'destructive' } => {
  const diff = amount1 - amount2;
  const absDiff = Math.abs(diff);
  
  if (absDiff < 0.01) {
    return { text: 'Esatto', variant: 'success' };
  } else if (diff > 0) {
    return { text: `+${formatCurrency(absDiff)}`, variant: 'warning' };
  } else {
    return { text: `-${formatCurrency(absDiff)}`, variant: 'destructive' };
  }
};

// Confidence score formatting
export const formatConfidenceScore = (
  score: number | null | undefined
): { label: string; variant: 'default' | 'success' | 'warning' | 'destructive' } => {
  if (score === null || score === undefined) {
    return { label: 'N/A', variant: 'default' };
  }

  const percentage = score * 100;
  
  if (percentage >= 80) {
    return { label: `${percentage.toFixed(0)}% - Alta`, variant: 'success' };
  } else if (percentage >= 60) {
    return { label: `${percentage.toFixed(0)}% - Media`, variant: 'warning' };
  } else {
    return { label: `${percentage.toFixed(0)}% - Bassa`, variant: 'destructive' };
  }
};

// Chart data formatting
export const formatChartValue = (
  value: number,
  type: 'currency' | 'number' | 'percentage' = 'number'
): string => {
  switch (type) {
    case 'currency':
      return formatCurrencyCompact(value);
    case 'percentage':
      return formatPercentage(value);
    case 'number':
    default:
      return formatNumber(value);
  }
};

// Duration formatting (for payment terms, etc.)
export const formatDuration = (days: number | null | undefined): string => {
  if (!days || days === 0) {
    return 'Immediato';
  }

  if (days === 1) {
    return '1 giorno';
  }

  if (days < 30) {
    return `${days} giorni`;
  }

  const months = Math.round(days / 30);
  if (months === 1) {
    return '1 mese';
  }

  return `${months} mesi`;
};

// Search highlighting
export const highlightSearchTerm = (
  text: string,
  searchTerm: string
): React.ReactNode => {
  if (!searchTerm.trim()) {
    return text;
  }

  const regex = new RegExp(`(${searchTerm.replace(/[.*+?^${}()|[\]\\]/g, '\\// Document number formatting
export const formatDocumentNumber = (
  docNumber: string | null | undefined,
  docType?: string
): string => {
  if (!docNumber')})`, 'gi');
  const parts = text.split(regex);

  return parts.map((part, index) =>
    regex.test(part) ? (
      <mark key={index} className="bg-yellow-200 dark:bg-yellow-800 px-1 rounded">
        {part}
      </mark>
    ) : (
      part
    )
  );
};

// Truncate text with ellipsis
export const truncateText = (
  text: string | null | undefined,
  maxLength: number = 50
): string => {
  if (!text) {
    return '-';
  }

  if (text.length <= maxLength) {
    return text;
  }

  return `${text.slice(0, maxLength)}...`;
};

// Score formatting with color
export const formatScore = (
  score: number | null | undefined
): { text: string; variant: 'default' | 'success' | 'warning' | 'destructive' } => {
  if (score === null || score === undefined) {
    return { text: 'N/A', variant: 'default' };
  }

  const roundedScore = Math.round(score);
  
  if (roundedScore >= 80) {
    return { text: `${roundedScore}/100`, variant: 'success' };
  } else if (roundedScore >= 60) {
    return { text: `${roundedScore}/100`, variant: 'warning' };
  } else {
    return { text: `${roundedScore}/100`, variant: 'destructive' };
  }
};
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/lib/formatters.ts ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/lib/utils.ts ---
import { type ClassValue, clsx } from "clsx"
import { twMerge } from "tailwind-merge"

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs))
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/lib/utils.ts ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/lib/validations.ts ---
// Path: frontend/src/lib/validations.ts

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/lib/validations.ts ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/main.tsx ---
import React from 'react';
import ReactDOM from 'react-dom/client';
import App from './App';
import './globals.css';

// Disable right-click context menu in production
if (import.meta.env.PROD) {
  document.addEventListener('contextmenu', (e) => e.preventDefault());
}

// Disable F12 and other dev tools shortcuts in production
if (import.meta.env.PROD) {
  document.addEventListener('keydown', (e) => {
    if (
      e.key === 'F12' ||
      (e.ctrlKey && e.shiftKey && e.key === 'I') ||
      (e.ctrlKey && e.shiftKey && e.key === 'C') ||
      (e.ctrlKey && e.shiftKey && e.key === 'J') ||
      (e.ctrlKey && e.key === 'U')
    ) {
      e.preventDefault();
    }
  });
}

ReactDOM.createRoot(document.getElementById('root') as HTMLElement).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
);
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/main.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/pages/AnagraphicsDetailPage.tsx ---
// Path: frontend/src/pages/AnagraphicsDetailPage.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/pages/AnagraphicsDetailPage.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/pages/AnagraphicsPage.tsx ---
// Path: frontend/src/pages/AnagraphicsPage.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/pages/AnagraphicsPage.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/pages/AnalyticsPage.tsx ---
// Path: frontend/src/pages/AnalyticsPage.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/pages/AnalyticsPage.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/pages/DashboardPage.tsx ---
import React from 'react';
import { useQuery } from '@tanstack/react-query';
import {
  TrendingUp,
  TrendingDown,
  DollarSign,
  CreditCard,
  AlertTriangle,
  Users,
  FileText,
  Calendar,
  ArrowUpRight,
  ArrowDownRight,
} from 'lucide-react';

// Components
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';
import { Button } from '@/components/ui/button';
import { Skeleton } from '@/components/ui/skeleton';
import { KPICards } from '@/components/dashboard/KPICards';
import { RevenueChart } from '@/components/dashboard/RevenueChart';
import { CashFlowChart } from '@/components/dashboard/CashFlowChart';
import { TopClientsTable } from '@/components/dashboard/TopClientsTable';
import { RecentActivity } from '@/components/dashboard/RecentActivity';
import { OverdueInvoices } from '@/components/dashboard/OverdueInvoices';

// API & Store
import { apiClient } from '@/services/api';
import { useDataStore } from '@/store';

// Types
import type { DashboardData } from '@/types';

// Utils
import { formatCurrency, formatDate } from '@/lib/formatters';

export function DashboardPage() {
  const { setDashboardData } = useDataStore();

  // Fetch dashboard data
  const { data: dashboardData, isLoading, error, refetch } = useQuery({
    queryKey: ['dashboard'],
    queryFn: async () => {
      const response = await apiClient.getDashboardData();
      if (response.success && response.data) {
        setDashboardData(response.data);
        return response.data as DashboardData;
      }
      throw new Error(response.message || 'Errore nel caricamento dashboard');
    },
    staleTime: 2 * 60 * 1000, // 2 minutes
    refetchInterval: 5 * 60 * 1000, // Refresh every 5 minutes
  });

  if (error) {
    return (
      <div className="space-y-6">
        <div className="flex items-center justify-between">
          <div>
            <h1 className="text-3xl font-bold tracking-tight">Dashboard</h1>
            <p className="text-muted-foreground">
              Panoramica generale dell'attività
            </p>
          </div>
        </div>

        <Card className="border-destructive">
          <CardHeader>
            <CardTitle className="flex items-center gap-2 text-destructive">
              <AlertTriangle className="h-5 w-5" />
              Errore caricamento dati
            </CardTitle>
            <CardDescription>
              Si è verificato un errore durante il caricamento della dashboard
            </CardDescription>
          </CardHeader>
          <CardContent>
            <div className="space-y-2">
              <p className="text-sm text-muted-foreground">
                {error instanceof Error ? error.message : 'Errore sconosciuto'}
              </p>
              <Button onClick={() => refetch()} variant="outline" size="sm">
                Riprova
              </Button>
            </div>
          </CardContent>
        </Card>
      </div>
    );
  }

  return (
    <div className="space-y-6 animate-fade-in">
      {/* Header */}
      <div className="flex items-center justify-between">
        <div>
          <h1 className="text-3xl font-bold tracking-tight">Dashboard</h1>
          <p className="text-muted-foreground">
            Panoramica generale dell'attività aziendale
          </p>
        </div>
        <div className="flex items-center space-x-2">
          <Button
            variant="outline"
            size="sm"
            onClick={() => refetch()}
            disabled={isLoading}
          >
            {isLoading ? 'Aggiornamento...' : 'Aggiorna'}
          </Button>
          <Button size="sm">
            Esporta Report
          </Button>
        </div>
      </div>

      {/* KPI Cards */}
      {isLoading ? (
        <div className="grid gap-4 md:grid-cols-2 lg:grid-cols-4">
          {Array.from({ length: 4 }).map((_, i) => (
            <Card key={i}>
              <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
                <Skeleton className="h-4 w-32" />
                <Skeleton className="h-4 w-4" />
              </CardHeader>
              <CardContent>
                <Skeleton className="h-8 w-24 mb-2" />
                <Skeleton className="h-3 w-20" />
              </CardContent>
            </Card>
          ))}
        </div>
      ) : dashboardData ? (
        <KPICards data={dashboardData.kpis} />
      ) : null}

      {/* Charts Row */}
      <div className="grid gap-4 md:grid-cols-2">
        {/* Revenue Chart */}
        <Card>
          <CardHeader>
            <CardTitle className="flex items-center gap-2">
              <TrendingUp className="h-5 w-5" />
              Trend Fatturato
            </CardTitle>
            <CardDescription>
              Andamento mensile ultimi 12 mesi
            </CardDescription>
          </CardHeader>
          <CardContent>
            {isLoading ? (
              <Skeleton className="h-64 w-full" />
            ) : (
              <RevenueChart />
            )}
          </CardContent>
        </Card>

        {/* Cash Flow Chart */}
        <Card>
          <CardHeader>
            <CardTitle className="flex items-center gap-2">
              <CreditCard className="h-5 w-5" />
              Cash Flow
            </CardTitle>
            <CardDescription>
              Flussi di cassa mensili
            </CardDescription>
          </CardHeader>
          <CardContent>
            {isLoading ? (
              <Skeleton className="h-64 w-full" />
            ) : dashboardData?.cash_flow_summary ? (
              <CashFlowChart data={dashboardData.cash_flow_summary} />
            ) : (
              <div className="flex h-64 items-center justify-center text-muted-foreground">
                Nessun dato disponibile
              </div>
            )}
          </CardContent>
        </Card>
      </div>

      {/* Tables Row */}
      <div className="grid gap-4 md:grid-cols-2">
        {/* Top Clients */}
        <Card>
          <CardHeader>
            <CardTitle className="flex items-center gap-2">
              <Users className="h-5 w-5" />
              Top Clienti
            </CardTitle>
            <CardDescription>
              Clienti per fatturato ultimi 12 mesi
            </CardDescription>
          </CardHeader>
          <CardContent>
            {isLoading ? (
              <div className="space-y-3">
                {Array.from({ length: 5 }).map((_, i) => (
                  <div key={i} className="flex items-center space-x-3">
                    <Skeleton className="h-10 w-10 rounded-full" />
                    <div className="space-y-1 flex-1">
                      <Skeleton className="h-4 w-32" />
                      <Skeleton className="h-3 w-20" />
                    </div>
                    <Skeleton className="h-4 w-16" />
                  </div>
                ))}
              </div>
            ) : dashboardData?.top_clients ? (
              <TopClientsTable data={dashboardData.top_clients} />
            ) : (
              <div className="flex h-40 items-center justify-center text-muted-foreground">
                Nessun dato clienti disponibile
              </div>
            )}
          </CardContent>
        </Card>

        {/* Overdue Invoices */}
        <Card>
          <CardHeader>
            <CardTitle className="flex items-center gap-2">
              <AlertTriangle className="h-5 w-5 text-orange-500" />
              Fatture Scadute
            </CardTitle>
            <CardDescription>
              Fatture con pagamenti in ritardo
            </CardDescription>
          </CardHeader>
          <CardContent>
            {isLoading ? (
              <div className="space-y-3">
                {Array.from({ length: 4 }).map((_, i) => (
                  <div key={i} className="flex items-center justify-between">
                    <div className="space-y-1">
                      <Skeleton className="h-4 w-24" />
                      <Skeleton className="h-3 w-32" />
                    </div>
                    <div className="text-right space-y-1">
                      <Skeleton className="h-4 w-16" />
                      <Skeleton className="h-3 w-12" />
                    </div>
                  </div>
                ))}
              </div>
            ) : dashboardData?.overdue_invoices ? (
              <OverdueInvoices data={dashboardData.overdue_invoices} />
            ) : (
              <div className="flex h-40 items-center justify-center text-muted-foreground">
                <div className="text-center">
                  <AlertTriangle className="h-8 w-8 mx-auto mb-2 text-green-500" />
                  <p>Nessuna fattura scaduta!</p>
                  <p className="text-xs">Ottimo lavoro 🎉</p>
                </div>
              </div>
            )}
          </CardContent>
        </Card>
      </div>

      {/* Recent Activity */}
      <Card>
        <CardHeader>
          <CardTitle className="flex items-center gap-2">
            <Calendar className="h-5 w-5" />
            Attività Recente
          </CardTitle>
          <CardDescription>
            Ultime operazioni e movimenti
          </CardDescription>
        </CardHeader>
        <CardContent>
          {isLoading ? (
            <div className="space-y-4">
              {Array.from({ length: 6 }).map((_, i) => (
                <div key={i} className="flex items-start space-x-3">
                  <Skeleton className="h-8 w-8 rounded-full" />
                  <div className="space-y-1 flex-1">
                    <Skeleton className="h-4 w-48" />
                    <Skeleton className="h-3 w-32" />
                  </div>
                  <Skeleton className="h-3 w-16" />
                </div>
              ))}
            </div>
          ) : dashboardData ? (
            <RecentActivity 
              invoices={dashboardData.recent_invoices || []}
              transactions={dashboardData.recent_transactions || []}
            />
          ) : (
            <div className="flex h-32 items-center justify-center text-muted-foreground">
              Nessuna attività recente
            </div>
          )}
        </CardContent>
      </Card>

      {/* Quick Actions */}
      <Card>
        <CardHeader>
          <CardTitle>Azioni Rapide</CardTitle>
          <CardDescription>
            Operazioni frequenti per velocizzare il lavoro
          </CardDescription>
        </CardHeader>
        <CardContent>
          <div className="grid gap-3 md:grid-cols-2 lg:grid-cols-4">
            <Button className="h-16 flex-col space-y-1" variant="outline">
              <FileText className="h-5 w-5" />
              <span className="text-xs">Nuova Fattura</span>
            </Button>
            <Button className="h-16 flex-col space-y-1" variant="outline">
              <CreditCard className="h-5 w-5" />
              <span className="text-xs">Import Movimenti</span>
            </Button>
            <Button className="h-16 flex-col space-y-1" variant="outline">
              <TrendingUp className="h-5 w-5" />
              <span className="text-xs">Riconciliazione</span>
            </Button>
            <Button className="h-16 flex-col space-y-1" variant="outline">
              <Users className="h-5 w-5" />
              <span className="text-xs">Nuovo Cliente</span>
            </Button>
          </div>
        </CardContent>
      </Card>
    </div>
  );
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/pages/DashboardPage.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/pages/ImportExportPage.tsx ---
// Path: frontend/src/pages/ImportExportPage.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/pages/ImportExportPage.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/pages/InvoiceDetailPage.tsx ---
// Path: frontend/src/pages/InvoiceDetailPage.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/pages/InvoiceDetailPage.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/pages/InvoicesPage.tsx ---
import React, { useState, useMemo } from 'react';
import { Link } from 'react-router-dom';
import {
  ColumnDef,
  flexRender,
  getCoreRowModel,
  getFilteredRowModel,
  getPaginationRowModel,
  getSortedRowModel,
  useReactTable,
} from '@tanstack/react-table';
import {
  FileText,
  Plus,
  Search,
  Filter,
  Download,
  Upload,
  MoreHorizontal,
  Eye,
  Edit,
  Trash,
  DollarSign,
  Calendar,
  CheckCircle,
  AlertTriangle,
  Clock,
} from 'lucide-react';

// Components
import {
  Button,
  Card,
  CardContent,
  CardDescription,
  CardHeader,
  CardTitle,
  Input,
  Table,
  TableBody,
  TableCell,
  TableHead,
  TableHeader,
  TableRow,
  Badge,
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuLabel,
  DropdownMenuSeparator,
  DropdownMenuTrigger,
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
  Checkbox,
  Skeleton,
  Dialog,
  DialogContent,
  DialogDescription,
  DialogHeader,
  DialogTitle,
  DialogTrigger,
} from '@/components/ui';

// Hooks
import { useInvoices, useBulkInvoiceOperations, useDeleteInvoice } from '@/hooks/useInvoices';

// Utils
import { 
  formatCurrency, 
  formatDate, 
  formatPaymentStatus, 
  formatDueDate 
} from '@/lib/formatters';
import { cn } from '@/lib/utils';

// Types
import type { Invoice, InvoiceFilters, PaymentStatus, InvoiceType } from '@/types';

export function InvoicesPage() {
  // State per filtri e selezione
  const [globalFilter, setGlobalFilter] = useState('');
  const [filters, setFilters] = useState<InvoiceFilters>({
    page: 1,
    size: 50,
  });
  const [selectedRows, setSelectedRows] = useState<number[]>([]);
  const [showDeleteDialog, setShowDeleteDialog] = useState(false);
  const [invoiceToDelete, setInvoiceToDelete] = useState<number | null>(null);

  // API Hooks
  const { data: invoicesData, isLoading, error, refetch } = useInvoices(filters);
  const { bulkUpdateStatus, bulkDelete } = useBulkInvoiceOperations();
  const deleteInvoiceMutation = useDeleteInvoice();

  // Definizione colonne tabella
  const columns = useMemo<ColumnDef<Invoice>[]>(() => [
    {
      id: 'select',
      header: ({ table }) => (
        <Checkbox
          checked={table.getIsAllPageRowsSelected()}
          onCheckedChange={(value) => table.toggleAllPageRowsSelected(!!value)}
          aria-label="Seleziona tutto"
        />
      ),
      cell: ({ row }) => (
        <Checkbox
          checked={row.getIsSelected()}
          onCheckedChange={(value) => row.toggleSelected(!!value)}
          aria-label="Seleziona riga"
        />
      ),
      enableSorting: false,
      enableHiding: false,
    },
    {
      accessorKey: 'doc_number',
      header: 'Numero',
      cell: ({ row }) => (
        <div className="font-medium">
          <Link 
            to={`/invoices/${row.original.id}`}
            className="text-primary hover:underline"
          >
            {row.getValue('doc_number')}
          </Link>
          <div className="text-xs text-muted-foreground">
            {row.original.type}
          </div>
        </div>
      ),
    },
    {
      accessorKey: 'doc_date',
      header: 'Data',
      cell: ({ row }) => (
        <div className="text-sm">
          {formatDate(row.getValue('doc_date'))}
        </div>
      ),
    },
    {
      accessorKey: 'counterparty_name',
      header: 'Cliente/Fornitore',
      cell: ({ row }) => (
        <div className="max-w-48 truncate" title={row.getValue('counterparty_name')}>
          {row.getValue('counterparty_name')}
        </div>
      ),
    },
    {
      accessorKey: 'total_amount',
      header: 'Importo',
      cell: ({ row }) => (
        <div className="text-right font-medium">
          {formatCurrency(row.getValue('total_amount'))}
        </div>
      ),
    },
    {
      accessorKey: 'due_date',
      header: 'Scadenza',
      cell: ({ row }) => {
        const dueDate = row.getValue('due_date') as string;
        const isPaid = row.original.payment_status === 'Pagata Tot.';
        const dueDateInfo = formatDueDate(dueDate, isPaid);
        
        return (
          <Badge variant={dueDateInfo.variant} className="text-xs">
            {dueDateInfo.text}
          </Badge>
        );
      },
    },
    {
      accessorKey: 'payment_status',
      header: 'Stato',
      cell: ({ row }) => {
        const status = formatPaymentStatus(row.getValue('payment_status'));
        return (
          <Badge variant={status.variant} className="text-xs">
            {status.label}
          </Badge>
        );
      },
    },
    {
      accessorKey: 'open_amount',
      header: 'Residuo',
      cell: ({ row }) => {
        const openAmount = row.original.open_amount || 0;
        return (
          <div className={cn(
            'text-right font-medium text-sm',
            openAmount > 0 ? 'text-orange-600' : 'text-green-600'
          )}>
            {openAmount > 0 ? formatCurrency(openAmount) : '€ 0,00'}
          </div>
        );
      },
    },
    {
      id: 'actions',
      cell: ({ row }) => (
        <DropdownMenu>
          <DropdownMenuTrigger asChild>
            <Button variant="ghost" className="h-8 w-8 p-0">
              <MoreHorizontal className="h-4 w-4" />
            </Button>
          </DropdownMenuTrigger>
          <DropdownMenuContent align="end">
            <DropdownMenuLabel>Azioni</DropdownMenuLabel>
            <DropdownMenuSeparator />
            <DropdownMenuItem asChild>
              <Link to={`/invoices/${row.original.id}`}>
                <Eye className="mr-2 h-4 w-4" />
                Visualizza
              </Link>
            </DropdownMenuItem>
            <DropdownMenuItem asChild>
              <Link to={`/invoices/${row.original.id}/edit`}>
                <Edit className="mr-2 h-4 w-4" />
                Modifica
              </Link>
            </DropdownMenuItem>
            <DropdownMenuSeparator />
            <DropdownMenuItem
              className="text-destructive"
              onClick={() => {
                setInvoiceToDelete(row.original.id);
                setShowDeleteDialog(true);
              }}
            >
              <Trash className="mr-2 h-4 w-4" />
              Elimina
            </DropdownMenuItem>
          </DropdownMenuContent>
        </DropdownMenu>
      ),
    },
  ], []);

  // Setup tabella
  const table = useReactTable({
    data: invoicesData?.items || [],
    columns,
    onGlobalFilterChange: setGlobalFilter,
    getCoreRowModel: getCoreRowModel(),
    getFilteredRowModel: getFilteredRowModel(),
    getPaginationRowModel: getPaginationRowModel(),
    getSortedRowModel: getSortedRowModel(),
    onRowSelectionChange: (updater) => {
      const newSelection = typeof updater === 'function' 
        ? updater(selectedRows.reduce((acc, id) => ({ ...acc, [id]: true }), {}))
        : updater;
      setSelectedRows(Object.keys(newSelection).map(Number));
    },
    state: {
      globalFilter,
      rowSelection: selectedRows.reduce((acc, id) => ({ ...acc, [id]: true }), {}),
    },
  });

  // Handlers
  const handleFilterChange = (key: keyof InvoiceFilters, value: any) => {
    setFilters(prev => ({ ...prev, [key]: value, page: 1 }));
  };

  const handleBulkStatusUpdate = async (status: PaymentStatus) => {
    if (selectedRows.length === 0) return;
    
    try {
      await bulkUpdateStatus.mutateAsync({
        invoiceIds: selectedRows,
        payment_status: status,
      });
      setSelectedRows([]);
      refetch();
    } catch (error) {
      console.error('Bulk update error:', error);
    }
  };

  const handleDeleteInvoice = async () => {
    if (!invoiceToDelete) return;
    
    try {
      await deleteInvoiceMutation.mutateAsync(invoiceToDelete);
      setShowDeleteDialog(false);
      setInvoiceToDelete(null);
      refetch();
    } catch (error) {
      console.error('Delete error:', error);
    }
  };

  // Statistiche rapide
  const stats = useMemo(() => {
    if (!invoicesData?.items) return null;
    
    const invoices = invoicesData.items;
    return {
      total: invoices.length,
      totalAmount: invoices.reduce((sum, inv) => sum + inv.total_amount, 0),
      paidAmount: invoices.reduce((sum, inv) => sum + inv.paid_amount, 0),
      overdue: invoices.filter(inv => {
        if (!inv.due_date || inv.payment_status === 'Pagata Tot.') return false;
        return new Date(inv.due_date) < new Date();
      }).length,
    };
  }, [invoicesData?.items]);

  return (
    <div className="space-y-6">
      {/* Header */}
      <div className="flex items-center justify-between">
        <div>
          <h1 className="text-3xl font-bold tracking-tight">Fatture</h1>
          <p className="text-muted-foreground">
            Gestione fatture attive e passive
          </p>
        </div>
        <div className="flex items-center space-x-2">
          <Button variant="outline" size="sm">
            <Upload className="mr-2 h-4 w-4" />
            Importa
          </Button>
          <Button variant="outline" size="sm">
            <Download className="mr-2 h-4 w-4" />
            Esporta
          </Button>
          <Button size="sm" asChild>
            <Link to="/invoices/new">
              <Plus className="mr-2 h-4 w-4" />
              Nuova Fattura
            </Link>
          </Button
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/pages/InvoicesPage.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/pages/ReconciliationPage.tsx ---
// Path: frontend/src/pages/ReconciliationPage.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/pages/ReconciliationPage.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/pages/SettingsPage.tsx ---
// Path: frontend/src/pages/SettingsPage.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/pages/SettingsPage.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/pages/TransactionDetailPage.tsx ---
// Path: frontend/src/pages/TransactionDetailPage.tsx

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/pages/TransactionDetailPage.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/pages/TransactionsPage.tsx ---
import React, { useState, useMemo, useRef } from 'react';
import { Link } from 'react-router-dom';
import {
  ColumnDef,
  flexRender,
  getCoreRowModel,
  getFilteredRowModel,
  getPaginationRowModel,
  getSortedRowModel,
  useReactTable,
} from '@tanstack/react-table';
import {
  CreditCard,
  Plus,
  Search,
  Filter,
  Download,
  Upload,
  MoreHorizontal,
  Eye,
  Edit,
  Trash,
  DollarSign,
  Calendar,
  CheckCircle,
  AlertTriangle,
  FileSpreadsheet,
  ArrowUpDown,
  TrendingUp,
  TrendingDown,
} from 'lucide-react';

// Components
import {
  Button,
  Card,
  CardContent,
  CardDescription,
  CardHeader,
  CardTitle,
  Input,
  Table,
  TableBody,
  TableCell,
  TableHead,
  TableHeader,
  TableRow,
  Badge,
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuLabel,
  DropdownMenuSeparator,
  DropdownMenuTrigger,
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
  Checkbox,
  Skeleton,
  Dialog,
  DialogContent,
  DialogDescription,
  DialogHeader,
  DialogTitle,
  DialogTrigger,
} from '@/components/ui';

// Hooks
import { 
  useTransactions, 
  useBulkTransactionOperations, 
  useDeleteTransaction,
  useImportTransactionsCSV,
  useExportTransactions,
  useDownloadTransactionTemplate,
  useTransactionStats
} from '@/hooks/useTransactions';

// Utils
import { 
  formatCurrency, 
  formatDate, 
  formatReconciliationStatus 
} from '@/lib/formatters';
import { cn } from '@/lib/utils';

// Types
import type { BankTransaction, TransactionFilters, ReconciliationStatus } from '@/types';

export function TransactionsPage() {
  // State per filtri e selezione
  const [globalFilter, setGlobalFilter] = useState('');
  const [filters, setFilters] = useState<TransactionFilters>({
    page: 1,
    size: 50,
  });
  const [selectedRows, setSelectedRows] = useState<number[]>([]);
  const [showDeleteDialog, setShowDeleteDialog] = useState(false);
  const [transactionToDelete, setTransactionToDelete] = useState<number | null>(null);
  const [showImportDialog, setShowImportDialog] = useState(false);
  
  // File input ref
  const fileInputRef = useRef<HTMLInputElement>(null);

  // API Hooks
  const { data: transactionsData, isLoading, error, refetch } = useTransactions(filters);
  const { data: statsData } = useTransactionStats();
  const { bulkDelete } = useBulkTransactionOperations();
  const deleteTransactionMutation = useDeleteTransaction();
  const importCSVMutation = useImportTransactionsCSV();
  const exportMutation = useExportTransactions();
  const downloadTemplateMutation = useDownloadTransactionTemplate();

  // Definizione colonne tabella
  const columns = useMemo<ColumnDef<BankTransaction>[]>(() => [
    {
      id: 'select',
      header: ({ table }) => (
        <Checkbox
          checked={table.getIsAllPageRowsSelected()}
          onCheckedChange={(value) => table.toggleAllPageRowsSelected(!!value)}
          aria-label="Seleziona tutto"
        />
      ),
      cell: ({ row }) => (
        <Checkbox
          checked={row.getIsSelected()}
          onCheckedChange={(value) => row.toggleSelected(!!value)}
          aria-label="Seleziona riga"
        />
      ),
      enableSorting: false,
      enableHiding: false,
    },
    {
      accessorKey: 'transaction_date',
      header: 'Data',
      cell: ({ row }) => (
        <div className="text-sm">
          {formatDate(row.getValue('transaction_date'))}
        </div>
      ),
    },
    {
      accessorKey: 'description',
      header: 'Descrizione',
      cell: ({ row }) => (
        <div className="max-w-64 truncate" title={row.getValue('description')}>
          {row.getValue('description') || 'N/A'}
        </div>
      ),
    },
    {
      accessorKey: 'amount',
      header: ({ column }) => (
        <Button
          variant="ghost"
          onClick={() => column.toggleSorting(column.getIsSorted() === "asc")}
          className="h-8 p-0 font-medium"
        >
          Importo
          <ArrowUpDown className="ml-2 h-4 w-4" />
        </Button>
      ),
      cell: ({ row }) => {
        const amount = row.getValue('amount') as number;
        const isIncome = amount > 0;
        
        return (
          <div className={cn(
            'text-right font-medium flex items-center justify-end space-x-1',
            isIncome ? 'text-green-600' : 'text-red-600'
          )}>
            {isIncome ? (
              <TrendingUp className="h-3 w-3" />
            ) : (
              <TrendingDown className="h-3 w-3" />
            )}
            <span>{formatCurrency(Math.abs(amount))}</span>
          </div>
        );
      },
    },
    {
      accessorKey: 'reconciliation_status',
      header: 'Stato Riconciliazione',
      cell: ({ row }) => {
        const status = formatReconciliationStatus(row.getValue('reconciliation_status'));
        return (
          <Badge variant={status.variant} className="text-xs">
            {status.label}
          </Badge>
        );
      },
    },
    {
      accessorKey: 'remaining_amount',
      header: 'Residuo',
      cell: ({ row }) => {
        const remaining = row.original.remaining_amount || 0;
        const total = Math.abs(row.original.amount);
        const percentage = total > 0 ? ((total - Math.abs(remaining)) / total) * 100 : 0;
        
        return (
          <div className="space-y-1">
            <div className={cn(
              'text-right font-medium text-sm',
              remaining > 0 ? 'text-orange-600' : 'text-green-600'
            )}>
              {remaining > 0 ? formatCurrency(remaining) : '€ 0,00'}
            </div>
            {percentage > 0 && (
              <div className="w-full bg-muted rounded-full h-1">
                <div
                  className="bg-primary h-1 rounded-full transition-all"
                  style={{ width: `${percentage}%` }}
                />
              </div>
            )}
          </div>
        );
      },
    },
    {
      accessorKey: 'causale_abi',
      header: 'Causale ABI',
      cell: ({ row }) => {
        const causale = row.getValue('causale_abi');
        return causale ? (
          <Badge variant="outline" className="text-xs">
            {causale}
          </Badge>
        ) : null;
      },
    },
    {
      id: 'actions',
      cell: ({ row }) => (
        <DropdownMenu>
          <DropdownMenuTrigger asChild>
            <Button variant="ghost" className="h-8 w-8 p-0">
              <MoreHorizontal className="h-4 w-4" />
            </Button>
          </DropdownMenuTrigger>
          <DropdownMenuContent align="end">
            <DropdownMenuLabel>Azioni</DropdownMenuLabel>
            <DropdownMenuSeparator />
            <DropdownMenuItem asChild>
              <Link to={`/transactions/${row.original.id}`}>
                <Eye className="mr-2 h-4 w-4" />
                Visualizza
              </Link>
            </DropdownMenuItem>
            <DropdownMenuItem asChild>
              <Link to={`/reconciliation?transaction=${row.original.id}`}>
                <CheckCircle className="mr-2 h-4 w-4" />
                Riconcilia
              </Link>
            </DropdownMenuItem>
            <DropdownMenuSeparator />
            <DropdownMenuItem
              className="text-destructive"
              onClick={() => {
                setTransactionToDelete(row.original.id);
                setShowDeleteDialog(true);
              }}
            >
              <Trash className="mr-2 h-4 w-4" />
              Elimina
            </DropdownMenuItem>
          </DropdownMenuContent>
        </DropdownMenu>
      ),
    },
  ], []);

  // Setup tabella
  const table = useReactTable({
    data: transactionsData?.items || [],
    columns,
    onGlobalFilterChange: setGlobalFilter,
    getCoreRowModel: getCoreRowModel(),
    getFilteredRowModel: getFilteredRowModel(),
    getPaginationRowModel: getPaginationRowModel(),
    getSortedRowModel: getSortedRowModel(),
    onRowSelectionChange: (updater) => {
      const newSelection = typeof updater === 'function' 
        ? updater(selectedRows.reduce((acc, id) => ({ ...acc, [id]: true }), {}))
        : updater;
      setSelectedRows(Object.keys(newSelection).map(Number));
    },
    state: {
      globalFilter,
      rowSelection: selectedRows.reduce((acc, id) => ({ ...acc, [id]: true }), {}),
    },
  });

  // Handlers
  const handleFilterChange = (key: keyof TransactionFilters, value: any) => {
    setFilters(prev => ({ ...prev, [key]: value, page: 1 }));
  };

  const handleFileUpload = (event: React.ChangeEvent<HTMLInputElement>) => {
    const file = event.target.files?.[0];
    if (file) {
      importCSVMutation.mutate(file, {
        onSuccess: () => {
          setShowImportDialog(false);
          refetch();
          if (fileInputRef.current) {
            fileInputRef.current.value = '';
          }
        },
      });
    }
  };

  const handleExport = (format: 'excel' | 'csv' | 'json') => {
    exportMutation.mutate({
      format,
      filters: {
        ...filters,
        page: undefined, // Export all matching records
        size: undefined,
      },
    });
  };

  const handleDeleteTransaction = async () => {
    if (!transactionToDelete) return;
    
    try {
      await deleteTransactionMutation.mutateAsync(transactionToDelete);
      setShowDeleteDialog(false);
      setTransactionToDelete(null);
      refetch();
    } catch (error) {
      console.error('Delete error:', error);
    }
  };

  // Statistiche rapide
  const stats = useMemo(() => {
    if (!transactionsData?.items) return null;
    
    const transactions = transactionsData.items;
    const income = transactions.filter(t => t.amount > 0);
    const expenses = transactions.filter(t => t.amount < 0);
    
    return {
      total: transactions.length,
      totalIncome: income.reduce((sum, t) => sum + t.amount, 0),
      totalExpenses: Math.abs(expenses.reduce((sum, t) => sum + t.amount, 0)),
      reconciled: transactions.filter(t => t.reconciliation_status === 'Riconciliato Tot.').length,
      pending: transactions.filter(t => t.reconciliation_status === 'Da Riconciliare').length,
    };
  }, [transactionsData?.items]);

  return (
    <div className="space-y-6">
      {/* Header */}
      <div className="flex items-center justify-between">
        <div>
          <h1 className="text-3xl font-bold tracking-tight">Movimenti Bancari</h1>
          <p className="text-muted-foreground">
            Gestione transazioni e movimenti bancari
          </p>
        </div>
        <div className="flex items-center space-x-2">
          <Button 
            variant="outline" 
            size="sm"
            onClick={() => downloadTemplateMutation.mutate()}
            disabled={downloadTemplateMutation.isPending}
          >
            <FileSpreadsheet className="mr-2 h-4 w-4" />
            Template CSV
          </Button>
          <Button 
            variant="outline" 
            size="sm"
            onClick={() => setShowImportDialog(true)}
          >
            <Upload className="mr-2 h-4 w-4" />
            Importa CSV
          </Button>
          <DropdownMenu>
            <DropdownMenuTrigger asChild>
              <Button variant="outline" size="sm">
                <Download className="mr-2 h-4 w-4" />
                Esporta
              </Button>
            </DropdownMenuTrigger>
            <DropdownMenuContent>
              <DropdownMenuItem onClick={() => handleExport('excel')}>
                Excel (.xlsx)
              </DropdownMenuItem>
              <DropdownMenuItem onClick={() => handleExport('csv')}>
                CSV (.csv)
              </DropdownMenuItem>
              <DropdownMenuItem onClick={() => handleExport('json')}>
                JSON (.json)
              </DropdownMenuItem>
            </DropdownMenuContent>
          </DropdownMenu>
          <Button size="sm" asChild>
            <Link to="/transactions/new">
              <Plus className="mr-2 h-4 w-4" />
              Nuova Transazione
            </Link>
          </Button>
        </div>
      </div>

      {/* Stats Cards */}
      {stats && (
        <div className="grid gap-4 md:grid-cols-2 lg:grid-cols-5">
          <Card>
            <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
              <CardTitle className="text-sm font-medium">Totale</CardTitle>
              <CreditCard className="h-4 w-4 text-muted-foreground" />
            </CardHeader>
            <CardContent>
              <div className="text-2xl font-bold">{stats.total}</div>
              <p className="text-xs text-muted-foreground">transazioni</p>
            </CardContent>
          </Card>
          
          <Card>
            <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
              <CardTitle className="text-sm font-medium">Entrate</CardTitle>
              <TrendingUp className="h-4 w-4 text-green-600" />
            </CardHeader>
            <CardContent>
              <div className="text-2xl font-bold text-green-600">
                {formatCurrency(stats.totalIncome)}
              </div>
              <p className="text-xs text-muted-foreground">incassi totali</p>
            </CardContent>
          </Card>

          <Card>
            <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
              <CardTitle className="text-sm font-medium">Uscite</CardTitle>
              <TrendingDown className="h-4 w-4 text-red-600" />
            </CardHeader>
            <CardContent>
              <div className="text-2xl font-bold text-red-600">
                {formatCurrency(stats.totalExpenses)}
              </div>
              <p className="text-xs text-muted-foreground">pagamenti totali</p>
            </CardContent>
          </Card>

          <Card>
            <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
              <CardTitle className="text-sm font-medium">Riconciliate</CardTitle>
              <CheckCircle className="h-4 w-4 text-green-600" />
            </CardHeader>
            <CardContent>
              <div className="text-2xl font-bold text-green-600">{stats.reconciled}</div>
              <p className="text-xs text-muted-foreground">completate</p>
            </CardContent>
          </Card>

          <Card>
            <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
              <CardTitle className="text-sm font-medium">Da Riconciliare</CardTitle>
              <AlertTriangle className="h-4 w-4 text-orange-600" />
            </CardHeader>
            <CardContent>
              <div className="text-2xl font-bold text-orange-600">{stats.pending}</div>
              <p className="text-xs text-muted-foreground">in sospeso</p>
            </CardContent>
          </Card>
        </div>
      )}

      {/* Filtri e Ricerca */}
      <Card>
        <CardHeader>
          <CardTitle className="text-lg">Filtri</CardTitle>
          <CardDescription>
            Filtra e cerca tra le transazioni bancarie
          </CardDescription>
        </CardHeader>
        <CardContent>
          <div className="grid gap-4 md:grid-cols-2 lg:grid-cols-6">
            {/* Ricerca globale */}
            <div className="relative">
              <Search className="absolute left-3 top-1/2 h-4 w-4 -translate-y-1/2 text-muted-foreground" />
              <Input
                placeholder="Cerca transazioni..."
                value={globalFilter}
                onChange={(e) => setGlobalFilter(e.target.value)}
                className="pl-10"
              />
            </div>

            {/* Filtro stato riconciliazione */}
            <Select
              value={filters.status_filter || 'all'}
              onValueChange={(value) => 
                handleFilterChange('status_filter', value === 'all' ? undefined : value as ReconciliationStatus)
              }
            >
              <SelectTrigger>
                <SelectValue placeholder="Stato riconciliazione" />
              </SelectTrigger>
              <SelectContent>
                <SelectItem value="all">Tutti gli stati</SelectItem>
                <SelectItem value="Da Riconciliare">Da Riconciliare</SelectItem>
                <SelectItem value="Riconciliato Parz.">Parzialmente Riconciliate</SelectItem>
                <SelectItem value="Riconciliato Tot.">Completamente Riconciliate</SelectItem>
                <SelectItem value="Ignorato">Ignorate</SelectItem>
              </SelectContent>
            </Select>

            {/* Filtro importo minimo */}
            <Input
              type="number"
              placeholder="Importo min €"
              value={filters.min_amount || ''}
              onChange={(e) => 
                handleFilterChange('min_amount', e.target.value ? parseFloat(e.target.value) : undefined)
              }
            />

            {/* Filtro importo massimo */}
            <Input
              type="number"
              placeholder="Importo max €"
              value={filters.max_amount || ''}
              onChange={(e) => 
                handleFilterChange('max_amount', e.target.value ? parseFloat(e.target.value) : undefined)
              }
            />

            {/* Filtri speciali */}
            <div className="flex items-center space-x-2">
              <Checkbox
                id="hide_pos"
                checked={filters.hide_pos || false}
                onCheckedChange={(checked) => handleFilterChange('hide_pos', checked)}
              />
              <label htmlFor="hide_pos" className="text-sm">Nascondi POS</label>
            </div>

            <div className="flex items-center space-x-2">
              <Checkbox
                id="hide_commissions"
                checked={filters.hide_commissions || false}
                onCheckedChange={(checked) => handleFilterChange('hide_commissions', checked)}
              />
              <label htmlFor="hide_commissions" className="text-sm">Nascondi Commissioni</label>
            </div>
          </div>

          {/* Azioni bulk */}
          {selectedRows.length > 0 && (
            <div className="mt-4 p-4 bg-muted/50 rounded-lg">
              <div className="flex items-center justify-between">
                <p className="text-sm text-muted-foreground">
                  {selectedRows.length} transazioni selezionate
                </p>
                <div className="flex items-center space-x-2">
                  <Button
                    variant="outline"
                    size="sm"
                    onClick={() => {/* TODO: Bulk reconcile */}}
                  >
                    Riconcilia Selezionate
                  </Button>
                  <Button
                    variant="destructive"
                    size="sm"
                    onClick={() => bulkDelete.mutate(selectedRows)}
                    disabled={bulkDelete.isPending}
                  >
                    <Trash className="mr-2 h-4 w-4" />
                    Elimina
                  </Button>
                </div>
              </div>
            </div>
          )}
        </CardContent>
      </Card>

      {/* Tabella */}
      <Card>
        <CardHeader>
          <CardTitle>
            Elenco Transazioni
            {transactionsData?.total && (
              <span className="ml-2 text-sm font-normal text-muted-foreground">
                ({transactionsData.total} totali)
              </span>
            )}
          </CardTitle>
        </CardHeader>
        <CardContent>
          {/* Loading State */}
          {isLoading && (
            <div className="space-y-3">
              {Array.from({ length: 5 }).map((_, i) => (
                <div key={i} className="flex items-center space-x-4">
                  <Skeleton className="h-4 w-4" />
                  <Skeleton className="h-4 w-20" />
                  <Skeleton className="h-4 w-48" />
                  <Skeleton className="h-4 w-24" />
                  <Skeleton className="h-4 w-20" />
                  <Skeleton className="h-4 w-16" />
                  <Skeleton className="h-4 w-12" />
                </div>
              ))}
            </div>
          )}

          {/* Error State */}
          {error && (
            <div className="text-center py-8">
              <div className="text-destructive">
                <AlertTriangle className="h-8 w-8 mx-auto mb-2" />
                <p className="text-sm">Errore nel caricamento delle transazioni</p>
                <Button
                  variant="outline"
                  size="sm"
                  onClick={() => refetch()}
                  className="mt-2"
                >
                  Riprova
                </Button>
              </div>
            </div>
          )}

          {/* Tabella dati */}
          {!isLoading && !error && (
            <>
              <div className="rounded-md border">
                <Table>
                  <TableHeader>
                    {table.getHeaderGroups().map((headerGroup) => (
                      <TableRow key={headerGroup.id}>
                        {headerGroup.headers.map((header) => (
                          <TableHead key={header.id}>
                            {header.isPlaceholder
                              ? null
                              : flexRender(
                                  header.column.columnDef.header,
                                  header.getContext()
                                )}
                          </TableHead>
                        ))}
                      </TableRow>
                    ))}
                  </TableHeader>
                  <TableBody>
                    {table.getRowModel().rows?.length ? (
                      table.getRowModel().rows.map((row) => (
                        <TableRow
                          key={row.id}
                          data-state={row.getIsSelected() && "selected"}
                          className="hover:bg-muted/50"
                        >
                          {row.getVisibleCells().map((cell) => (
                            <TableCell key={cell.id}>
                              {flexRender(
                                cell.column.columnDef.cell,
                                cell.getContext()
                              )}
                            </TableCell>
                          ))}
                        </TableRow>
                      ))
                    ) : (
                      <TableRow>
                        <TableCell
                          colSpan={columns.length}
                          className="h-24 text-center"
                        >
                          <div className="text-muted-foreground">
                            <CreditCard className="h-8 w-8 mx-auto mb-2 opacity-50" />
                            <p>Nessuna transazione trovata</p>
                            <p className="text-xs mt-1">
                              Prova a modificare i filtri o importa movimenti bancari
                            </p>
                          </div>
                        </TableCell>
                      </TableRow>
                    )}
                  </TableBody>
                </Table>
              </div>

              {/* Paginazione */}
              <div className="flex items-center justify-between space-x-2 py-4">
                <div className="text-sm text-muted-foreground">
                  {selectedRows.length > 0 && (
                    <span>{selectedRows.length} di {table.getFilteredRowModel().rows.length} righe selezionate. </span>
                  )}
                  Mostra {table.getState().pagination.pageSize} di {table.getFilteredRowModel().rows.length} risultati
                </div>
                <div className="flex items-center space-x-2">
                  <Button
                    variant="outline"
                    size="sm"
                    onClick={() => table.previousPage()}
                    disabled={!table.getCanPreviousPage()}
                  >
                    Precedente
                  </Button>
                  <div className="flex w-[100px] items-center justify-center text-sm font-medium">
                    Pagina {table.getState().pagination.pageIndex + 1} di{' '}
                    {table.getPageCount()}
                  </div>
                  <Button
                    variant="outline"
                    size="sm"
                    onClick={() => table.nextPage()}
                    disabled={!table.getCanNextPage()}
                  >
                    Successiva
                  </Button>
                </div>
              </div>
            </>
          )}
        </CardContent>
      </Card>

      {/* Dialog Import CSV */}
      <Dialog open={showImportDialog} onOpenChange={setShowImportDialog}>
        <DialogContent>
          <DialogHeader>
            <DialogTitle>Importa Movimenti Bancari</DialogTitle>
            <DialogDescription>
              Seleziona un file CSV contenente i movimenti bancari da importare.
              Il file deve seguire il formato del template scaricabile.
            </DialogDescription>
          </DialogHeader>
          <div className="space-y-4">
            <div className="border-2 border-dashed border-muted-foreground/25 rounded-lg p-6 text-center">
              <input
                ref={fileInputRef}
                type="file"
                accept=".csv"
                onChange={handleFileUpload}
                className="hidden"
                id="csv-upload"
              />
              <label
                htmlFor="csv-upload"
                className="cursor-pointer flex flex-col items-center space-y-2"
              >
                <Upload className="h-8 w-8 text-muted-foreground" />
                <div>
                  <p className="font-medium">Clicca per selezionare un file CSV</p>
                  <p className="text-sm text-muted-foreground">o trascina qui il file</p>
                </div>
              </label>
            </div>
            <div className="flex justify-between">
              <Button
                variant="outline"
                onClick={() => downloadTemplateMutation.mutate()}
                disabled={downloadTemplateMutation.isPending}
              >
                <FileSpreadsheet className="mr-2 h-4 w-4" />
                Scarica Template
              </Button>
              <Button
                variant="outline"
                onClick={() => setShowImportDialog(false)}
              >
                Annulla
              </Button>
            </div>
            {importCSVMutation.isPending && (
              <div className="text-center">
                <div className="animate-spin rounded-full h-6 w-6 border-b-2 border-primary mx-auto"></div>
                <p className="text-sm text-muted-foreground mt-2">Importazione in corso...</p>
              </div>
            )}
          </div>
        </DialogContent>
      </Dialog>

      {/* Dialog conferma eliminazione */}
      <Dialog open={showDeleteDialog} onOpenChange={setShowDeleteDialog}>
        <DialogContent>
          <DialogHeader>
            <DialogTitle>Conferma eliminazione</DialogTitle>
            <DialogDescription>
              Sei sicuro di voler eliminare questa transazione? Questa azione non può essere annullata.
              {transactionToDelete && (
                <div className="mt-2 p-2 bg-muted rounded text-sm">
                  ID Transazione: {transactionToDelete}
                </div>
              )}
            </DialogDescription>
          </DialogHeader>
          <div className="flex justify-end space-x-2">
            <Button
              variant="outline"
              onClick={() => setShowDeleteDialog(false)}
            >
              Annulla
            </Button>
            <Button
              variant="destructive"
              onClick={handleDeleteTransaction}
              disabled={deleteTransactionMutation.isPending}
            >
              {deleteTransactionMutation.isPending ? 'Eliminazione...' : 'Elimina'}
            </Button>
          </div>
        </DialogContent>
      </Dialog>
    </div>
  );
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/pages/TransactionsPage.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/providers/AuthProvider.tsx ---
// Path: frontend/src/providers/AuthProvider.tsx
import React, { createContext, useState, useContext, ReactNode, useEffect } from 'react';

interface User { id: string; username: string; }
interface AuthContextType { user: User | null; isAuthenticated: boolean; login: (userData: User, token: string) => Promise<void>; logout: () => Promise<void>; isLoading: boolean; }
const AuthContext = createContext<AuthContextType | undefined>(undefined);

export function AuthProvider({ children }: { children: ReactNode }) {
  const [user, setUser] = useState<User | null>(null);
  const [isLoading, setIsLoading] = useState(true);

  useEffect(() => {
    const checkSession = async () => {
      setIsLoading(true);
      // Example: const token = localStorage.getItem('authToken'); if (token) { /* Validate token, fetch user */ }
      setIsLoading(false);
    };
    checkSession();
  }, []);

  const login = async (userData: User, token: string) => { setUser(userData); /* localStorage.setItem('authToken', token); */ };

  const logout = async () => { setUser(null); /* localStorage.removeItem('authToken'); */ };

  return (
    <AuthContext.Provider value={{ user, isAuthenticated: !!user, login, logout, isLoading }}>
      {children}
    </AuthContext.Provider>
  );
}

export const useAuth = () => {
  const context = useContext(AuthContext);
  if (!context) { throw new Error('useAuth must be used within an AuthProvider'); }
  return context;
};

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/providers/AuthProvider.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/providers/QueryClientProvider.tsx ---
// Path: frontend/src/providers/QueryClientProvider.tsx
import React, { ReactNode } from 'react';
// import { QueryClient, QueryClientProvider as RQProvider } from '@tanstack/react-query';

// const queryClient = new QueryClient();

export function QueryClientProviderComponent({ children }: { children: ReactNode }) {
  // return <RQProvider client={queryClient}>{children}</RQProvider>;
  return <>{children}</>; // Placeholder
}

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/providers/QueryClientProvider.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/providers/ThemeProvider.tsx ---
import React, { createContext, useContext, useEffect, useState } from 'react';
import { useUIStore } from '@/store';
import type { Theme } from '@/types';

interface ThemeProviderProps {
  children: React.ReactNode;
  defaultTheme?: Theme;
  storageKey?: string;
}

interface ThemeProviderState {
  theme: Theme;
  setTheme: (theme: Theme) => void;
}

const initialState: ThemeProviderState = {
  theme: 'system',
  setTheme: () => null,
};

const ThemeProviderContext = createContext<ThemeProviderState>(initialState);

export function ThemeProvider({
  children,
  defaultTheme = 'system',
  storageKey = 'fattura-analyzer-theme',
  ...props
}: ThemeProviderProps) {
  const { theme: storeTheme, setTheme: setStoreTheme } = useUIStore();
  const [theme, setTheme] = useState<Theme>(storeTheme || defaultTheme);

  useEffect(() => {
    const root = window.document.documentElement;

    root.classList.remove('light', 'dark');

    if (theme === 'system') {
      const systemTheme = window.matchMedia('(prefers-color-scheme: dark)')
        .matches
        ? 'dark'
        : 'light';

      root.classList.add(systemTheme);
      return;
    }

    root.classList.add(theme);
  }, [theme]);

  // Listen to system theme changes
  useEffect(() => {
    if (theme === 'system') {
      const mediaQuery = window.matchMedia('(prefers-color-scheme: dark)');
      
      const handleChange = () => {
        const root = window.document.documentElement;
        root.classList.remove('light', 'dark');
        root.classList.add(mediaQuery.matches ? 'dark' : 'light');
      };

      mediaQuery.addEventListener('change', handleChange);
      return () => mediaQuery.removeEventListener('change', handleChange);
    }
  }, [theme]);

  // Sync with store
  useEffect(() => {
    if (storeTheme !== theme) {
      setTheme(storeTheme);
    }
  }, [storeTheme]);

  const handleSetTheme = (newTheme: Theme) => {
    setTheme(newTheme);
    setStoreTheme(newTheme);
  };

  const value = {
    theme,
    setTheme: handleSetTheme,
  };

  return (
    <ThemeProviderContext.Provider {...props} value={value}>
      {children}
    </ThemeProviderContext.Provider>
  );
}

export const useTheme = () => {
  const context = useContext(ThemeProviderContext);

  if (context === undefined)
    throw new Error('useTheme must be used within a ThemeProvider');

  return context;
};
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/providers/ThemeProvider.tsx ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/providers/index.ts ---
// Path: frontend/src/providers/index.ts
// Barrel file for context providers.
// Add your exports here, e.g.:
// export * from './ThemeProvider';

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/providers/index.ts ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/services/api.ts ---
/**
 * API Client for FatturaAnalyzer Backend
 * Configurato per comunicare con FastAPI backend
 */

import { Invoice, BankTransaction, Anagraphics, APIResponse } from '@/types';

const API_BASE_URL = import.meta.env.VITE_API_URL || 'http://127.0.0.1:8000';

export interface PaginationParams {
  page?: number;
  size?: number;
}

export interface DateRangeFilter {
  start_date?: string;
  end_date?: string;
}

export interface InvoiceFilters extends PaginationParams {
  type_filter?: 'Attiva' | 'Passiva';
  status_filter?: string;
  anagraphics_id?: number;
  search?: string;
  start_date?: string;
  end_date?: string;
  min_amount?: number;
  max_amount?: number;
}

export interface TransactionFilters extends PaginationParams {
  status_filter?: string;
  search?: string;
  start_date?: string;
  end_date?: string;
  min_amount?: number;
  max_amount?: number;
  hide_pos?: boolean;
  hide_worldline?: boolean;
  hide_cash?: boolean;
  hide_commissions?: boolean;
}

export interface AnagraphicsFilters extends PaginationParams {
  type_filter?: 'Cliente' | 'Fornitore';
  search?: string;
  city?: string;
  province?: string;
}

class ApiClient {
  private baseURL: string;

  constructor(baseURL: string = API_BASE_URL) {
    this.baseURL = baseURL;
  }

  private async request<T>(
    endpoint: string,
    options: RequestInit = {}
  ): Promise<T> {
    const url = `${this.baseURL}${endpoint}`;
    
    const defaultHeaders = {
      'Content-Type': 'application/json',
    };

    const response = await fetch(url, {
      headers: { ...defaultHeaders, ...options.headers },
      ...options,
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw new Error(errorData.detail || `HTTP ${response.status}: ${response.statusText}`);
    }

    return response.json();
  }

  private buildQuery(params: Record<string, any>): string {
    const searchParams = new URLSearchParams();
    
    Object.entries(params).forEach(([key, value]) => {
      if (value !== undefined && value !== null && value !== '') {
        searchParams.append(key, String(value));
      }
    });

    return searchParams.toString();
  }

  // Health Check
  async healthCheck(): Promise<{ status: string; version: string }> {
    return this.request('/health');
  }

  // Anagraphics API
  async getAnagraphics(filters: AnagraphicsFilters = {}) {
    const query = this.buildQuery(filters);
    return this.request(`/api/anagraphics?${query}`);
  }

  async getAnagraphicsById(id: number): Promise<Anagraphics> {
    return this.request(`/api/anagraphics/${id}`);
  }

  async createAnagraphics(data: Partial<Anagraphics>): Promise<Anagraphics> {
    return this.request('/api/anagraphics', {
      method: 'POST',
      body: JSON.stringify(data),
    });
  }

  async updateAnagraphics(id: number, data: Partial<Anagraphics>): Promise<Anagraphics> {
    return this.request(`/api/anagraphics/${id}`, {
      method: 'PUT',
      body: JSON.stringify(data),
    });
  }

  async deleteAnagraphics(id: number): Promise<APIResponse> {
    return this.request(`/api/anagraphics/${id}`, {
      method: 'DELETE',
    });
  }

  async searchAnagraphics(query: string, type_filter?: string): Promise<APIResponse> {
    const params = this.buildQuery({ type_filter });
    return this.request(`/api/anagraphics/search/${encodeURIComponent(query)}?${params}`);
  }

  async getAnagraphicsFinancialSummary(id: number): Promise<APIResponse> {
    return this.request(`/api/anagraphics/${id}/financial-summary`);
  }

  // Invoices API
  async getInvoices(filters: InvoiceFilters = {}) {
    const query = this.buildQuery(filters);
    return this.request(`/api/invoices?${query}`);
  }

  async getInvoiceById(id: number): Promise<Invoice> {
    return this.request(`/api/invoices/${id}`);
  }

  async createInvoice(data: Partial<Invoice>): Promise<Invoice> {
    return this.request('/api/invoices', {
      method: 'POST',
      body: JSON.stringify(data),
    });
  }

  async updateInvoice(id: number, data: Partial<Invoice>): Promise<Invoice> {
    return this.request(`/api/invoices/${id}`, {
      method: 'PUT',
      body: JSON.stringify(data),
    });
  }

  async deleteInvoice(id: number): Promise<APIResponse> {
    return this.request(`/api/invoices/${id}`, {
      method: 'DELETE',
    });
  }

  async getOverdueInvoices(limit: number = 20): Promise<APIResponse> {
    return this.request(`/api/invoices/overdue/list?limit=${limit}`);
  }

  async getAgingSummary(invoice_type: 'Attiva' | 'Passiva' = 'Attiva'): Promise<APIResponse> {
    return this.request(`/api/invoices/aging/summary?invoice_type=${invoice_type}`);
  }

  async searchInvoices(query: string, type_filter?: string): Promise<APIResponse> {
    const params = this.buildQuery({ type_filter });
    return this.request(`/api/invoices/search/${encodeURIComponent(query)}?${params}`);
  }

  async updateInvoicePaymentStatus(
    id: number, 
    payment_status: string, 
    paid_amount?: number
  ): Promise<APIResponse> {
    const params = this.buildQuery({ payment_status, paid_amount });
    return this.request(`/api/invoices/${id}/update-payment-status?${params}`, {
      method: 'POST',
    });
  }

  // Transactions API
  async getTransactions(filters: TransactionFilters = {}) {
    const query = this.buildQuery(filters);
    return this.request(`/api/transactions?${query}`);
  }

  async getTransactionById(id: number): Promise<BankTransaction> {
    return this.request(`/api/transactions/${id}`);
  }

  async createTransaction(data: Partial<BankTransaction>): Promise<BankTransaction> {
    return this.request('/api/transactions', {
      method: 'POST',
      body: JSON.stringify(data),
    });
  }

  async updateTransaction(id: number, data: Partial<BankTransaction>): Promise<BankTransaction> {
    return this.request(`/api/transactions/${id}`, {
      method: 'PUT',
      body: JSON.stringify(data),
    });
  }

  async deleteTransaction(id: number): Promise<APIResponse> {
    return this.request(`/api/transactions/${id}`, {
      method: 'DELETE',
    });
  }

  async getTransactionPotentialMatches(id: number, limit: number = 10): Promise<APIResponse> {
    return this.request(`/api/transactions/${id}/potential-matches?limit=${limit}`);
  }

  async updateTransactionStatus(
    id: number, 
    reconciliation_status: string, 
    reconciled_amount?: number
  ): Promise<APIResponse> {
    const body = { reconciliation_status, reconciled_amount };
    return this.request(`/api/transactions/${id}/update-status`, {
      method: 'POST',
      body: JSON.stringify(body),
    });
  }

  async batchUpdateTransactionStatus(
    transaction_ids: number[], 
    reconciliation_status: string
  ): Promise<APIResponse> {
    return this.request('/api/transactions/batch/update-status', {
      method: 'POST',
      body: JSON.stringify({ transaction_ids, reconciliation_status }),
    });
  }

  async searchTransactions(query: string, include_reconciled: boolean = false): Promise<APIResponse> {
    return this.request(`/api/transactions/search/${encodeURIComponent(query)}?include_reconciled=${include_reconciled}`);
  }

  async getTransactionStats(): Promise<APIResponse> {
    return this.request('/api/transactions/stats/summary');
  }

  async getCashFlowAnalysis(months: number = 12): Promise<APIResponse> {
    return this.request(`/api/transactions/analysis/cash-flow?months=${months}`);
  }

  // Reconciliation API
  async getReconciliationSuggestions(
    max_suggestions: number = 50, 
    confidence_threshold: number = 0.5
  ): Promise<APIResponse> {
    const params = this.buildQuery({ max_suggestions, confidence_threshold });
    return this.request(`/api/reconciliation/suggestions?${params}`);
  }

  async getReconciliationOpportunities(
    limit: number = 20, 
    amount_tolerance: number = 0.01
  ): Promise<APIResponse> {
    const params = this.buildQuery({ limit, amount_tolerance });
    return this.request(`/api/reconciliation/opportunities?${params}`);
  }

  async performReconciliation(
    invoice_id: number, 
    transaction_id: number, 
    amount: number
  ): Promise<APIResponse> {
    return this.request('/api/reconciliation/reconcile', {
      method: 'POST',
      body: JSON.stringify({ invoice_id, transaction_id, amount }),
    });
  }

  async performBatchReconciliation(reconciliations: Array<{
    invoice_id: number;
    transaction_id: number;
    amount: number;
  }>): Promise<APIResponse> {
    return this.request('/api/reconciliation/reconcile/batch', {
      method: 'POST',
      body: JSON.stringify(reconciliations),
    });
  }

  async autoReconcile(
    confidence_threshold: number = 0.8, 
    max_auto_reconcile: number = 10
  ): Promise<APIResponse> {
    const params = this.buildQuery({ confidence_threshold, max_auto_reconcile });
    return this.request(`/api/reconciliation/auto-reconcile?${params}`, {
      method: 'POST',
    });
  }

  async undoInvoiceReconciliation(invoice_id: number): Promise<APIResponse> {
    return this.request(`/api/reconciliation/undo/invoice/${invoice_id}`, {
      method: 'DELETE',
    });
  }

  async undoTransactionReconciliation(transaction_id: number): Promise<APIResponse> {
    return this.request(`/api/reconciliation/undo/transaction/${transaction_id}`, {
      method: 'DELETE',
    });
  }

  async getReconciliationStatus(): Promise<APIResponse> {
    return this.request('/api/reconciliation/status');
  }

  async getReconciliationLinks(
    invoice_id?: number, 
    transaction_id?: number, 
    limit: number = 50, 
    offset: number = 0
  ): Promise<APIResponse> {
    const params = this.buildQuery({ invoice_id, transaction_id, limit, offset });
    return this.request(`/api/reconciliation/links?${params}`);
  }

  async validateReconciliationMatch(
    invoice_id: number, 
    transaction_id: number, 
    amount: number
  ): Promise<APIResponse> {
    return this.request('/api/reconciliation/validate-match', {
      method: 'POST',
      body: JSON.stringify({ invoice_id, transaction_id, amount }),
    });
  }

  async getReconciliationAnalytics(): Promise<APIResponse> {
    return this.request('/api/reconciliation/analytics/summary');
  }

  // Analytics API
  async getKPIs(): Promise<APIResponse> {
    return this.request('/api/analytics/kpis');
  }

  async getDashboardData(): Promise<APIResponse> {
    return this.request('/api/analytics/dashboard');
  }

  async getMonthlyRevenue(months: number = 12, invoice_type?: string): Promise<APIResponse> {
    const params = this.buildQuery({ months, invoice_type });
    return this.request(`/api/analytics/revenue/monthly?${params}`);
  }

  async getTopClients(limit: number = 20, period_months: number = 12): Promise<APIResponse> {
    const params = this.buildQuery({ limit, period_months });
    return this.request(`/api/analytics/clients/top?${params}`);
  }

  async getProductAnalysis(limit: number = 50, period_months: number = 12): Promise<APIResponse> {
    const params = this.buildQuery({ limit, period_months });
    return this.request(`/api/analytics/products?${params}`);
  }

  async getCashFlowForecast(months_ahead: number = 6): Promise<APIResponse> {
    return this.request(`/api/analytics/cash-flow/forecast?months_ahead=${months_ahead}`);
  }

  async getYearOverYearComparison(metric: string = 'revenue', current_year?: number): Promise<APIResponse> {
    const params = this.buildQuery({ metric, current_year });
    return this.request(`/api/analytics/year-over-year?${params}`);
  }

  // Import/Export API
  async importInvoicesXML(files: FileList): Promise<APIResponse> {
    const formData = new FormData();
    Array.from(files).forEach(file => {
      formData.append('files', file);
    });

    return this.request('/api/import/invoices/xml', {
      method: 'POST',
      body: formData,
      headers: {}, // Let browser set Content-Type for FormData
    });
  }

  async importInvoicesZip(file: File): Promise<APIResponse> {
    const formData = new FormData();
    formData.append('file', file);

    return this.request('/api/import/invoices/zip', {
      method: 'POST',
      body: formData,
      headers: {},
    });
  }

  async importTransactionsCSV(file: File): Promise<APIResponse> {
    const formData = new FormData();
    formData.append('file', file);

    return this.request('/api/import/transactions/csv', {
      method: 'POST',
      body: formData,
      headers: {},
    });
  }

  async validateFile(file: File): Promise<APIResponse> {
    const formData = new FormData();
    formData.append('file', file);

    return this.request('/api/import/validate-file', {
      method: 'POST',
      body: formData,
      headers: {},
    });
  }

  async downloadTransactionTemplate(): Promise<Blob> {
    const response = await fetch(`${this.baseURL}/api/import/templates/transactions-csv`);
    if (!response.ok) {
      throw new Error('Failed to download template');
    }
    return response.blob();
  }

  async exportInvoices(
    format: 'excel' | 'csv' | 'json' = 'excel',
    filters: any = {}
  ): Promise<Blob | APIResponse> {
    const params = this.buildQuery({ format, ...filters });
    const response = await fetch(`${this.baseURL}/api/import/export/invoices?${params}`);
    
    if (!response.ok) {
      throw new Error('Export failed');
    }

    if (format === 'json') {
      return response.json();
    } else {
      return response.blob();
    }
  }

  async exportTransactions(
    format: 'excel' | 'csv' | 'json' = 'excel',
    filters: any = {}
  ): Promise<Blob | APIResponse> {
    const params = this.buildQuery({ format, ...filters });
    const response = await fetch(`${this.baseURL}/api/import/export/transactions?${params}`);
    
    if (!response.ok) {
      throw new Error('Export failed');
    }

    if (format === 'json') {
      return response.json();
    } else {
      return response.blob();
    }
  }

  async createBackup(): Promise<Blob> {
    const response = await fetch(`${this.baseURL}/api/import/backup/create`, {
      method: 'POST',
    });
    
    if (!response.ok) {
      throw new Error('Backup creation failed');
    }

    return response.blob();
  }

  async getImportHistory(limit: number = 50): Promise<APIResponse> {
    return this.request(`/api/import/status/import-history?limit=${limit}`);
  }

  // Cloud Sync API
  async getSyncStatus(): Promise<APIResponse> {
    return this.request('/api/sync/status');
  }

  async performSync(): Promise<APIResponse> {
    return this.request('/api/sync/upload', {
      method: 'POST',
    });
  }

  async downloadFromCloud(): Promise<APIResponse> {
    return this.request('/api/sync/download', {
      method: 'POST',
    });
  }

  async enableAutoSync(): Promise<APIResponse> {
    return this.request('/api/sync/auto/enable', {
      method: 'POST',
    });
  }

  async disableAutoSync(): Promise<APIResponse> {
    return this.request('/api/sync/auto/disable', {
      method: 'POST',
    });
  }
}

// Singleton instance
export const apiClient = new ApiClient();

// Default export
export default apiClient;
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/services/api.ts ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/store/index.ts ---
/**
 * Zustand Store Configuration
 * State management principale per l'applicazione
 */

import { create } from 'zustand';
import { devtools, persist } from 'zustand/middleware';
import { immer } from 'zustand/middleware/immer';
import { subscribeWithSelector } from 'zustand/middleware';

import type { 
  Theme, 
  AppSettings, 
  Notification, 
  LoadingState, 
  ErrorState,
  DashboardData,
  Invoice,
  BankTransaction,
  Anagraphics
} from '@/types';

// UI Store - gestisce stato UI globale
interface UIState {
  // Theme e impostazioni
  theme: Theme;
  sidebarCollapsed: boolean;
  
  // Loading states
  loading: LoadingState;
  
  // Error states  
  errors: ErrorState;
  
  // Notifications
  notifications: Notification[];
  
  // Modal states
  modals: {
    [key: string]: boolean;
  };
  
  // Settings
  settings: AppSettings;
  
  // Actions
  setTheme: (theme: Theme) => void;
  toggleSidebar: () => void;
  setSidebarCollapsed: (collapsed: boolean) => void;
  setLoading: (key: string, loading: boolean) => void;
  setError: (key: string, error: string | null) => void;
  clearErrors: () => void;
  addNotification: (notification: Omit<Notification, 'id'>) => void;
  removeNotification: (id: string) => void;
  clearNotifications: () => void;
  openModal: (key: string) => void;
  closeModal: (key: string) => void;
  updateSettings: (settings: Partial<AppSettings>) => void;
}

export const useUIStore = create<UIState>()(
  devtools(
    persist(
      subscribeWithSelector(
        immer((set, get) => ({
          // Initial state
          theme: 'system',
          sidebarCollapsed: false,
          loading: {},
          errors: {},
          notifications: [],
          modals: {},
          settings: {
            theme: 'system',
            language: 'it',
            currency: 'EUR',
            date_format: 'dd/MM/yyyy',
            number_format: 'it-IT',
            pagination_size: 50,
            auto_sync: false,
            notifications: {
              email: true,
              desktop: true,
              sounds: false,
            },
          },
          
          // Actions
          setTheme: (theme) => set((state) => {
            state.theme = theme;
            state.settings.theme = theme;
          }),
          
          toggleSidebar: () => set((state) => {
            state.sidebarCollapsed = !state.sidebarCollapsed;
          }),
          
          setSidebarCollapsed: (collapsed) => set((state) => {
            state.sidebarCollapsed = collapsed;
          }),
          
          setLoading: (key, loading) => set((state) => {
            if (loading) {
              state.loading[key] = true;
            } else {
              delete state.loading[key];
            }
          }),
          
          setError: (key, error) => set((state) => {
            if (error) {
              state.errors[key] = error;
            } else {
              delete state.errors[key];
            }
          }),
          
          clearErrors: () => set((state) => {
            state.errors = {};
          }),
          
          addNotification: (notification) => set((state) => {
            const id = Date.now().toString() + Math.random().toString(36);
            state.notifications.push({
              ...notification,
              id,
            });
          }),
          
          removeNotification: (id) => set((state) => {
            state.notifications = state.notifications.filter(n => n.id !== id);
          }),
          
          clearNotifications: () => set((state) => {
            state.notifications = [];
          }),
          
          openModal: (key) => set((state) => {
            state.modals[key] = true;
          }),
          
          closeModal: (key) => set((state) => {
            state.modals[key] = false;
          }),
          
          updateSettings: (newSettings) => set((state) => {
            state.settings = { ...state.settings, ...newSettings };
          }),
        }))
      ),
      {
        name: 'fattura-analyzer-ui',
        partialize: (state) => ({
          theme: state.theme,
          sidebarCollapsed: state.sidebarCollapsed,
          settings: state.settings,
        }),
      }
    ),
    { name: 'UIStore' }
  )
);

// Data Store - gestisce dati di business
interface DataState {
  // Dashboard data
  dashboardData: DashboardData | null;
  dashboardLastUpdated: string | null;
  
  // Cached data
  invoices: {
    data: Invoice[];
    total: number;
    lastFetch: string | null;
  };
  
  transactions: {
    data: BankTransaction[];
    total: number;
    lastFetch: string | null;
  };
  
  anagraphics: {
    data: Anagraphics[];
    total: number;
    lastFetch: string | null;
  };
  
  // Recently viewed items
  recentInvoices: Invoice[];
  recentTransactions: BankTransaction[];
  recentAnagraphics: Anagraphics[];
  
  // Actions
  setDashboardData: (data: DashboardData) => void;
  setInvoices: (data: Invoice[], total: number) => void;
  setTransactions: (data: BankTransaction[], total: number) => void;
  setAnagraphics: (data: Anagraphics[], total: number) => void;
  addRecentInvoice: (invoice: Invoice) => void;
  addRecentTransaction: (transaction: BankTransaction) => void;
  addRecentAnagraphics: (anagraphics: Anagraphics) => void;
  updateInvoice: (id: number, updates: Partial<Invoice>) => void;
  updateTransaction: (id: number, updates: Partial<BankTransaction>) => void;
  updateAnagraphics: (id: number, updates: Partial<Anagraphics>) => void;
  removeInvoice: (id: number) => void;
  removeTransaction: (id: number) => void;
  removeAnagraphics: (id: number) => void;
  clearCache: () => void;
}

export const useDataStore = create<DataState>()(
  devtools(
    subscribeWithSelector(
      immer((set, get) => ({
        // Initial state
        dashboardData: null,
        dashboardLastUpdated: null,
        invoices: {
          data: [],
          total: 0,
          lastFetch: null,
        },
        transactions: {
          data: [],
          total: 0,
          lastFetch: null,
        },
        anagraphics: {
          data: [],
          total: 0,
          lastFetch: null,
        },
        recentInvoices: [],
        recentTransactions: [],
        recentAnagraphics: [],
        
        // Actions
        setDashboardData: (data) => set((state) => {
          state.dashboardData = data;
          state.dashboardLastUpdated = new Date().toISOString();
        }),
        
        setInvoices: (data, total) => set((state) => {
          state.invoices.data = data;
          state.invoices.total = total;
          state.invoices.lastFetch = new Date().toISOString();
        }),
        
        setTransactions: (data, total) => set((state) => {
          state.transactions.data = data;
          state.transactions.total = total;
          state.transactions.lastFetch = new Date().toISOString();
        }),
        
        setAnagraphics: (data, total) => set((state) => {
          state.anagraphics.data = data;
          state.anagraphics.total = total;
          state.anagraphics.lastFetch = new Date().toISOString();
        }),
        
        addRecentInvoice: (invoice) => set((state) => {
          // Remove if already exists
          state.recentInvoices = state.recentInvoices.filter(i => i.id !== invoice.id);
          // Add to beginning
          state.recentInvoices.unshift(invoice);
          // Keep only 10 recent items
          state.recentInvoices = state.recentInvoices.slice(0, 10);
        }),
        
        addRecentTransaction: (transaction) => set((state) => {
          state.recentTransactions = state.recentTransactions.filter(t => t.id !== transaction.id);
          state.recentTransactions.unshift(transaction);
          state.recentTransactions = state.recentTransactions.slice(0, 10);
        }),
        
        addRecentAnagraphics: (anagraphics) => set((state) => {
          state.recentAnagraphics = state.recentAnagraphics.filter(a => a.id !== anagraphics.id);
          state.recentAnagraphics.unshift(anagraphics);
          state.recentAnagraphics = state.recentAnagraphics.slice(0, 10);
        }),
        
        updateInvoice: (id, updates) => set((state) => {
          // Update in main list
          const index = state.invoices.data.findIndex(i => i.id === id);
          if (index !== -1) {
            state.invoices.data[index] = { ...state.invoices.data[index], ...updates };
          }
          
          // Update in recent list
          const recentIndex = state.recentInvoices.findIndex(i => i.id === id);
          if (recentIndex !== -1) {
            state.recentInvoices[recentIndex] = { ...state.recentInvoices[recentIndex], ...updates };
          }
          
          // Update in dashboard if present
          if (state.dashboardData?.recent_invoices) {
            const dashIndex = state.dashboardData.recent_invoices.findIndex(i => i.id === id);
            if (dashIndex !== -1) {
              state.dashboardData.recent_invoices[dashIndex] = { 
                ...state.dashboardData.recent_invoices[dashIndex], 
                ...updates 
              };
            }
          }
        }),
        
        updateTransaction: (id, updates) => set((state) => {
          const index = state.transactions.data.findIndex(t => t.id === id);
          if (index !== -1) {
            state.transactions.data[index] = { ...state.transactions.data[index], ...updates };
          }
          
          const recentIndex = state.recentTransactions.findIndex(t => t.id === id);
          if (recentIndex !== -1) {
            state.recentTransactions[recentIndex] = { ...state.recentTransactions[recentIndex], ...updates };
          }
          
          if (state.dashboardData?.recent_transactions) {
            const dashIndex = state.dashboardData.recent_transactions.findIndex(t => t.id === id);
            if (dashIndex !== -1) {
              state.dashboardData.recent_transactions[dashIndex] = { 
                ...state.dashboardData.recent_transactions[dashIndex], 
                ...updates 
              };
            }
          }
        }),
        
        updateAnagraphics: (id, updates) => set((state) => {
          const index = state.anagraphics.data.findIndex(a => a.id === id);
          if (index !== -1) {
            state.anagraphics.data[index] = { ...state.anagraphics.data[index], ...updates };
          }
          
          const recentIndex = state.recentAnagraphics.findIndex(a => a.id === id);
          if (recentIndex !== -1) {
            state.recentAnagraphics[recentIndex] = { ...state.recentAnagraphics[recentIndex], ...updates };
          }
        }),
        
        removeInvoice: (id) => set((state) => {
          state.invoices.data = state.invoices.data.filter(i => i.id !== id);
          state.invoices.total = Math.max(0, state.invoices.total - 1);
          state.recentInvoices = state.recentInvoices.filter(i => i.id !== id);
          
          if (state.dashboardData?.recent_invoices) {
            state.dashboardData.recent_invoices = state.dashboardData.recent_invoices.filter(i => i.id !== id);
          }
        }),
        
        removeTransaction: (id) => set((state) => {
          state.transactions.data = state.transactions.data.filter(t => t.id !== id);
          state.transactions.total = Math.max(0, state.transactions.total - 1);
          state.recentTransactions = state.recentTransactions.filter(t => t.id !== id);
          
          if (state.dashboardData?.recent_transactions) {
            state.dashboardData.recent_transactions = state.dashboardData.recent_transactions.filter(t => t.id !== id);
          }
        }),
        
        removeAnagraphics: (id) => set((state) => {
          state.anagraphics.data = state.anagraphics.data.filter(a => a.id !== id);
          state.anagraphics.total = Math.max(0, state.anagraphics.total - 1);
          state.recentAnagraphics = state.recentAnagraphics.filter(a => a.id !== id);
        }),
        
        clearCache: () => set((state) => {
          state.invoices = { data: [], total: 0, lastFetch: null };
          state.transactions = { data: [], total: 0, lastFetch: null };
          state.anagraphics = { data: [], total: 0, lastFetch: null };
          state.dashboardData = null;
          state.dashboardLastUpdated = null;
        }),
      }))
    ),
    { name: 'DataStore' }
  )
);

// Reconciliation Store - gestisce stato riconciliazione
interface ReconciliationState {
  // Current reconciliation session
  selectedInvoices: Invoice[];
  selectedTransactions: BankTransaction[];
  suggestions: any[];
  opportunities: any[];
  
  // Drag & drop state
  draggedItem: { type: 'invoice' | 'transaction'; data: Invoice | BankTransaction } | null;
  dropTarget: { type: 'invoice' | 'transaction'; id: number } | null;
  
  // Reconciliation history
  recentReconciliations: any[];
  
  // Actions
  addSelectedInvoice: (invoice: Invoice) => void;
  removeSelectedInvoice: (id: number) => void;
  addSelectedTransaction: (transaction: BankTransaction) => void;
  removeSelectedTransaction: (id: number) => void;
  clearSelection: () => void;
  setSuggestions: (suggestions: any[]) => void;
  setOpportunities: (opportunities: any[]) => void;
  setDraggedItem: (item: { type: 'invoice' | 'transaction'; data: Invoice | BankTransaction } | null) => void;
  setDropTarget: (target: { type: 'invoice' | 'transaction'; id: number } | null) => void;
  addRecentReconciliation: (reconciliation: any) => void;
  clearReconciliationState: () => void;
}

export const useReconciliationStore = create<ReconciliationState>()(
  devtools(
    immer((set, get) => ({
      // Initial state
      selectedInvoices: [],
      selectedTransactions: [],
      suggestions: [],
      opportunities: [],
      draggedItem: null,
      dropTarget: null,
      recentReconciliations: [],
      
      // Actions
      addSelectedInvoice: (invoice) => set((state) => {
        if (!state.selectedInvoices.find(i => i.id === invoice.id)) {
          state.selectedInvoices.push(invoice);
        }
      }),
      
      removeSelectedInvoice: (id) => set((state) => {
        state.selectedInvoices = state.selectedInvoices.filter(i => i.id !== id);
      }),
      
      addSelectedTransaction: (transaction) => set((state) => {
        if (!state.selectedTransactions.find(t => t.id === transaction.id)) {
          state.selectedTransactions.push(transaction);
        }
      }),
      
      removeSelectedTransaction: (id) => set((state) => {
        state.selectedTransactions = state.selectedTransactions.filter(t => t.id !== id);
      }),
      
      clearSelection: () => set((state) => {
        state.selectedInvoices = [];
        state.selectedTransactions = [];
      }),
      
      setSuggestions: (suggestions) => set((state) => {
        state.suggestions = suggestions;
      }),
      
      setOpportunities: (opportunities) => set((state) => {
        state.opportunities = opportunities;
      }),
      
      setDraggedItem: (item) => set((state) => {
        state.draggedItem = item;
      }),
      
      setDropTarget: (target) => set((state) => {
        state.dropTarget = target;
      }),
      
      addRecentReconciliation: (reconciliation) => set((state) => {
        state.recentReconciliations.unshift(reconciliation);
        state.recentReconciliations = state.recentReconciliations.slice(0, 20);
      }),
      
      clearReconciliationState: () => set((state) => {
        state.selectedInvoices = [];
        state.selectedTransactions = [];
        state.draggedItem = null;
        state.dropTarget = null;
        state.suggestions = [];
        state.opportunities = [];
      }),
    })),
    { name: 'ReconciliationStore' }
  )
);

// Export stores and selectors
export { useUIStore, useDataStore, useReconciliationStore };

// Utility selectors
export const useTheme = () => useUIStore(state => state.theme);
export const useLoading = (key?: string) => useUIStore(state => 
  key ? state.loading[key] || false : Object.keys(state.loading).length > 0
);
export const useError = (key?: string) => useUIStore(state => 
  key ? state.errors[key] || null : Object.values(state.errors).filter(Boolean)[0] || null
);
export const useNotifications = () => useUIStore(state => state.notifications);
export const useSettings = () => useUIStore(state => state.settings);

// Data selectors
export const useDashboardData = () => useDataStore(state => state.dashboardData);
export const useInvoicesCache = () => useDataStore(state => state.invoices);
export const useTransactionsCache = () => useDataStore(state => state.transactions);
export const useAnagraphicsCache = () => useDataStore(state => state.anagraphics);
export const useRecentItems = () => useDataStore(state => ({
  invoices: state.recentInvoices,
  transactions: state.recentTransactions,
  anagraphics: state.recentAnagraphics,
}));

// Reconciliation selectors
export const useReconciliationSelection = () => useReconciliationStore(state => ({
  invoices: state.selectedInvoices,
  transactions: state.selectedTransactions,
}));
export const useReconciliationSuggestions = () => useReconciliationStore(state => state.suggestions);
export const useReconciliationOpportunities = () => useReconciliationStore(state => state.opportunities);
export const useDragAndDrop = () => useReconciliationStore(state => ({
  draggedItem: state.draggedItem,
  dropTarget: state.dropTarget,
}));
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/store/index.ts ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/styles/components.css ---

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/styles/components.css ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/styles/globals.css ---

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/styles/globals.css ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/types/index.ts ---
/**
 * TypeScript type definitions for FatturaAnalyzer Frontend
 * Sincronizzate con i modelli Pydantic del backend
 */

// Base types
export type AnagraphicsType = "Cliente" | "Fornitore";
export type InvoiceType = "Attiva" | "Passiva";
export type PaymentStatus = "Aperta" | "Scaduta" | "Pagata Parz." | "Pagata Tot." | "Insoluta" | "Riconciliata";
export type ReconciliationStatus = "Da Riconciliare" | "Riconciliato Parz." | "Riconciliato Tot." | "Riconciliato Eccesso" | "Ignorato";

// API Response wrapper
export interface APIResponse<T = any> {
  success: boolean;
  message: string;
  data?: T;
  timestamp: string;
}

export interface ErrorResponse {
  success: false;
  error: string;
  message: string;
  timestamp: string;
}

// Pagination
export interface PaginationParams {
  page: number;
  size: number;
}

export interface PaginatedResponse<T> {
  items: T[];
  total: number;
  page: number;
  size: number;
  pages: number;
}

// Anagraphics
export interface AnagraphicsBase {
  type: AnagraphicsType;
  piva?: string;
  cf?: string;
  denomination: string;
  address?: string;
  cap?: string;
  city?: string;
  province?: string;
  country: string;
  iban?: string;
  email?: string;
  phone?: string;
  pec?: string;
  codice_destinatario?: string;
}

export interface AnagraphicsCreate extends AnagraphicsBase {}

export interface AnagraphicsUpdate extends Partial<AnagraphicsBase> {}

export interface Anagraphics extends AnagraphicsBase {
  id: number;
  score: number;
  created_at: string;
  updated_at: string;
}

// Invoice Lines
export interface InvoiceLineBase {
  line_number: number;
  description?: string;
  quantity?: number;
  unit_measure?: string;
  unit_price?: number;
  total_price: number;
  vat_rate: number;
  item_code?: string;
  item_type?: string;
}

export interface InvoiceLineCreate extends InvoiceLineBase {}

export interface InvoiceLine extends InvoiceLineBase {
  id: number;
  invoice_id: number;
}

// Invoice VAT Summary
export interface InvoiceVATSummaryBase {
  vat_rate: number;
  taxable_amount: number;
  vat_amount: number;
}

export interface InvoiceVATSummaryCreate extends InvoiceVATSummaryBase {}

export interface InvoiceVATSummary extends InvoiceVATSummaryBase {
  id: number;
  invoice_id: number;
}

// Invoices
export interface InvoiceBase {
  anagraphics_id: number;
  type: InvoiceType;
  doc_type?: string;
  doc_number: string;
  doc_date: string; // ISO date string
  total_amount: number;
  due_date?: string; // ISO date string
  payment_method?: string;
  notes?: string;
  xml_filename?: string;
  p7m_source_file?: string;
}

export interface InvoiceCreate extends InvoiceBase {
  lines?: InvoiceLineCreate[];
  vat_summary?: InvoiceVATSummaryCreate[];
}

export interface InvoiceUpdate extends Partial<InvoiceBase> {
  payment_status?: PaymentStatus;
  paid_amount?: number;
}

export interface Invoice extends InvoiceBase {
  id: number;
  payment_status: PaymentStatus;
  paid_amount: number;
  unique_hash: string;
  created_at: string;
  updated_at: string;
  
  // Computed fields
  open_amount?: number;
  counterparty_name?: string;
  
  // Relations
  lines: InvoiceLine[];
  vat_summary: InvoiceVATSummary[];
}

// Bank Transactions
export interface BankTransactionBase {
  transaction_date: string; // ISO date string
  value_date?: string; // ISO date string
  amount: number;
  description?: string;
  causale_abi?: number;
}

export interface BankTransactionCreate extends BankTransactionBase {
  unique_hash: string;
}

export interface BankTransactionUpdate extends Partial<BankTransactionBase> {
  reconciliation_status?: ReconciliationStatus;
  reconciled_amount?: number;
}

export interface BankTransaction extends BankTransactionBase {
  id: number;
  reconciliation_status: ReconciliationStatus;
  reconciled_amount: number;
  unique_hash: string;
  created_at: string;
  updated_at?: string;
  
  // Computed fields
  remaining_amount?: number;
  is_income?: boolean;
  is_expense?: boolean;
}

// Reconciliation
export interface ReconciliationLinkBase {
  transaction_id: number;
  invoice_id: number;
  reconciled_amount: number;
  notes?: string;
}

export interface ReconciliationLinkCreate extends ReconciliationLinkBase {}

export interface ReconciliationLink extends ReconciliationLinkBase {
  id: number;
  reconciliation_date: string;
  created_at: string;
}

export interface ReconciliationSuggestion {
  confidence: "Alta" | "Media" | "Bassa";
  confidence_score: number;
  invoice_ids: number[];
  transaction_ids?: number[];
  description: string;
  total_amount: number;
  match_details?: Record<string, any>;
  reasons?: string[];
}

export interface ReconciliationRequest {
  invoice_id: number;
  transaction_id: number;
  amount: number;
}

export interface ReconciliationBatchRequest {
  invoice_ids: number[];
  transaction_ids: number[];
}

// Analytics
export interface KPIData {
  total_receivables: number;
  total_payables: number;
  overdue_receivables_count: number;
  overdue_receivables_amount: number;
  overdue_payables_count: number;
  overdue_payables_amount: number;
  revenue_ytd: number;
  revenue_prev_year_ytd: number;
  revenue_yoy_change_ytd?: number;
  gross_margin_ytd: number;
  margin_percent_ytd?: number;
  avg_days_to_payment?: number;
  inventory_turnover_estimate?: number;
  active_customers_month: number;
  new_customers_month: number;
}

export interface CashFlowData {
  month: string;
  incassi_clienti: number;
  incassi_contanti: number;
  altri_incassi: number;
  pagamenti_fornitori: number;
  spese_carte: number;
  carburanti: number;
  trasporti: number;
  utenze: number;
  tasse_tributi: number;
  commissioni_bancarie: number;
  altri_pagamenti: number;
  net_operational_flow: number;
  total_inflows: number;
  total_outflows: number;
  net_cash_flow: number;
}

export interface MonthlyRevenueData {
  month: string;
  revenue: number;
  cost: number;
  gross_margin: number;
  margin_percent: number;
}

export interface TopClientData {
  id: number;
  denomination: string;
  total_revenue: number;
  num_invoices: number;
  score: number;
  avg_order_value: number;
  last_order_date?: string;
}

export interface ProductAnalysisData {
  normalized_product: string;
  total_quantity: number;
  total_value: number;
  num_invoices: number;
  avg_unit_price: number;
  original_descriptions: string[];
}

export interface AgingBucket {
  label: string;
  amount: number;
  count: number;
}

export interface AgingSummary {
  buckets: AgingBucket[];
  total_amount: number;
  total_count: number;
}

// Import/Export
export interface ImportResult {
  processed: number;
  success: number;
  duplicates: number;
  errors: number;
  unsupported: number;
  files: Array<{ name: string; status: string }>;
}

export interface FileUploadResponse {
  filename: string;
  size: number;
  content_type: string;
  status: string;
  message?: string;
}

// Cloud Sync
export interface SyncStatus {
  enabled: boolean;
  service_available: boolean;
  remote_file_id?: string;
  last_sync_time?: string;
  auto_sync_running: boolean;
}

export interface SyncResult {
  success: boolean;
  action?: string;
  message: string;
  timestamp: string;
}

// Filter types
export interface DateRangeFilter {
  start_date?: string;
  end_date?: string;
}

export interface AnagraphicsFilter {
  type?: AnagraphicsType;
  search?: string;
  city?: string;
  province?: string;
}

export interface InvoiceFilter {
  type?: InvoiceType;
  status?: PaymentStatus;
  anagraphics_id?: number;
  search?: string;
  date_range?: DateRangeFilter;
  min_amount?: number;
  max_amount?: number;
}

export interface TransactionFilter {
  status?: ReconciliationStatus;
  search?: string;
  date_range?: DateRangeFilter;
  min_amount?: number;
  max_amount?: number;
  anagraphics_id_heuristic?: number;
  hide_pos?: boolean;
  hide_worldline?: boolean;
  hide_cash?: boolean;
  hide_commissions?: boolean;
}

// Dashboard
export interface DashboardKPIs {
  total_receivables: number;
  total_payables: number;
  overdue_receivables_amount: number;
  overdue_receivables_count: number;
  overdue_payables_amount: number;
  overdue_payables_count: number;
  revenue_ytd: number;
  revenue_prev_year_ytd: number;
  revenue_yoy_change_ytd?: number;
  gross_margin_ytd: number;
  margin_percent_ytd?: number;
  avg_days_to_payment?: number;
  inventory_turnover_estimate?: number;
  active_customers_month: number;
  new_customers_month: number;
}

export interface DashboardData {
  kpis: DashboardKPIs;
  recent_invoices: Invoice[];
  recent_transactions: BankTransaction[];
  cash_flow_summary: CashFlowData[];
  top_clients: TopClientData[];
  overdue_invoices: Invoice[];
}

// Search
export interface SearchResult {
  type: "invoice" | "transaction" | "anagraphics";
  id: number;
  title: string;
  subtitle: string;
  amount?: number;
  date?: string;
  status?: string;
}

export interface SearchResponse {
  query: string;
  results: SearchResult[];
  total: number;
  took_ms: number;
}

// UI State types
export interface TableState {
  pagination: {
    pageIndex: number;
    pageSize: number;
  };
  sorting: Array<{
    id: string;
    desc: boolean;
  }>;
  columnFilters: Array<{
    id: string;
    value: any;
  }>;
  globalFilter: string;
}

export interface LoadingState {
  [key: string]: boolean;
}

export interface ErrorState {
  [key: string]: string | null;
}

// Form types
export interface AnagraphicsFormData extends Omit<AnagraphicsCreate, 'country'> {
  country?: string;
}

export interface InvoiceFormData extends Omit<InvoiceCreate, 'doc_date' | 'due_date'> {
  doc_date: Date;
  due_date?: Date;
}

export interface TransactionFormData extends Omit<BankTransactionCreate, 'transaction_date' | 'value_date'> {
  transaction_date: Date;
  value_date?: Date;
}

// Theme types
export type Theme = "light" | "dark" | "system";

// Navigation types
export interface NavigationItem {
  title: string;
  href: string;
  icon?: string;
  badge?: string | number;
  children?: NavigationItem[];
}

// Notification types
export interface Notification {
  id: string;
  type: "info" | "success" | "warning" | "error";
  title: string;
  message?: string;
  duration?: number;
  action?: {
    label: string;
    onClick: () => void;
  };
}

// Chart data types
export interface ChartDataPoint {
  name: string;
  value: number;
  label?: string;
  color?: string;
}

export interface TimeSeriesDataPoint {
  date: string;
  value: number;
  label?: string;
}

export interface MultiSeriesDataPoint {
  name: string;
  [key: string]: string | number;
}

// Settings types
export interface AppSettings {
  theme: Theme;
  language: string;
  currency: string;
  date_format: string;
  number_format: string;
  pagination_size: number;
  auto_sync: boolean;
  notifications: {
    email: boolean;
    desktop: boolean;
    sounds: boolean;
  };
}

// Hook return types
export interface UseApiResult<T> {
  data: T | null;
  loading: boolean;
  error: string | null;
  refetch: () => Promise<void>;
}

export interface UseMutationResult<T, V = any> {
  mutate: (variables: V) => Promise<T>;
  loading: boolean;
  error: string | null;
  reset: () => void;
}

// File upload types
export interface UploadProgress {
  loaded: number;
  total: number;
  percentage: number;
}

export interface FileWithProgress extends File {
  progress?: UploadProgress;
  status?: "pending" | "uploading" | "success" | "error";
  error?: string;
}

// Export commonly used types
export type {
  AnagraphicsType,
  InvoiceType,
  PaymentStatus,
  ReconciliationStatus,
  Theme,
};
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/types/index.ts ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/utils/constants.ts ---
// Path: frontend/src/utils/constants.ts

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/utils/constants.ts ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/utils/formatters.ts ---
// Path: frontend/src/utils/formatters.ts

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/utils/formatters.ts ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/src/utils/validators.ts ---
// Path: frontend/src/utils/validators.ts

--- Fine contenuto di: FatturaAnalyzer-v2/frontend/src/utils/validators.ts ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/tailwind.config.js ---
/** @type {import('tailwindcss').Config} */
module.exports = {
  darkMode: ["class"],
  content: [
    "./pages/**/*.{ts,tsx}",
    "./components/**/*.{ts,tsx}",
    "./app/**/*.{ts,tsx}",
    "./src/**/*.{ts,tsx}",
  ],
  theme: {
    container: {
      center: true,
      padding: "2rem",
      screens: {
        "2xl": "1400px",
      },
    },
    extend: {
      colors: {
        border: "hsl(var(--border))",
        input: "hsl(var(--input))",
        ring: "hsl(var(--ring))",
        background: "hsl(var(--background))",
        foreground: "hsl(var(--foreground))",
        primary: {
          DEFAULT: "hsl(var(--primary))",
          foreground: "hsl(var(--primary-foreground))",
        },
        secondary: {
          DEFAULT: "hsl(var(--secondary))",
          foreground: "hsl(var(--secondary-foreground))",
        },
        destructive: {
          DEFAULT: "hsl(var(--destructive))",
          foreground: "hsl(var(--destructive-foreground))",
        },
        muted: {
          DEFAULT: "hsl(var(--muted))",
          foreground: "hsl(var(--muted-foreground))",
        },
        accent: {
          DEFAULT: "hsl(var(--accent))",
          foreground: "hsl(var(--accent-foreground))",
        },
        popover: {
          DEFAULT: "hsl(var(--popover))",
          foreground: "hsl(var(--popover-foreground))",
        },
        card: {
          DEFAULT: "hsl(var(--card))",
          foreground: "hsl(var(--card-foreground))",
        },
      },
      borderRadius: {
        lg: "var(--radius)",
        md: "calc(var(--radius) - 2px)",
        sm: "calc(var(--radius) - 4px)",
      },
      keyframes: {
        "accordion-down": {
          from: { height: 0 },
          to: { height: "var(--radix-accordion-content-height)" },
        },
        "accordion-up": {
          from: { height: "var(--radix-accordion-content-height)" },
          to: { height: 0 },
        },
        "fade-in": {
          "0%": { opacity: 0, transform: "translateY(10px)" },
          "100%": { opacity: 1, transform: "translateY(0)" },
        },
        "slide-in": {
          "0%": { transform: "translateX(-100%)" },
          "100%": { transform: "translateX(0)" },
        },
      },
      animation: {
        "accordion-down": "accordion-down 0.2s ease-out",
        "accordion-up": "accordion-up 0.2s ease-out",
        "fade-in": "fade-in 0.3s ease-out",
        "slide-in": "slide-in 0.3s ease-out",
      },
    },
  },
  plugins: [require("tailwindcss-animate")],
};
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/tailwind.config.js ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/tsconfig.json ---
{
  "compilerOptions": {
    "target": "ESNext",
"useDefineForClassFields": true,
"lib": ["DOM", "DOM.Iterable", "ESNext"],
    "allowJs": false,
"skipLibCheck": true,
"esModuleInterop": true,
"allowSyntheticDefaultImports": true,
    "strict": true,
"forceConsistentCasingInFileNames": true,
"module": "ESNext",
    "moduleResolution": "Bundler",
"resolveJsonModule": true,
"isolatedModules": true,
    "noEmit": true,
"jsx": "react-jsx",
"baseUrl": ".",
"paths": {
      "@/*": ["./src/*"]
    }
  },
  "include": ["src", "vite.config.ts", "tailwind.config.js", "postcss.config.js"],
  "references": [{ "path": "./tsconfig.node.json" }]
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/tsconfig.json ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/tsconfig.node.json ---
{
  "compilerOptions": {
    "composite": true,
"skipLibCheck": true,
"module": "ESNext",
    "moduleResolution": "bundler",
"allowSyntheticDefaultImports": true
  },
  "include": ["vite.config.ts"]
}
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/tsconfig.node.json ---

--- Contenuto di: FatturaAnalyzer-v2/frontend/vite.config.ts ---
import { defineConfig } from "vite";
import react from "@vitejs/plugin-react";

// https://vitejs.dev/config/
export default defineConfig(async () => ({
  plugins: [react()],

  // Vite options tailored for Tauri development and only applied in `tauri dev` or `tauri build`
  //
  // 1. prevent vite from obscuring rust errors
  clearScreen: false,
  // 2. tauri expects a fixed port, fail if that port is not available
  server: {
    port: 1420,
    strictPort: true,
    watch: {
      // 3. tell vite to ignore watching `src-tauri`
      ignored: ["**/src-tauri/**"],
    },
  },
  // 3. to make use of `TAURI_DEBUG` and other env variables
  // https://tauri.studio/v1/api/config#buildconfig.beforedevcommand
  envPrefix: ["VITE_", "TAURI_"],
  
  resolve: {
    alias: {
      "@": "/src",
    },
  },
}));
--- Fine contenuto di: FatturaAnalyzer-v2/frontend/vite.config.ts ---

--- Contenuto di: FatturaAnalyzer-v2/package.json ---
{
  "name": "fatturaanalyzer-v2-monorepo",
  "version": "0.1.0",
"private": true,
  "description": "Main package for FatturaAnalyzer v2.",
  "scripts": {
    "dev:backend": "cd backend && python run.py",
    "dev:frontend": "cd frontend && npm run dev",
    "dev": "echo \"Run dev scripts or use ./scripts/dev.sh\"",
    "build": "echo \"Run build scripts or use ./scripts/build.sh\"",
    "tauri:dev": "cd src-tauri && cargo tauri dev",
    "tauri:build": "cd src-tauri && cargo tauri build"
  },
  "workspaces": ["backend", "frontend"]
}
--- Fine contenuto di: FatturaAnalyzer-v2/package.json ---

--- Contenuto di: FatturaAnalyzer-v2/scripts/build.sh ---

--- Fine contenuto di: FatturaAnalyzer-v2/scripts/build.sh ---

--- Contenuto di: FatturaAnalyzer-v2/scripts/dev.sh ---

--- Fine contenuto di: FatturaAnalyzer-v2/scripts/dev.sh ---

--- Contenuto di: FatturaAnalyzer-v2/scripts/release.sh ---

--- Fine contenuto di: FatturaAnalyzer-v2/scripts/release.sh ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/Cargo.toml ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/Cargo.toml ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/build.rs ---
// Path: src-tauri/build.rs

--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/build.rs ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/icons/128x128.png ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/icons/128x128.png ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/icons/32x32.png ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/icons/32x32.png ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/icons/Square107x107Logo.png ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/icons/Square107x107Logo.png ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/icons/Square150x150Logo.png ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/icons/Square150x150Logo.png ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/icons/Square24x24Logo.png ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/icons/Square24x24Logo.png ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/icons/Square284x284Logo.png ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/icons/Square284x284Logo.png ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/icons/Square30x30Logo.png ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/icons/Square30x30Logo.png ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/icons/Square310x310Logo.png ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/icons/Square310x310Logo.png ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/icons/Square44x44Logo.png ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/icons/Square44x44Logo.png ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/icons/Square70x70Logo.png ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/icons/Square70x70Logo.png ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/icons/StoreLogo.png ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/icons/StoreLogo.png ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/icons/icon.icns ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/icons/icon.icns ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/icons/icon.ico ---
[INFO] File binario o estensione non testuale, contenuto non mostrato.
--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/icons/icon.ico ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/src/commands.rs ---
// Path: src-tauri/src/commands.rs

--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/src/commands.rs ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/src/lib.rs ---
// Path: src-tauri/src/lib.rs

--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/src/lib.rs ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/src/main.rs ---
// Path: src-tauri/src/main.rs

--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/src/main.rs ---

--- Contenuto di: FatturaAnalyzer-v2/src-tauri/tauri.conf.json ---
{
  "$schema": "../node_modules/@tauri-apps/cli/schema.json",
  "build": {
    "beforeDevCommand": "npm run dev --prefix ../frontend",
    "beforeBuildCommand": "npm run build --prefix ../frontend",
    "devPath": "http://localhost:1420",
"distDir": "../frontend/dist",
"withGlobalTauri": true
  },
  "package": {
    "productName": "FatturaAnalyzer-v2",
"version": "0.1.0"
  },
  "tauri": {
    "allowlist": {
      "all": false,
"shell": { "open": true },
      "http": { "all": true, "scope": ["http://localhost:8000/*"] },
      "dialog": { "all": true },
"fs": { "all": true, "scope": ["$APPDATA/*", "$DOCUMENT/*"] },
      "path": { "all": true }
    },
    "bundle": {
      "active": true,
"targets": "all",
"identifier": "com.example.fatturaanalyzer",
      "icon": ["icons/32x32.png", "icons/128x128.png", "icons/icon.icns", "icons/icon.ico"]
    },
    "security": { "csp": null },
    "windows": [{
      "fullscreen": false,
"resizable": true,
"title": "FatturaAnalyzer-v2",
      "width": 1200,
"height": 800,
"minWidth": 800,
"minHeight": 600
    }]
  }
}
--- Fine contenuto di: FatturaAnalyzer-v2/src-tauri/tauri.conf.json ---

--- Contenuto di: create.py ---
import os
from pathlib import Path

BASE_DIR = "FatturaAnalyzer-v2"

# Struttura delle directory e dei file completa
structure = [
    # --- BACKEND ---
    (f"{BASE_DIR}/backend/app", ["__init__.py", "main.py", "config.py"]),
    (f"{BASE_DIR}/backend/app/models", ["__init__.py", "invoice.py", "anagraphics.py", "transaction.py", "reconciliation.py"]),
    (f"{BASE_DIR}/backend/app/api", ["__init__.py", "invoices.py", "anagraphics.py", "transactions.py", "reconciliation.py", "analytics.py", "import_export.py", "sync.py"]),
    (f"{BASE_DIR}/backend/app/adapters", ["__init__.py", "database_adapter.py", "reconciliation_adapter.py", "analytics_adapter.py", "importer_adapter.py"]),
    (f"{BASE_DIR}/backend/app/core", ["__init__.py", "database.py", "utils.py", "analysis.py", "reconciliation.py", "smart_client_reconciliation.py", "parser_xml.py", "parser_p7m.py", "parser_csv.py", "importer.py", "cloud_sync.py"]),
    (f"{BASE_DIR}/backend/app/middleware", ["__init__.py", "auth.py", "cors.py", "error_handler.py"]),
    (f"{BASE_DIR}/backend", ["Dockerfile", "requirements.txt", "run.py"]),

    # --- FRONTEND ---
    (f"{BASE_DIR}/frontend/src/components/ui", [
        "index.ts", "button.tsx", "card.tsx", "input.tsx", "table.tsx", "dialog.tsx",
        "badge.tsx", "skeleton.tsx", "dropdown-menu.tsx", "tooltip.tsx",
        "select.tsx", "label.tsx", "checkbox.tsx", "sonner.tsx",
    ]),
    (f"{BASE_DIR}/frontend/src/components/layout", ["Header.tsx", "Sidebar.tsx", "Layout.tsx"]),
    (f"{BASE_DIR}/frontend/src/components/dashboard", [
        "KPICards.tsx", "Charts.tsx", "RevenueChart.tsx", "CashFlowChart.tsx",
        "TopClientsTable.tsx", "RecentActivity.tsx", "OverdueInvoices.tsx", "DashboardView.tsx"
    ]),
    (f"{BASE_DIR}/frontend/src/components/invoices", ["InvoiceList.tsx", "InvoiceDetail.tsx", "InvoiceForm.tsx"]),
    (f"{BASE_DIR}/frontend/src/components/transactions", ["TransactionList.tsx", "TransactionImport.tsx"]),
    (f"{BASE_DIR}/frontend/src/components/reconciliation", ["ReconciliationView.tsx", "MatchSuggestions.tsx", "DragDropReconciliation.tsx", "ReconciliationActions.tsx"]),
    (f"{BASE_DIR}/frontend/src/components/anagraphics", ["AnagraphicsList.tsx", "AnagraphicsForm.tsx"]),
    (f"{BASE_DIR}/frontend/src/components/analytics", ["ReportsView.tsx", "ChartsLibrary.tsx", "ExportTools.tsx"]),

    (f"{BASE_DIR}/frontend/src/hooks", ["useApi.ts", "useInvoices.ts", "useTransactions.ts", "useReconciliation.ts", "useAnalytics.ts"]),
    
    (f"{BASE_DIR}/frontend/src/providers", [ # Cartella providers aggiunta
        "index.ts", "ThemeProvider.tsx", "AuthProvider.tsx", "QueryClientProvider.tsx"
    ]),

    (f"{BASE_DIR}/frontend/src/services", ["api.ts"]),
    (f"{BASE_DIR}/frontend/src/store", ["index.ts"]),
    (f"{BASE_DIR}/frontend/src/types", ["index.ts"]),
    (f"{BASE_DIR}/frontend/src/utils", ["formatters.ts", "validators.ts", "constants.ts"]),
    (f"{BASE_DIR}/frontend/src/lib", ["utils.ts", "validations.ts"]),
    (f"{BASE_DIR}/frontend/src/pages", [
        "DashboardPage.tsx", "InvoicesPage.tsx", "InvoiceDetailPage.tsx",
        "TransactionsPage.tsx", "TransactionDetailPage.tsx",
        "ReconciliationPage.tsx", "AnagraphicsPage.tsx", "AnagraphicsDetailPage.tsx",
        "AnalyticsPage.tsx", "ImportExportPage.tsx", "SettingsPage.tsx"
    ]),
    (f"{BASE_DIR}/frontend/src/styles", ["globals.css", "components.css"]),
    (f"{BASE_DIR}/frontend/src", ["App.tsx", "main.tsx", "vite-env.d.ts"]),
    (f"{BASE_DIR}/frontend/public/icons", []),
    (f"{BASE_DIR}/frontend/public", ["favicon.ico"]),
    (f"{BASE_DIR}/frontend", ["package.json", "vite.config.ts", "tailwind.config.js", "postcss.config.js", "tsconfig.json", "tsconfig.node.json"]),

    # --- SRC-TAURI ---
    (f"{BASE_DIR}/src-tauri/src", ["main.rs", "commands.rs", "lib.rs"]),
    (f"{BASE_DIR}/src-tauri/icons", [
        "32x32.png", "128x128.png", "icon.icns", "icon.ico",
        "Square107x107Logo.png", "Square24x24Logo.png", "Square30x30Logo.png",
        "Square44x44Logo.png", "Square70x70Logo.png", "Square150x150Logo.png",
        "Square284x284Logo.png", "Square310x310Logo.png", "StoreLogo.png"
    ]),
    (f"{BASE_DIR}/src-tauri", ["Cargo.toml", "tauri.conf.json", "build.rs"]),

    # --- SCRIPTS, DOCS, CONFIG (Root Level rispetto a BASE_DIR) ---
    (f"{BASE_DIR}/scripts", ["build.sh", "dev.sh", "release.sh"]),
    (f"{BASE_DIR}/docs", ["API.md", "SETUP.md", "DEPLOYMENT.md"]),
    (f"{BASE_DIR}/config", ["development.ini", "production.ini", "database_schema.sql"]),
    (f"{BASE_DIR}", [".env.example", ".gitignore", "README.md", "package.json"]),
]

files_to_overwrite_if_exists = [
    (".gitignore", BASE_DIR),
    (".env.example", BASE_DIR),
    ("README.md", BASE_DIR),
    ("package.json", BASE_DIR),
    ("requirements.txt", f"{BASE_DIR}/backend"),
    ("Dockerfile", f"{BASE_DIR}/backend"),
    ("package.json", f"{BASE_DIR}/frontend"),
    ("vite.config.ts", f"{BASE_DIR}/frontend"),
    ("tailwind.config.js", f"{BASE_DIR}/frontend"),
    ("postcss.config.js", f"{BASE_DIR}/frontend"),
    ("tsconfig.json", f"{BASE_DIR}/frontend"),
    ("tsconfig.node.json", f"{BASE_DIR}/frontend"),
    ("Cargo.toml", f"{BASE_DIR}/src-tauri"),
    ("tauri.conf.json", f"{BASE_DIR}/src-tauri"),
    ("build.rs", f"{BASE_DIR}/src-tauri"),
    ("main.rs", f"{BASE_DIR}/src-tauri/src"),
    # Considera se aggiungere file da providers/ se vuoi che i loro scheletri siano sovrascritti
    ("index.ts", f"{BASE_DIR}/frontend/src/providers"),
    # ("ThemeProvider.tsx", f"{BASE_DIR}/frontend/src/providers"), # Descommenta se vuoi sovrascrivere
    # ("AuthProvider.tsx", f"{BASE_DIR}/frontend/src/providers"),   # Descommenta se vuoi sovrascrivere
]

def get_file_content(file_name, dir_path_obj, base_path_obj, relative_file_path_str):
    content = ""
    # --- INIZIO LOGICA CONTENUTO FILE (identica alle versioni precedenti, inclusi i providers) ---
    if file_name.endswith((".py", ".pyw")):
        content = f"# Path: {relative_file_path_str}\n\n"
        if file_name == "__init__.py":
            content += "# This file makes Python treat the directory as a package.\n"
    elif file_name.endswith((".ts", ".tsx", ".js", ".jsx")):
        if file_name == "index.ts" and "components/ui" in str(relative_file_path_str):
            content = (f"// Path: {relative_file_path_str}\n// Barrel file for UI components.\n"
                       "// Add your exports here, e.g.:\n// export * from './button';\n")
        elif file_name == "index.ts" and "providers" in str(relative_file_path_str) and "frontend/src" in str(relative_file_path_str):
             content = (f"// Path: {relative_file_path_str}\n// Barrel file for context providers.\n"
                        "// Add your exports here, e.g.:\n// export * from './ThemeProvider';\n")
        elif file_name == "ThemeProvider.tsx" and "providers" in str(relative_file_path_str):
            content = (
                f"// Path: {relative_file_path_str}\n"
                "import React, { createContext, useState, useContext, useMemo, ReactNode } from 'react';\n\n"
                "type Theme = 'light' | 'dark';\n"
                "interface ThemeContextType { theme: Theme; setTheme: (theme: Theme) => void; toggleTheme: () => void; }\n"
                "const ThemeContext = createContext<ThemeContextType | undefined>(undefined);\n"
                "interface ThemeProviderProps { children: ReactNode; defaultTheme?: Theme; storageKey?: string; }\n\n"
                "export function ThemeProvider({ children, defaultTheme = 'system', storageKey = 'vite-ui-theme', ...props }: ThemeProviderProps) {\n"
                "  const [theme, setThemeState] = useState<Theme>(() => {\n"
                "    try {\n      const storedTheme = localStorage.getItem(storageKey) as Theme | null;\n"
                "      if (storedTheme) return storedTheme;\n"
                "      return defaultTheme === 'system' ? 'light' : defaultTheme;\n"
                "    } catch (e) { return defaultTheme === 'system' ? 'light' : defaultTheme; }\n  });\n\n"
                "  const setTheme = (newTheme: Theme) => {\n"
                "    try { localStorage.setItem(storageKey, newTheme); } catch (e) { /* ignore */ }\n"
                "    setThemeState(newTheme);\n    const root = window.document.documentElement;\n"
                "    root.classList.remove('light', 'dark'); root.classList.add(newTheme);\n  };\n\n"
                "  const toggleTheme = () => { setTheme(theme === 'light' ? 'dark' : 'light'); };\n\n"
                "  React.useEffect(() => {\n    const root = window.document.documentElement;\n"
                "    root.classList.remove('light', 'dark'); root.classList.add(theme);\n  }, [theme]);\n\n"
                "  const value = useMemo(() => ({ theme, setTheme, toggleTheme }), [theme]);\n\n"
                "  return (\n    <ThemeContext.Provider {...props} value={value}>\n      {children}\n    </ThemeContext.Provider>\n  );\n}\n\n"
                "export const useTheme = () => {\n  const context = useContext(ThemeContext);\n"
                "  if (context === undefined) { throw new Error('useTheme must be used within a ThemeProvider'); }\n  return context;\n};\n"
            )
        elif file_name == "AuthProvider.tsx" and "providers" in str(relative_file_path_str):
            content = (
                f"// Path: {relative_file_path_str}\n"
                "import React, { createContext, useState, useContext, ReactNode, useEffect } from 'react';\n\n"
                "interface User { id: string; username: string; }\n"
                "interface AuthContextType { user: User | null; isAuthenticated: boolean; login: (userData: User, token: string) => Promise<void>; logout: () => Promise<void>; isLoading: boolean; }\n"
                "const AuthContext = createContext<AuthContextType | undefined>(undefined);\n\n"
                "export function AuthProvider({ children }: { children: ReactNode }) {\n"
                "  const [user, setUser] = useState<User | null>(null);\n  const [isLoading, setIsLoading] = useState(true);\n\n"
                "  useEffect(() => {\n    const checkSession = async () => {\n      setIsLoading(true);\n      // Example: const token = localStorage.getItem('authToken'); if (token) { /* Validate token, fetch user */ }\n      setIsLoading(false);\n    };\n    checkSession();\n  }, []);\n\n"
                "  const login = async (userData: User, token: string) => { setUser(userData); /* localStorage.setItem('authToken', token); */ };\n\n"
                "  const logout = async () => { setUser(null); /* localStorage.removeItem('authToken'); */ };\n\n"
                "  return (\n    <AuthContext.Provider value={{ user, isAuthenticated: !!user, login, logout, isLoading }}>\n      {children}\n    </AuthContext.Provider>\n  );\n}\n\n"
                "export const useAuth = () => {\n  const context = useContext(AuthContext);\n"
                "  if (!context) { throw new Error('useAuth must be used within an AuthProvider'); }\n  return context;\n};\n"
            )
        elif file_name == "QueryClientProvider.tsx" and "providers" in str(relative_file_path_str):
             content = (
                f"// Path: {relative_file_path_str}\n"
                "import React, { ReactNode } from 'react';\n"
                "// import { QueryClient, QueryClientProvider as RQProvider } from '@tanstack/react-query';\n\n"
                "// const queryClient = new QueryClient();\n\n"
                "export function QueryClientProviderComponent({ children }: { children: ReactNode }) {\n"
                "  // return <RQProvider client={queryClient}>{children}</RQProvider>;\n"
                "  return <>{children}</>; // Placeholder\n"
                "}\n"
            )
        else:
            content = f"// Path: {relative_file_path_str}\n"
    elif file_name.endswith(".rs"):
        content = f"// Path: {relative_file_path_str}\n"
    elif file_name.endswith(".md"):
        content = f"# {file_name.replace('.md', '')}\n\nPath: {relative_file_path_str}\n"
        if file_name == "README.md" and str(dir_path_obj) == str(base_path_obj):
             content = f"# {BASE_DIR}\n\nDescrizione del progetto FatturaAnalyzer v2.\n\nPath: {relative_file_path_str}\n"
    elif file_name == ".gitignore":
        content = ("# General ignores\n__pycache__/\n*.pyc\n*.pyo\n*.pyd\n.Python\n"
                   "env/\nvenv/\n.env\n.env.*\n!*.env.example\n.vscode/\n.idea/\n*.swp\n*.swo\n.DS_Store\n\n"
                   "# Node / Frontend\nnode_modules/\ndist/\nbuild/\ncoverage/\n*.log\n"
                   "npm-debug.log*\nyarn-debug.log*\nyarn-error.log*\n\n"
                   "# Tauri build artifacts\nsrc-tauri/target/\nsrc-tauri/Cargo.lock\nsrc-tauri/gen/\n"
                   "*.AppImage\n*.deb\n*.rpm\n*.dmg\n*.msi\n*.app\n*.exe\n")
    elif file_name == ".env.example":
        content = ("# Environment variables example\nDEBUG=True\n\n"
                   "# Backend API configuration\nAPI_HOST=0.0.0.0\nAPI_PORT=8000\n\n"
                   "# Database (esempio per SQLite)\nDATABASE_URL=sqlite:///./fattura_analyzer.db\n\n"
                   "# Frontend\nVITE_API_BASE_URL=http://localhost:8000\n")
    elif file_name == "package.json" and str(dir_path_obj) == str(base_path_obj):
         content = ("{\n"
                   f'  "name": "{BASE_DIR.lower()}-monorepo",\n'
                   '  "version": "0.1.0",\n"private": true,\n'
                   '  "description": "Main package for FatturaAnalyzer v2.",\n'
                   '  "scripts": {\n'
                   '    "dev:backend": "cd backend && python run.py",\n'
                   '    "dev:frontend": "cd frontend && npm run dev",\n'
                   '    "dev": "echo \\"Run dev scripts or use ./scripts/dev.sh\\"",\n'
                   '    "build": "echo \\"Run build scripts or use ./scripts/build.sh\\"",\n'
                   '    "tauri:dev": "cd src-tauri && cargo tauri dev",\n'
                   '    "tauri:build": "cd src-tauri && cargo tauri build"\n'
                   '  },\n'
                   '  "workspaces": ["backend", "frontend"]\n'
                   "}")
    elif file_name == "requirements.txt" and dir_path_obj == base_path_obj / "backend":
        content = ("# Backend Python Dependencies\n"
                   "fastapi>=0.95.0,<0.110.0\nuvicorn[standard]>=0.20.0,<0.24.0\n"
                   "pydantic>=1.10.0,<3.0.0\nsqlalchemy>=1.4.0,<2.1.0\n"
                   "databases[sqlite]>=0.7.0,<0.9.0\npython-dotenv>=0.20.0,<1.1.0\n"
                   "python-jose[cryptography]>=3.3.0,<3.4.0\npasslib[bcrypt]>=1.7.4,<1.8.0\n"
                   "lxml>=4.9.0,<5.0.0\n"
                   "# pyOpenSSL>=22.0.0,<24.0.0\n# cryptography>=38.0.0,<42.0.0\n")
    elif file_name == "package.json" and dir_path_obj == base_path_obj / "frontend":
         content = ("{\n"
                   f'  "name": "frontend",\n"private": true,\n"version": "0.0.0",\n"type": "module",\n'
                   '  "scripts": {\n'
                   '    "dev": "vite",\n"build": "tsc && vite build",\n'
                   '    "lint": "eslint . --ext ts,tsx --report-unused-disable-directives --max-warnings 0",\n'
                   '    "preview": "vite preview",\n"tauri": "tauri"\n'
                   '  },\n'
                   '  "dependencies": {\n'
                   '    "react": "^18.2.0",\n"react-dom": "^18.2.0",\n"react-router-dom": "^6.15.0",\n'
                   '    "zustand": "^4.4.1",\n"axios": "^1.4.0",\n"clsx": "^2.0.0",\n'
                   '    "tailwind-merge": "^1.14.0",\n"lucide-react": "^0.274.0",\n'
                   '    "@radix-ui/react-slot": "^1.0.2",\n"tailwindcss-animate": "^1.0.7"\n'
                   '  },\n'
                   '  "devDependencies": {\n'
                   '    "@tauri-apps/api": "^1.4.0",\n"@types/react": "^18.2.15",\n"@types/react-dom": "^18.2.7",\n'
                   '    "@typescript-eslint/eslint-plugin": "^6.0.0",\n"@typescript-eslint/parser": "^6.0.0",\n'
                   '    "@vitejs/plugin-react": "^4.0.3",\n"autoprefixer": "^10.4.14",\n"eslint": "^8.45.0",\n'
                   '    "eslint-plugin-react-hooks": "^4.6.0",\n"eslint-plugin-react-refresh": "^0.4.3",\n'
                   '    "postcss": "^8.4.27",\n"tailwindcss": "^3.3.3",\n"typescript": "^5.0.2",\n"vite": "^4.4.5"\n'
                   '  }\n'
                   "}")
    elif file_name == "vite.config.ts" and dir_path_obj == base_path_obj / "frontend":
        content = ("// Path: frontend/vite.config.ts\n"
                   "import { defineConfig } from 'vite';\nimport react from '@vitejs/plugin-react';\n"
                   "import path from 'path';\n\n"
                   "export default defineConfig({\n"
                   "  plugins: [react()],\n  resolve: {\n    alias: {\n      '@': path.resolve(__dirname, './src'),\n    },\n  },\n"
                   "  clearScreen: false,\n  server: {\n    port: 1420,\n    strictPort: true,\n    watch: {\n      ignored: [\"**/src-tauri/**\"],\n    },\n  },\n"
                   "  envPrefix: ['VITE_', 'TAURI_'],\n  build: {\n"
                   "    target: process.env.TAURI_PLATFORM == 'windows' ? 'chrome105' : 'safari13',\n"
                   "    minify: !process.env.TAURI_DEBUG ? 'esbuild' : false,\n"
                   "    sourcemap: !!process.env.TAURI_DEBUG,\n  },\n"
                   "})")
    elif file_name == "tailwind.config.js" and dir_path_obj == base_path_obj / "frontend":
        content = ("// Path: frontend/tailwind.config.js\n/** @type {import('tailwindcss').Config} */\n"
                   "export default {\n  darkMode: ['class'],\n"
                   "  content: ['./pages/**/*.{ts,tsx}', './components/**/*.{ts,tsx}', './app/**/*.{ts,tsx}', './src/**/*.{ts,tsx}'],\n"
                   "  theme: {\n    container: {\n      center: true,\n      padding: '2rem',\n      screens: {\n        '2xl': '1400px',\n      },\n    },\n"
                   "    extend: {\n      keyframes: {\n        'accordion-down': { from: { height: 0 }, to: { height: 'var(--radix-accordion-content-height)' } },\n"
                   "        'accordion-up': { from: { height: 'var(--radix-accordion-content-height)' }, to: { height: 0 } },\n      },\n"
                   "      animation: { 'accordion-down': 'accordion-down 0.2s ease-out', 'accordion-up': 'accordion-up 0.2s ease-out' },\n"
                   "    },\n  },\n  plugins: [require('tailwindcss-animate')],\n}")
    elif file_name == "postcss.config.js" and dir_path_obj == base_path_obj / "frontend":
        content = ("// Path: frontend/postcss.config.js\nexport default {\n"
                   "  plugins: {\n    tailwindcss: {},\n    autoprefixer: {},\n  },\n}")
    elif file_name == "tsconfig.json" and dir_path_obj == base_path_obj / "frontend":
        content = ("{\n  \"compilerOptions\": {\n"
                   "    \"target\": \"ESNext\",\n\"useDefineForClassFields\": true,\n\"lib\": [\"DOM\", \"DOM.Iterable\", \"ESNext\"],\n"
                   "    \"allowJs\": false,\n\"skipLibCheck\": true,\n\"esModuleInterop\": true,\n\"allowSyntheticDefaultImports\": true,\n"
                   "    \"strict\": true,\n\"forceConsistentCasingInFileNames\": true,\n\"module\": \"ESNext\",\n"
                   "    \"moduleResolution\": \"Bundler\",\n\"resolveJsonModule\": true,\n\"isolatedModules\": true,\n"
                   "    \"noEmit\": true,\n\"jsx\": \"react-jsx\",\n\"baseUrl\": \".\",\n\"paths\": {\n      \"@/*\": [\"./src/*\"]\n    }\n  },\n"
                   "  \"include\": [\"src\", \"vite.config.ts\", \"tailwind.config.js\", \"postcss.config.js\"],\n"
                   "  \"references\": [{ \"path\": \"./tsconfig.node.json\" }]\n}")
    elif file_name == "tsconfig.node.json" and dir_path_obj == base_path_obj / "frontend":
        content = ("{\n  \"compilerOptions\": {\n"
                   "    \"composite\": true,\n\"skipLibCheck\": true,\n\"module\": \"ESNext\",\n"
                   "    \"moduleResolution\": \"bundler\",\n\"allowSyntheticDefaultImports\": true\n  },\n"
                   "  \"include\": [\"vite.config.ts\"]\n}")
    elif file_name == "Cargo.toml" and dir_path_obj == base_path_obj / "src-tauri":
        content = ("[package]\n"
                   f'name = "{BASE_DIR.lower().replace("-", "_")}_desktop"\n'
                   'version = "0.1.0"\ndescription = "Desktop application for FatturaAnalyzer v2"\n'
                   'authors = ["Il Tuo Nome Qui"]\nlicense = ""\nrepository = ""\nedition = "2021"\n\n'
                   "[build-dependencies]\ntauri-build = { version = \"1.4.0\", features = [] }\n\n"
                   "[dependencies]\n"
                   "tauri = { version = \"1.4.0\", features = [\"shell-open\", \"http-all\", \"dialog-all\", \"fs-all\", \"path-all\"] }\n"
                   "serde = { version = \"1.0\", features = [\"derive\"] }\nserde_json = \"1.0\"\n"
                   "tokio = { version = \"1\", features = [\"full\"] }\nanyhow = \"1.0\"\nlog = \"0.4\"\nenv_logger = \"0.10\"\n\n"
                   "[features]\ncustom-protocol = [\"tauri/custom-protocol\"]\n")
    elif file_name == "tauri.conf.json" and dir_path_obj == base_path_obj / "src-tauri":
        content = ("{\n  \"$schema\": \"../node_modules/@tauri-apps/cli/schema.json\",\n"
                   '  "build": {\n'
                   '    "beforeDevCommand": "npm run dev --prefix ../frontend",\n'
                   '    "beforeBuildCommand": "npm run build --prefix ../frontend",\n'
                   '    "devPath": "http://localhost:1420",\n"distDir": "../frontend/dist",\n"withGlobalTauri": true\n  },\n'
                   '  "package": {\n    "productName": "FatturaAnalyzer-v2",\n"version": "0.1.0"\n  },\n'
                   '  "tauri": {\n    "allowlist": {\n      "all": false,\n"shell": { "open": true },\n'
                   '      "http": { "all": true, "scope": ["http://localhost:8000/*"] },\n'
                   '      "dialog": { "all": true },\n"fs": { "all": true, "scope": ["$APPDATA/*", "$DOCUMENT/*"] },\n'
                   '      "path": { "all": true }\n    },\n'
                   '    "bundle": {\n      "active": true,\n"targets": "all",\n"identifier": "com.example.fatturaanalyzer",\n'
                   '      "icon": ["icons/32x32.png", "icons/128x128.png", "icons/icon.icns", "icons/icon.ico"]\n    },\n'
                   '    "security": { "csp": null },\n'
                   '    "windows": [{\n      "fullscreen": false,\n"resizable": true,\n"title": "FatturaAnalyzer-v2",\n'
                   '      "width": 1200,\n"height": 800,\n"minWidth": 800,\n"minHeight": 600\n    }]\n  }\n}')
    elif file_name == "build.rs" and dir_path_obj == base_path_obj / "src-tauri":
        content = "// Path: src-tauri/build.rs\nfn main() {\n  tauri_build::build()\n}"
    elif file_name == "main.rs" and dir_path_obj == base_path_obj / "src-tauri" / "src":
         content = ("// Prevents additional console window on Windows in release, DO NOT REMOVE!!\n"
                    "#![cfg_attr(not(debug_assertions), windows_subsystem = \"windows\")]\n\n"
                    "#[tauri::command]\nfn greet(name: &str) -> String {\n"
                    "    format!(\"Hello, {}! You've been greeted from Rust!\", name)\n}\n\n"
                    "fn main() {\n    tauri::Builder::default()\n"
                    "        .invoke_handler(tauri::generate_handler![greet])\n"
                    "        .run(tauri::generate_context!())\n"
                    "        .expect(\"error while running tauri application\");\n}")
    # --- FINE LOGICA CONTENUTO FILE ---
    return content

def create_or_update_project_structure():
    base_path = Path(BASE_DIR)
    if not base_path.exists():
        print(f"La directory base '{BASE_DIR}' non esiste. Creala prima o esegui lo script di creazione completo.")
        return
    if not base_path.is_dir():
        print(f"'{BASE_DIR}' esiste ma non è una directory.")
        return

    print(f"Verifica e aggiornamento della struttura del progetto in: {base_path.resolve()}")

    for dir_path_str_template, files_in_dir in structure:
        dir_path = Path(dir_path_str_template)

        if not dir_path.exists():
            dir_path.mkdir(parents=True, exist_ok=True)
            print(f"  Creata directory mancante: {dir_path.resolve()}")
        elif not dir_path.is_dir():
            print(f"  ATTENZIONE: Prevista directory ma trovato file: {dir_path.resolve()} - Saltato.")
            continue
        else:
            print(f"  Verificata directory esistente: {dir_path.resolve()}")

        for file_name in files_in_dir:
            file_path = dir_path / file_name
            relative_file_path_str = str(file_path.relative_to(base_path))
            
            should_overwrite_this_file = False
            for fname_to_overwrite, fdir_template_to_overwrite in files_to_overwrite_if_exists:
                fdir_to_overwrite = Path(fdir_template_to_overwrite)
                if file_name == fname_to_overwrite and dir_path == fdir_to_overwrite:
                    should_overwrite_this_file = True
                    break
            
            file_content_to_write = get_file_content(file_name, dir_path, base_path, relative_file_path_str)

            if not file_path.exists():
                print(f"    Creato file mancante: {file_path.resolve()}")
                try:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(file_content_to_write)
                except Exception as e:
                    print(f"      ERRORE durante la scrittura del nuovo file {file_path}: {e}")
            elif should_overwrite_this_file:
                print(f"    Sovrascritto file di configurazione: {file_path.resolve()}")
                try:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(file_content_to_write)
                except Exception as e:
                    print(f"      ERRORE durante la sovrascrittura del file {file_path}: {e}")
            else:
                print(f"    File esistente, contenuto non modificato: {file_path.resolve()}")

    print(f"\nVerifica e aggiornamento della struttura del progetto '{BASE_DIR}' completati.")
    print("Rivedi l'output per eventuali messaggi di attenzione o errore.")
    print("\nPassi successivi suggeriti (se non già fatti):")
    print(f"1. Naviga in '{BASE_DIR}/frontend' ed esegui 'npm install' (o yarn/pnpm).")
    print(f"2. Assicurati che l'ambiente virtuale per il backend in '{BASE_DIR}/backend' sia attivo e le dipendenze da 'requirements.txt' siano installate.")
    print(f"3. Controlla/installa le dipendenze di sistema per Tauri e poi esegui 'cargo check' o 'cargo build' in '{BASE_DIR}/src-tauri'.")

if __name__ == "__main__":
    if not os.path.exists(BASE_DIR) or not os.path.isdir(BASE_DIR):
        print(f"ERRORE: La cartella base del progetto '{BASE_DIR}' non esiste o non è una directory.")
        print("Questo script è pensato per CORREGGERE una struttura esistente.")
        print(f"Esegui prima lo script di creazione completo se '{BASE_DIR}' non esiste, oppure assicurati che lo script sia nella directory genitore di '{BASE_DIR}'.")
        exit()

    print(f"Questo script verificherà la struttura della cartella '{BASE_DIR}' e aggiungerà file/cartelle mancanti.")
    print("Alcuni file di configurazione di base (definiti in 'files_to_overwrite_if_exists') verranno sovrascritti se esistono.")
    print("I file di codice esistenti non verranno modificati nel loro contenuto se non sono in tale lista.")
    confirm = input(f"Continuare? (s/N): ")
    if confirm.lower() != 's':
        print("Operazione annullata.")
        exit()

    create_or_update_project_structure()
--- Fine contenuto di: create.py ---
